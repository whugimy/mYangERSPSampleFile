{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERSP 22-23 Michael Yang\n",
    "\n",
    "This is a notebook I wrote that annotates the training process of a very simple, low parameter feedforward neural network on the [CIC-IDS2017 Intrusion Detection Evaluation Dataset](https://www.unb.ca/cic/datasets/ids-2017.html).\n",
    "\n",
    "It also uses the TRUSTEE framework published by Jacobs, et al. in the paper [AI/ML for Network Security: The Emperor has no Clothes](https://sites.cs.ucsb.edu/~arpitgupta/pdfs/trustee.pdf) to evaluate the neural network.\n",
    "\n",
    "My research was under the Systems and Networking Lab (SNL) at UCSB, headed by Arpit Gupta. My mentor was Roman Beltiukov. All research conducted as part of the Early Research Scholars Program run by the UCSB CS department, which I highly recommend!\n",
    "\n",
    "For context on this project, head to [my website](https://whugimy.github.io/projects/ersp/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all libraries. We set a random seed for reproducibility. We can see that this notebook was most recently run on a CPU, not a GPU. The model has intentionally been made to be simple and small for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "SEED=190\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take a look at the features in our balanced dataset. The original published full dataset is relatively skewed towards normal network traffic. We have created a balanced subsample containing 50% normal points, 50% intrusions. The .csv file has not been included in the repository due to size contraints; the file was approximately 10GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n",
      "       'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
      "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
      "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
      "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
      "       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
      "       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
      "       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
      "       'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
      "       'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
      "       'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
      "       'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
      "       'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
      "       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
      "       'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
      "       'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
      "       'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
      "       'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
      "       'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
      "       'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
      "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('balanced_set.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare our data for the model. We first normalize our data to prevent gradient explosions. We use z-score normalization.\n",
    "\n",
    "We now split our pandas dataframe into two sets of separate `X` and `Y` dataframes for training and testing later. We also drop known problematic or unnecessary features from `X`. I have very little networking expertise; these features were suggested by Roman Beltiukov.\n",
    "\n",
    "We finally output the shape of our dataframes. We've kept 68 features and randomly taken 10% of our dataset to use as a testing set. This random subsample was done with `sklearn`'s `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4944481, 68)\n",
      "(549387, 68)\n",
      "(4944481,)\n",
      "(549387,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Label', axis=1)\n",
    "\n",
    "X = (X-X.mean())/X.std()\n",
    "Y = df['Label']\n",
    "\n",
    "X.columns[X.isna().any()].tolist()\n",
    "X = X.drop(['Protocol','Bwd PSH Flags','Bwd URG Flags','Fwd Byts/b Avg','Fwd Pkts/b Avg','Fwd Blk Rate Avg','Bwd Byts/b Avg','Bwd Pkts/b Avg','Bwd Blk Rate Avg'], axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring our remaining features are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts',\n",
      "       'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min',\n",
      "       'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max',\n",
      "       'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s',\n",
      "       'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
      "       'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
      "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean',\n",
      "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
      "       'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
      "       'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
      "       'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
      "       'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
      "       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
      "       'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts',\n",
      "       'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
      "       'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
      "       'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
      "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we start using PyTorch to create a Dataset and Dataloader to input to our model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.x = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels.values, dtype=torch.long)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now declare our datasets with our created dataframes as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(X_train, Y_train)\n",
    "valid_ds = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take a quick look at a random datapoint. As we can see, the data has been normalized so that they are all fairly close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.5358e-02, -2.1717e-02, -2.7047e-02, -2.3709e-02, -1.5888e-02,\n",
       "         -5.6697e-01, -3.4663e-01, -6.8576e-01, -5.4623e-01, -6.4991e-01,\n",
       "         -3.9474e-01, -6.2277e-01, -6.0950e-01, -5.4105e-02,  7.6153e-01,\n",
       "         -1.8992e-01, -2.9473e-03, -8.7435e-03, -3.0280e-03, -1.4896e-02,\n",
       "         -2.0126e-01, -3.0395e-03, -8.4495e-03, -3.1752e-03, -2.3742e-01,\n",
       "         -1.4929e-01, -2.1273e-01, -2.1053e-01, -6.5273e-02, -1.6745e-01,\n",
       "         -1.6794e-02, -2.3241e-02, -3.0489e-02,  5.5680e-01,  9.0088e-01,\n",
       "         -3.6769e-01, -6.9129e-01, -6.6822e-01, -6.8519e-01, -1.5088e-01,\n",
       "         -5.4206e-02, -1.6745e-01, -5.0728e-01,  1.2159e+00, -8.5239e-01,\n",
       "         -2.0755e-01, -1.6794e-02, -5.0729e-01,  6.4583e-01, -7.2154e-01,\n",
       "         -6.8576e-01, -6.2277e-01, -2.1717e-02, -2.3709e-02, -2.7047e-02,\n",
       "         -1.5887e-02,  8.5792e-01, -3.2385e-01, -2.0935e-02,  2.2355e+00,\n",
       "         -5.7687e-02, -4.6765e-02, -6.5431e-02, -4.6213e-02, -1.4954e-02,\n",
       "         -1.4701e-03, -7.4057e-03, -1.3927e-01]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a very simple FFN model class. This model is low in parameters and is overall fairly small. It also does not make use of dropout to prevent overfitting. However, for the purpose of this sample, it still performs fairly well on the dataset as we will later see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential( # mostly linear fully connected layers followed by ReLU activation function. No dropout.\n",
    "            nn.Linear(68, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,15)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # F.log_softmax returns the log probabilities of each class\n",
    "        # of shape (num_samples, num_classes)\n",
    "        return x\n",
    "    def pred(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_tensor = torch.tensor(x.values, dtype=torch.float32)\n",
    "            output = self.layers(x_tensor)\n",
    "            return torch.argmax(output, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a general purpose training loop below. Annotations provided for a very detailed explanation.\n",
    "\n",
    "Minibatches: [http://d2l.ai/chapter_optimization/minibatch-sgd.html](http://d2l.ai/chapter_optimization/minibatch-sgd.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, criterion, train_loader, valid_loader):\n",
    "    best_acc = 0\n",
    "    for epoch in (range(epochs)): # we run some number of training epochs\n",
    "        \n",
    "        running_loss = 0.0 # store total loss\n",
    "        running_total = 0.0 # store number of batches per epoch\n",
    "        \n",
    "        model.train() # set model to train mode so that it stores gradients\n",
    "        for batch, (x,y) in (enumerate(train_loader)): # dataset is too big to load at once, so we split it into minibatches.\n",
    "            x,y = x.to(device), y.to(device) # load data to our device\n",
    "            y_hat = model(x) # process data in model to get initial results\n",
    "            _, predicted = torch.max(y_hat.data,1) # predictions are the maximum value across the 1 axis\n",
    "            loss = criterion(y_hat, y) # we calculate the loss (we are trying to minimize it)\n",
    "            optimizer.zero_grad() # set the gradient to 0 (get rid of prior gradient)\n",
    "            loss.backward() # back propagation (get gradient)\n",
    "            optimizer.step() # gradient descent step (update parameters)\n",
    "            running_loss += loss.item() # add to total loss\n",
    "            running_total += 1 # add to total batch count\n",
    "            if batch % 50 == 0: # output results for every 50th batch\n",
    "                print(f'   [epoch: {epoch}, batch: {batch*len(y):5d}/{len(train_loader.dataset)}, loss: {running_loss / running_total:.5f}]')\n",
    "                running_loss = 0.0\n",
    "                running_total = 0.0\n",
    "        current_accuracy = validate(model, valid_loader) # at the end of the batch, we validate the model against the other dataset split\n",
    "        if (current_accuracy > best_acc): # if it performed better, we save the best model checkpoint\n",
    "            best_acc = current_accuracy\n",
    "            torch.save(model, 'best_checkpoint.pt')\n",
    "            print('   best updated!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a validation loop. It is largely similar to the training loop, but we don't store gradients or update parameters. Sole purpose is to evaluate the model against the other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0.0\n",
    "        total = 0\n",
    "\n",
    "        model.eval()\n",
    "        for batch, (x,y) in (enumerate(valid_loader)):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            _, predicted = torch.max(y_hat.data,1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'   Validation Accuracy: {100 * correct / total} %')\n",
    "    \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters! We use Cross Entropy Loss (standard for classification) and [Adam](https://arxiv.org/abs/1412.6980) as our learning rate scheduler. I am aware that techniques like [untuned warmup](https://arxiv.org/pdf/1910.04209.pdf#:~:text=Adaptive%20optimization%20algorithms%20such%20as,schedule%20for%20the%20learning%20rate.) have demonstrated strong performance, but for simplicity's sake this is not used here. We can also see that the model has only about 170k parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 170015\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = FFN()\n",
    "test_optimizer = optim.Adam(model.parameters())\n",
    "test_criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 512\n",
    "test_batch_size = 1000\n",
    "\n",
    "model = model.to(device)\n",
    "train_load = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_load = torch.utils.data.DataLoader(valid_ds, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_params])\n",
    "print('model parameters:',params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the training loop, we can see that the model indeed improves over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [epoch: 0, batch:     0/4944481, loss: 2.70962]\n",
      "   [epoch: 0, batch: 25600/4944481, loss: 2.00988]\n",
      "   [epoch: 0, batch: 51200/4944481, loss: 1.34024]\n",
      "   [epoch: 0, batch: 76800/4944481, loss: 1.12538]\n",
      "   [epoch: 0, batch: 102400/4944481, loss: 0.80638]\n",
      "   [epoch: 0, batch: 128000/4944481, loss: 0.52152]\n",
      "   [epoch: 0, batch: 153600/4944481, loss: 0.33524]\n",
      "   [epoch: 0, batch: 179200/4944481, loss: 0.30501]\n",
      "   [epoch: 0, batch: 204800/4944481, loss: 0.26055]\n",
      "   [epoch: 0, batch: 230400/4944481, loss: 0.23211]\n",
      "   [epoch: 0, batch: 256000/4944481, loss: 0.22333]\n",
      "   [epoch: 0, batch: 281600/4944481, loss: 0.43532]\n",
      "   [epoch: 0, batch: 307200/4944481, loss: 0.27668]\n",
      "   [epoch: 0, batch: 332800/4944481, loss: 0.23332]\n",
      "   [epoch: 0, batch: 358400/4944481, loss: 0.22486]\n",
      "   [epoch: 0, batch: 384000/4944481, loss: 0.22042]\n",
      "   [epoch: 0, batch: 409600/4944481, loss: 0.21422]\n",
      "   [epoch: 0, batch: 435200/4944481, loss: 0.20378]\n",
      "   [epoch: 0, batch: 460800/4944481, loss: 0.21706]\n",
      "   [epoch: 0, batch: 486400/4944481, loss: 0.27694]\n",
      "   [epoch: 0, batch: 512000/4944481, loss: 0.69900]\n",
      "   [epoch: 0, batch: 537600/4944481, loss: 0.36214]\n",
      "   [epoch: 0, batch: 563200/4944481, loss: 0.27181]\n",
      "   [epoch: 0, batch: 588800/4944481, loss: 0.21755]\n",
      "   [epoch: 0, batch: 614400/4944481, loss: 0.21838]\n",
      "   [epoch: 0, batch: 640000/4944481, loss: 0.32691]\n",
      "   [epoch: 0, batch: 665600/4944481, loss: 0.23272]\n",
      "   [epoch: 0, batch: 691200/4944481, loss: 0.20324]\n",
      "   [epoch: 0, batch: 716800/4944481, loss: 0.19186]\n",
      "   [epoch: 0, batch: 742400/4944481, loss: 0.18703]\n",
      "   [epoch: 0, batch: 768000/4944481, loss: 0.18315]\n",
      "   [epoch: 0, batch: 793600/4944481, loss: 0.19095]\n",
      "   [epoch: 0, batch: 819200/4944481, loss: 0.20448]\n",
      "   [epoch: 0, batch: 844800/4944481, loss: 0.17800]\n",
      "   [epoch: 0, batch: 870400/4944481, loss: 0.17162]\n",
      "   [epoch: 0, batch: 896000/4944481, loss: 0.21259]\n",
      "   [epoch: 0, batch: 921600/4944481, loss: 0.18578]\n",
      "   [epoch: 0, batch: 947200/4944481, loss: 0.18938]\n",
      "   [epoch: 0, batch: 972800/4944481, loss: 0.17752]\n",
      "   [epoch: 0, batch: 998400/4944481, loss: 0.18861]\n",
      "   [epoch: 0, batch: 1024000/4944481, loss: 0.17787]\n",
      "   [epoch: 0, batch: 1049600/4944481, loss: 0.16582]\n",
      "   [epoch: 0, batch: 1075200/4944481, loss: 0.16645]\n",
      "   [epoch: 0, batch: 1100800/4944481, loss: 0.18456]\n",
      "   [epoch: 0, batch: 1126400/4944481, loss: 0.18037]\n",
      "   [epoch: 0, batch: 1152000/4944481, loss: 0.18344]\n",
      "   [epoch: 0, batch: 1177600/4944481, loss: 0.16661]\n",
      "   [epoch: 0, batch: 1203200/4944481, loss: 0.17143]\n",
      "   [epoch: 0, batch: 1228800/4944481, loss: 0.17104]\n",
      "   [epoch: 0, batch: 1254400/4944481, loss: 0.17002]\n",
      "   [epoch: 0, batch: 1280000/4944481, loss: 0.16343]\n",
      "   [epoch: 0, batch: 1305600/4944481, loss: 0.20433]\n",
      "   [epoch: 0, batch: 1331200/4944481, loss: 0.17361]\n",
      "   [epoch: 0, batch: 1356800/4944481, loss: 0.19149]\n",
      "   [epoch: 0, batch: 1382400/4944481, loss: 0.17148]\n",
      "   [epoch: 0, batch: 1408000/4944481, loss: 0.16578]\n",
      "   [epoch: 0, batch: 1433600/4944481, loss: 0.16381]\n",
      "   [epoch: 0, batch: 1459200/4944481, loss: 0.16371]\n",
      "   [epoch: 0, batch: 1484800/4944481, loss: 0.15957]\n",
      "   [epoch: 0, batch: 1510400/4944481, loss: 0.16455]\n",
      "   [epoch: 0, batch: 1536000/4944481, loss: 0.17251]\n",
      "   [epoch: 0, batch: 1561600/4944481, loss: 0.44659]\n",
      "   [epoch: 0, batch: 1587200/4944481, loss: 0.38169]\n",
      "   [epoch: 0, batch: 1612800/4944481, loss: 0.22047]\n",
      "   [epoch: 0, batch: 1638400/4944481, loss: 0.28417]\n",
      "   [epoch: 0, batch: 1664000/4944481, loss: 0.24480]\n",
      "   [epoch: 0, batch: 1689600/4944481, loss: 0.24578]\n",
      "   [epoch: 0, batch: 1715200/4944481, loss: 0.20574]\n",
      "   [epoch: 0, batch: 1740800/4944481, loss: 0.20165]\n",
      "   [epoch: 0, batch: 1766400/4944481, loss: 0.18026]\n",
      "   [epoch: 0, batch: 1792000/4944481, loss: 0.18488]\n",
      "   [epoch: 0, batch: 1817600/4944481, loss: 0.18903]\n",
      "   [epoch: 0, batch: 1843200/4944481, loss: 0.31974]\n",
      "   [epoch: 0, batch: 1868800/4944481, loss: 0.21419]\n",
      "   [epoch: 0, batch: 1894400/4944481, loss: 0.18067]\n",
      "   [epoch: 0, batch: 1920000/4944481, loss: 0.18432]\n",
      "   [epoch: 0, batch: 1945600/4944481, loss: 0.17545]\n",
      "   [epoch: 0, batch: 1971200/4944481, loss: 0.17173]\n",
      "   [epoch: 0, batch: 1996800/4944481, loss: 0.18068]\n",
      "   [epoch: 0, batch: 2022400/4944481, loss: 0.17236]\n",
      "   [epoch: 0, batch: 2048000/4944481, loss: 0.18080]\n",
      "   [epoch: 0, batch: 2073600/4944481, loss: 0.17408]\n",
      "   [epoch: 0, batch: 2099200/4944481, loss: 0.16410]\n",
      "   [epoch: 0, batch: 2124800/4944481, loss: 0.17037]\n",
      "   [epoch: 0, batch: 2150400/4944481, loss: 0.18945]\n",
      "   [epoch: 0, batch: 2176000/4944481, loss: 0.17535]\n",
      "   [epoch: 0, batch: 2201600/4944481, loss: 0.17204]\n",
      "   [epoch: 0, batch: 2227200/4944481, loss: 0.15977]\n",
      "   [epoch: 0, batch: 2252800/4944481, loss: 0.16933]\n",
      "   [epoch: 0, batch: 2278400/4944481, loss: 0.16888]\n",
      "   [epoch: 0, batch: 2304000/4944481, loss: 0.16427]\n",
      "   [epoch: 0, batch: 2329600/4944481, loss: 0.21527]\n",
      "   [epoch: 0, batch: 2355200/4944481, loss: 0.17688]\n",
      "   [epoch: 0, batch: 2380800/4944481, loss: 0.16954]\n",
      "   [epoch: 0, batch: 2406400/4944481, loss: 0.16934]\n",
      "   [epoch: 0, batch: 2432000/4944481, loss: 0.16512]\n",
      "   [epoch: 0, batch: 2457600/4944481, loss: 0.17209]\n",
      "   [epoch: 0, batch: 2483200/4944481, loss: 0.18767]\n",
      "   [epoch: 0, batch: 2508800/4944481, loss: 0.18020]\n",
      "   [epoch: 0, batch: 2534400/4944481, loss: 0.16943]\n",
      "   [epoch: 0, batch: 2560000/4944481, loss: 0.16687]\n",
      "   [epoch: 0, batch: 2585600/4944481, loss: 0.16048]\n",
      "   [epoch: 0, batch: 2611200/4944481, loss: 0.17382]\n",
      "   [epoch: 0, batch: 2636800/4944481, loss: 0.16170]\n",
      "   [epoch: 0, batch: 2662400/4944481, loss: 0.15369]\n",
      "   [epoch: 0, batch: 2688000/4944481, loss: 0.16487]\n",
      "   [epoch: 0, batch: 2713600/4944481, loss: 0.16459]\n",
      "   [epoch: 0, batch: 2739200/4944481, loss: 0.16335]\n",
      "   [epoch: 0, batch: 2764800/4944481, loss: 0.44971]\n",
      "   [epoch: 0, batch: 2790400/4944481, loss: 0.44226]\n",
      "   [epoch: 0, batch: 2816000/4944481, loss: 0.25340]\n",
      "   [epoch: 0, batch: 2841600/4944481, loss: 0.22957]\n",
      "   [epoch: 0, batch: 2867200/4944481, loss: 0.20506]\n",
      "   [epoch: 0, batch: 2892800/4944481, loss: 0.20024]\n",
      "   [epoch: 0, batch: 2918400/4944481, loss: 0.19692]\n",
      "   [epoch: 0, batch: 2944000/4944481, loss: 0.19972]\n",
      "   [epoch: 0, batch: 2969600/4944481, loss: 0.21660]\n",
      "   [epoch: 0, batch: 2995200/4944481, loss: 0.19380]\n",
      "   [epoch: 0, batch: 3020800/4944481, loss: 0.19286]\n",
      "   [epoch: 0, batch: 3046400/4944481, loss: 0.17651]\n",
      "   [epoch: 0, batch: 3072000/4944481, loss: 0.18695]\n",
      "   [epoch: 0, batch: 3097600/4944481, loss: 0.18722]\n",
      "   [epoch: 0, batch: 3123200/4944481, loss: 0.18543]\n",
      "   [epoch: 0, batch: 3148800/4944481, loss: 0.20305]\n",
      "   [epoch: 0, batch: 3174400/4944481, loss: 0.18287]\n",
      "   [epoch: 0, batch: 3200000/4944481, loss: 0.22820]\n",
      "   [epoch: 0, batch: 3225600/4944481, loss: 0.19275]\n",
      "   [epoch: 0, batch: 3251200/4944481, loss: 0.18123]\n",
      "   [epoch: 0, batch: 3276800/4944481, loss: 0.17532]\n",
      "   [epoch: 0, batch: 3302400/4944481, loss: 0.16953]\n",
      "   [epoch: 0, batch: 3328000/4944481, loss: 0.17772]\n",
      "   [epoch: 0, batch: 3353600/4944481, loss: 0.17675]\n",
      "   [epoch: 0, batch: 3379200/4944481, loss: 0.19435]\n",
      "   [epoch: 0, batch: 3404800/4944481, loss: 0.19534]\n",
      "   [epoch: 0, batch: 3430400/4944481, loss: 0.17875]\n",
      "   [epoch: 0, batch: 3456000/4944481, loss: 0.16222]\n",
      "   [epoch: 0, batch: 3481600/4944481, loss: 0.17505]\n",
      "   [epoch: 0, batch: 3507200/4944481, loss: 0.17047]\n",
      "   [epoch: 0, batch: 3532800/4944481, loss: 0.18797]\n",
      "   [epoch: 0, batch: 3558400/4944481, loss: 0.19658]\n",
      "   [epoch: 0, batch: 3584000/4944481, loss: 0.17900]\n",
      "   [epoch: 0, batch: 3609600/4944481, loss: 0.17413]\n",
      "   [epoch: 0, batch: 3635200/4944481, loss: 0.17611]\n",
      "   [epoch: 0, batch: 3660800/4944481, loss: 0.18074]\n",
      "   [epoch: 0, batch: 3686400/4944481, loss: 0.17118]\n",
      "   [epoch: 0, batch: 3712000/4944481, loss: 0.17156]\n",
      "   [epoch: 0, batch: 3737600/4944481, loss: 0.17181]\n",
      "   [epoch: 0, batch: 3763200/4944481, loss: 0.16842]\n",
      "   [epoch: 0, batch: 3788800/4944481, loss: 0.41657]\n",
      "   [epoch: 0, batch: 3814400/4944481, loss: 0.22707]\n",
      "   [epoch: 0, batch: 3840000/4944481, loss: 0.19345]\n",
      "   [epoch: 0, batch: 3865600/4944481, loss: 0.19083]\n",
      "   [epoch: 0, batch: 3891200/4944481, loss: 0.18811]\n",
      "   [epoch: 0, batch: 3916800/4944481, loss: 0.18642]\n",
      "   [epoch: 0, batch: 3942400/4944481, loss: 0.19942]\n",
      "   [epoch: 0, batch: 3968000/4944481, loss: 0.17710]\n",
      "   [epoch: 0, batch: 3993600/4944481, loss: 0.17554]\n",
      "   [epoch: 0, batch: 4019200/4944481, loss: 0.17655]\n",
      "   [epoch: 0, batch: 4044800/4944481, loss: 0.21345]\n",
      "   [epoch: 0, batch: 4070400/4944481, loss: 0.19038]\n",
      "   [epoch: 0, batch: 4096000/4944481, loss: 0.17815]\n",
      "   [epoch: 0, batch: 4121600/4944481, loss: 0.17158]\n",
      "   [epoch: 0, batch: 4147200/4944481, loss: 0.16367]\n",
      "   [epoch: 0, batch: 4172800/4944481, loss: 0.16778]\n",
      "   [epoch: 0, batch: 4198400/4944481, loss: 0.16399]\n",
      "   [epoch: 0, batch: 4224000/4944481, loss: 0.15742]\n",
      "   [epoch: 0, batch: 4249600/4944481, loss: 0.26600]\n",
      "   [epoch: 0, batch: 4275200/4944481, loss: 0.22719]\n",
      "   [epoch: 0, batch: 4300800/4944481, loss: 0.18143]\n",
      "   [epoch: 0, batch: 4326400/4944481, loss: 0.17038]\n",
      "   [epoch: 0, batch: 4352000/4944481, loss: 0.16653]\n",
      "   [epoch: 0, batch: 4377600/4944481, loss: 0.16246]\n",
      "   [epoch: 0, batch: 4403200/4944481, loss: 0.16311]\n",
      "   [epoch: 0, batch: 4428800/4944481, loss: 0.17811]\n",
      "   [epoch: 0, batch: 4454400/4944481, loss: 0.16506]\n",
      "   [epoch: 0, batch: 4480000/4944481, loss: 0.15656]\n",
      "   [epoch: 0, batch: 4505600/4944481, loss: 0.17013]\n",
      "   [epoch: 0, batch: 4531200/4944481, loss: 0.16595]\n",
      "   [epoch: 0, batch: 4556800/4944481, loss: 0.15922]\n",
      "   [epoch: 0, batch: 4582400/4944481, loss: 0.15613]\n",
      "   [epoch: 0, batch: 4608000/4944481, loss: 0.16179]\n",
      "   [epoch: 0, batch: 4633600/4944481, loss: 0.16110]\n",
      "   [epoch: 0, batch: 4659200/4944481, loss: 0.16173]\n",
      "   [epoch: 0, batch: 4684800/4944481, loss: 0.17538]\n",
      "   [epoch: 0, batch: 4710400/4944481, loss: 0.17477]\n",
      "   [epoch: 0, batch: 4736000/4944481, loss: 0.16160]\n",
      "   [epoch: 0, batch: 4761600/4944481, loss: 0.17491]\n",
      "   [epoch: 0, batch: 4787200/4944481, loss: 0.16264]\n",
      "   [epoch: 0, batch: 4812800/4944481, loss: 0.15928]\n",
      "   [epoch: 0, batch: 4838400/4944481, loss: 0.15904]\n",
      "   [epoch: 0, batch: 4864000/4944481, loss: 0.15600]\n",
      "   [epoch: 0, batch: 4889600/4944481, loss: 0.15893]\n",
      "   [epoch: 0, batch: 4915200/4944481, loss: 0.17121]\n",
      "   [epoch: 0, batch: 4940800/4944481, loss: 0.16742]\n",
      "   Validation Accuracy: 94.6498552022527 %\n",
      "   best updated!\n",
      "   [epoch: 1, batch:     0/4944481, loss: 0.16998]\n",
      "   [epoch: 1, batch: 25600/4944481, loss: 0.19823]\n",
      "   [epoch: 1, batch: 51200/4944481, loss: 0.15357]\n",
      "   [epoch: 1, batch: 76800/4944481, loss: 0.15828]\n",
      "   [epoch: 1, batch: 102400/4944481, loss: 0.16318]\n",
      "   [epoch: 1, batch: 128000/4944481, loss: 0.16016]\n",
      "   [epoch: 1, batch: 153600/4944481, loss: 0.15412]\n",
      "   [epoch: 1, batch: 179200/4944481, loss: 0.16307]\n",
      "   [epoch: 1, batch: 204800/4944481, loss: 0.16285]\n",
      "   [epoch: 1, batch: 230400/4944481, loss: 0.15836]\n",
      "   [epoch: 1, batch: 256000/4944481, loss: 0.16651]\n",
      "   [epoch: 1, batch: 281600/4944481, loss: 0.26277]\n",
      "   [epoch: 1, batch: 307200/4944481, loss: 0.18380]\n",
      "   [epoch: 1, batch: 332800/4944481, loss: 0.16967]\n",
      "   [epoch: 1, batch: 358400/4944481, loss: 0.17152]\n",
      "   [epoch: 1, batch: 384000/4944481, loss: 0.17576]\n",
      "   [epoch: 1, batch: 409600/4944481, loss: 0.17159]\n",
      "   [epoch: 1, batch: 435200/4944481, loss: 0.17567]\n",
      "   [epoch: 1, batch: 460800/4944481, loss: 0.16882]\n",
      "   [epoch: 1, batch: 486400/4944481, loss: 0.16231]\n",
      "   [epoch: 1, batch: 512000/4944481, loss: 0.16409]\n",
      "   [epoch: 1, batch: 537600/4944481, loss: 0.16124]\n",
      "   [epoch: 1, batch: 563200/4944481, loss: 0.16441]\n",
      "   [epoch: 1, batch: 588800/4944481, loss: 0.16297]\n",
      "   [epoch: 1, batch: 614400/4944481, loss: 0.15641]\n",
      "   [epoch: 1, batch: 640000/4944481, loss: 0.15502]\n",
      "   [epoch: 1, batch: 665600/4944481, loss: 0.15153]\n",
      "   [epoch: 1, batch: 691200/4944481, loss: 0.15278]\n",
      "   [epoch: 1, batch: 716800/4944481, loss: 0.15895]\n",
      "   [epoch: 1, batch: 742400/4944481, loss: 0.15169]\n",
      "   [epoch: 1, batch: 768000/4944481, loss: 0.16208]\n",
      "   [epoch: 1, batch: 793600/4944481, loss: 0.15721]\n",
      "   [epoch: 1, batch: 819200/4944481, loss: 0.17981]\n",
      "   [epoch: 1, batch: 844800/4944481, loss: 0.15727]\n",
      "   [epoch: 1, batch: 870400/4944481, loss: 0.17518]\n",
      "   [epoch: 1, batch: 896000/4944481, loss: 0.18974]\n",
      "   [epoch: 1, batch: 921600/4944481, loss: 0.16988]\n",
      "   [epoch: 1, batch: 947200/4944481, loss: 0.16834]\n",
      "   [epoch: 1, batch: 972800/4944481, loss: 0.16107]\n",
      "   [epoch: 1, batch: 998400/4944481, loss: 0.14592]\n",
      "   [epoch: 1, batch: 1024000/4944481, loss: 0.15966]\n",
      "   [epoch: 1, batch: 1049600/4944481, loss: 0.15216]\n",
      "   [epoch: 1, batch: 1075200/4944481, loss: 0.16030]\n",
      "   [epoch: 1, batch: 1100800/4944481, loss: 0.16426]\n",
      "   [epoch: 1, batch: 1126400/4944481, loss: 0.15607]\n",
      "   [epoch: 1, batch: 1152000/4944481, loss: 0.16583]\n",
      "   [epoch: 1, batch: 1177600/4944481, loss: 0.15503]\n",
      "   [epoch: 1, batch: 1203200/4944481, loss: 0.15511]\n",
      "   [epoch: 1, batch: 1228800/4944481, loss: 0.15317]\n",
      "   [epoch: 1, batch: 1254400/4944481, loss: 0.15712]\n",
      "   [epoch: 1, batch: 1280000/4944481, loss: 0.14797]\n",
      "   [epoch: 1, batch: 1305600/4944481, loss: 0.21109]\n",
      "   [epoch: 1, batch: 1331200/4944481, loss: 0.16745]\n",
      "   [epoch: 1, batch: 1356800/4944481, loss: 0.16168]\n",
      "   [epoch: 1, batch: 1382400/4944481, loss: 0.15652]\n",
      "   [epoch: 1, batch: 1408000/4944481, loss: 0.14884]\n",
      "   [epoch: 1, batch: 1433600/4944481, loss: 0.15840]\n",
      "   [epoch: 1, batch: 1459200/4944481, loss: 0.13944]\n",
      "   [epoch: 1, batch: 1484800/4944481, loss: 0.15531]\n",
      "   [epoch: 1, batch: 1510400/4944481, loss: 0.15276]\n",
      "   [epoch: 1, batch: 1536000/4944481, loss: 0.14781]\n",
      "   [epoch: 1, batch: 1561600/4944481, loss: 0.14985]\n",
      "   [epoch: 1, batch: 1587200/4944481, loss: 0.14456]\n",
      "   [epoch: 1, batch: 1612800/4944481, loss: 0.14983]\n",
      "   [epoch: 1, batch: 1638400/4944481, loss: 0.14383]\n",
      "   [epoch: 1, batch: 1664000/4944481, loss: 0.14331]\n",
      "   [epoch: 1, batch: 1689600/4944481, loss: 0.16324]\n",
      "   [epoch: 1, batch: 1715200/4944481, loss: 0.15017]\n",
      "   [epoch: 1, batch: 1740800/4944481, loss: 0.14334]\n",
      "   [epoch: 1, batch: 1766400/4944481, loss: 0.15643]\n",
      "   [epoch: 1, batch: 1792000/4944481, loss: 0.14821]\n",
      "   [epoch: 1, batch: 1817600/4944481, loss: 0.14916]\n",
      "   [epoch: 1, batch: 1843200/4944481, loss: 0.14775]\n",
      "   [epoch: 1, batch: 1868800/4944481, loss: 0.15635]\n",
      "   [epoch: 1, batch: 1894400/4944481, loss: 0.15853]\n",
      "   [epoch: 1, batch: 1920000/4944481, loss: 0.14564]\n",
      "   [epoch: 1, batch: 1945600/4944481, loss: 0.18605]\n",
      "   [epoch: 1, batch: 1971200/4944481, loss: 0.15604]\n",
      "   [epoch: 1, batch: 1996800/4944481, loss: 0.15315]\n",
      "   [epoch: 1, batch: 2022400/4944481, loss: 0.14740]\n",
      "   [epoch: 1, batch: 2048000/4944481, loss: 0.16217]\n",
      "   [epoch: 1, batch: 2073600/4944481, loss: 0.14862]\n",
      "   [epoch: 1, batch: 2099200/4944481, loss: 0.15303]\n",
      "   [epoch: 1, batch: 2124800/4944481, loss: 0.15537]\n",
      "   [epoch: 1, batch: 2150400/4944481, loss: 0.18135]\n",
      "   [epoch: 1, batch: 2176000/4944481, loss: 0.14615]\n",
      "   [epoch: 1, batch: 2201600/4944481, loss: 0.15084]\n",
      "   [epoch: 1, batch: 2227200/4944481, loss: 0.14442]\n",
      "   [epoch: 1, batch: 2252800/4944481, loss: 0.17400]\n",
      "   [epoch: 1, batch: 2278400/4944481, loss: 0.16060]\n",
      "   [epoch: 1, batch: 2304000/4944481, loss: 0.15540]\n",
      "   [epoch: 1, batch: 2329600/4944481, loss: 0.16082]\n",
      "   [epoch: 1, batch: 2355200/4944481, loss: 0.17008]\n",
      "   [epoch: 1, batch: 2380800/4944481, loss: 0.19435]\n",
      "   [epoch: 1, batch: 2406400/4944481, loss: 0.15777]\n",
      "   [epoch: 1, batch: 2432000/4944481, loss: 0.15086]\n",
      "   [epoch: 1, batch: 2457600/4944481, loss: 0.15672]\n",
      "   [epoch: 1, batch: 2483200/4944481, loss: 0.14557]\n",
      "   [epoch: 1, batch: 2508800/4944481, loss: 0.14760]\n",
      "   [epoch: 1, batch: 2534400/4944481, loss: 0.14560]\n",
      "   [epoch: 1, batch: 2560000/4944481, loss: 0.14939]\n",
      "   [epoch: 1, batch: 2585600/4944481, loss: 0.14757]\n",
      "   [epoch: 1, batch: 2611200/4944481, loss: 0.14258]\n",
      "   [epoch: 1, batch: 2636800/4944481, loss: 0.14894]\n",
      "   [epoch: 1, batch: 2662400/4944481, loss: 0.14632]\n",
      "   [epoch: 1, batch: 2688000/4944481, loss: 0.14848]\n",
      "   [epoch: 1, batch: 2713600/4944481, loss: 0.14245]\n",
      "   [epoch: 1, batch: 2739200/4944481, loss: 0.14840]\n",
      "   [epoch: 1, batch: 2764800/4944481, loss: 0.14915]\n",
      "   [epoch: 1, batch: 2790400/4944481, loss: 0.14959]\n",
      "   [epoch: 1, batch: 2816000/4944481, loss: 0.14758]\n",
      "   [epoch: 1, batch: 2841600/4944481, loss: 0.14885]\n",
      "   [epoch: 1, batch: 2867200/4944481, loss: 0.14057]\n",
      "   [epoch: 1, batch: 2892800/4944481, loss: 0.21117]\n",
      "   [epoch: 1, batch: 2918400/4944481, loss: 0.18049]\n",
      "   [epoch: 1, batch: 2944000/4944481, loss: 0.15337]\n",
      "   [epoch: 1, batch: 2969600/4944481, loss: 0.16182]\n",
      "   [epoch: 1, batch: 2995200/4944481, loss: 0.15033]\n",
      "   [epoch: 1, batch: 3020800/4944481, loss: 0.26809]\n",
      "   [epoch: 1, batch: 3046400/4944481, loss: 0.24919]\n",
      "   [epoch: 1, batch: 3072000/4944481, loss: 0.17970]\n",
      "   [epoch: 1, batch: 3097600/4944481, loss: 0.16755]\n",
      "   [epoch: 1, batch: 3123200/4944481, loss: 0.16979]\n",
      "   [epoch: 1, batch: 3148800/4944481, loss: 0.15798]\n",
      "   [epoch: 1, batch: 3174400/4944481, loss: 0.15762]\n",
      "   [epoch: 1, batch: 3200000/4944481, loss: 0.15587]\n",
      "   [epoch: 1, batch: 3225600/4944481, loss: 0.15089]\n",
      "   [epoch: 1, batch: 3251200/4944481, loss: 0.15227]\n",
      "   [epoch: 1, batch: 3276800/4944481, loss: 0.15434]\n",
      "   [epoch: 1, batch: 3302400/4944481, loss: 0.14554]\n",
      "   [epoch: 1, batch: 3328000/4944481, loss: 0.14966]\n",
      "   [epoch: 1, batch: 3353600/4944481, loss: 0.15206]\n",
      "   [epoch: 1, batch: 3379200/4944481, loss: 0.14862]\n",
      "   [epoch: 1, batch: 3404800/4944481, loss: 0.14933]\n",
      "   [epoch: 1, batch: 3430400/4944481, loss: 0.15010]\n",
      "   [epoch: 1, batch: 3456000/4944481, loss: 0.14906]\n",
      "   [epoch: 1, batch: 3481600/4944481, loss: 0.14118]\n",
      "   [epoch: 1, batch: 3507200/4944481, loss: 0.14624]\n",
      "   [epoch: 1, batch: 3532800/4944481, loss: 0.16360]\n",
      "   [epoch: 1, batch: 3558400/4944481, loss: 0.16391]\n",
      "   [epoch: 1, batch: 3584000/4944481, loss: 0.16462]\n",
      "   [epoch: 1, batch: 3609600/4944481, loss: 0.14232]\n",
      "   [epoch: 1, batch: 3635200/4944481, loss: 0.14908]\n",
      "   [epoch: 1, batch: 3660800/4944481, loss: 0.15393]\n",
      "   [epoch: 1, batch: 3686400/4944481, loss: 0.15036]\n",
      "   [epoch: 1, batch: 3712000/4944481, loss: 0.14747]\n",
      "   [epoch: 1, batch: 3737600/4944481, loss: 0.15269]\n",
      "   [epoch: 1, batch: 3763200/4944481, loss: 0.14827]\n",
      "   [epoch: 1, batch: 3788800/4944481, loss: 0.14681]\n",
      "   [epoch: 1, batch: 3814400/4944481, loss: 0.17737]\n",
      "   [epoch: 1, batch: 3840000/4944481, loss: 0.16733]\n",
      "   [epoch: 1, batch: 3865600/4944481, loss: 0.16486]\n",
      "   [epoch: 1, batch: 3891200/4944481, loss: 0.16266]\n",
      "   [epoch: 1, batch: 3916800/4944481, loss: 0.17792]\n",
      "   [epoch: 1, batch: 3942400/4944481, loss: 0.15420]\n",
      "   [epoch: 1, batch: 3968000/4944481, loss: 0.15438]\n",
      "   [epoch: 1, batch: 3993600/4944481, loss: 0.15590]\n",
      "   [epoch: 1, batch: 4019200/4944481, loss: 0.23278]\n",
      "   [epoch: 1, batch: 4044800/4944481, loss: 0.20387]\n",
      "   [epoch: 1, batch: 4070400/4944481, loss: 0.16912]\n",
      "   [epoch: 1, batch: 4096000/4944481, loss: 0.16213]\n",
      "   [epoch: 1, batch: 4121600/4944481, loss: 0.16087]\n",
      "   [epoch: 1, batch: 4147200/4944481, loss: 0.16839]\n",
      "   [epoch: 1, batch: 4172800/4944481, loss: 0.15475]\n",
      "   [epoch: 1, batch: 4198400/4944481, loss: 0.15309]\n",
      "   [epoch: 1, batch: 4224000/4944481, loss: 0.16272]\n",
      "   [epoch: 1, batch: 4249600/4944481, loss: 0.15724]\n",
      "   [epoch: 1, batch: 4275200/4944481, loss: 0.15774]\n",
      "   [epoch: 1, batch: 4300800/4944481, loss: 0.15248]\n",
      "   [epoch: 1, batch: 4326400/4944481, loss: 0.14959]\n",
      "   [epoch: 1, batch: 4352000/4944481, loss: 0.14745]\n",
      "   [epoch: 1, batch: 4377600/4944481, loss: 0.15485]\n",
      "   [epoch: 1, batch: 4403200/4944481, loss: 0.15824]\n",
      "   [epoch: 1, batch: 4428800/4944481, loss: 0.15599]\n",
      "   [epoch: 1, batch: 4454400/4944481, loss: 0.15440]\n",
      "   [epoch: 1, batch: 4480000/4944481, loss: 0.21465]\n",
      "   [epoch: 1, batch: 4505600/4944481, loss: 0.21085]\n",
      "   [epoch: 1, batch: 4531200/4944481, loss: 0.18110]\n",
      "   [epoch: 1, batch: 4556800/4944481, loss: 0.17144]\n",
      "   [epoch: 1, batch: 4582400/4944481, loss: 0.16576]\n",
      "   [epoch: 1, batch: 4608000/4944481, loss: 0.16187]\n",
      "   [epoch: 1, batch: 4633600/4944481, loss: 0.16636]\n",
      "   [epoch: 1, batch: 4659200/4944481, loss: 0.15173]\n",
      "   [epoch: 1, batch: 4684800/4944481, loss: 0.15170]\n",
      "   [epoch: 1, batch: 4710400/4944481, loss: 0.15412]\n",
      "   [epoch: 1, batch: 4736000/4944481, loss: 0.14673]\n",
      "   [epoch: 1, batch: 4761600/4944481, loss: 0.15908]\n",
      "   [epoch: 1, batch: 4787200/4944481, loss: 0.15282]\n",
      "   [epoch: 1, batch: 4812800/4944481, loss: 0.15673]\n",
      "   [epoch: 1, batch: 4838400/4944481, loss: 0.14953]\n",
      "   [epoch: 1, batch: 4864000/4944481, loss: 0.16823]\n",
      "   [epoch: 1, batch: 4889600/4944481, loss: 0.15756]\n",
      "   [epoch: 1, batch: 4915200/4944481, loss: 0.15600]\n",
      "   [epoch: 1, batch: 4940800/4944481, loss: 0.14590]\n",
      "   Validation Accuracy: 94.97858522316692 %\n",
      "   best updated!\n",
      "   [epoch: 2, batch:     0/4944481, loss: 0.13324]\n",
      "   [epoch: 2, batch: 25600/4944481, loss: 0.14806]\n",
      "   [epoch: 2, batch: 51200/4944481, loss: 0.29998]\n",
      "   [epoch: 2, batch: 76800/4944481, loss: 0.25227]\n",
      "   [epoch: 2, batch: 102400/4944481, loss: 0.16379]\n",
      "   [epoch: 2, batch: 128000/4944481, loss: 0.16040]\n",
      "   [epoch: 2, batch: 153600/4944481, loss: 0.15563]\n",
      "   [epoch: 2, batch: 179200/4944481, loss: 0.15209]\n",
      "   [epoch: 2, batch: 204800/4944481, loss: 0.14765]\n",
      "   [epoch: 2, batch: 230400/4944481, loss: 0.14975]\n",
      "   [epoch: 2, batch: 256000/4944481, loss: 0.15974]\n",
      "   [epoch: 2, batch: 281600/4944481, loss: 0.16456]\n",
      "   [epoch: 2, batch: 307200/4944481, loss: 0.15730]\n",
      "   [epoch: 2, batch: 332800/4944481, loss: 0.14952]\n",
      "   [epoch: 2, batch: 358400/4944481, loss: 0.15346]\n",
      "   [epoch: 2, batch: 384000/4944481, loss: 0.14430]\n",
      "   [epoch: 2, batch: 409600/4944481, loss: 0.15349]\n",
      "   [epoch: 2, batch: 435200/4944481, loss: 0.16299]\n",
      "   [epoch: 2, batch: 460800/4944481, loss: 0.14752]\n",
      "   [epoch: 2, batch: 486400/4944481, loss: 0.14210]\n",
      "   [epoch: 2, batch: 512000/4944481, loss: 0.14543]\n",
      "   [epoch: 2, batch: 537600/4944481, loss: 0.14226]\n",
      "   [epoch: 2, batch: 563200/4944481, loss: 0.14388]\n",
      "   [epoch: 2, batch: 588800/4944481, loss: 0.14790]\n",
      "   [epoch: 2, batch: 614400/4944481, loss: 0.15580]\n",
      "   [epoch: 2, batch: 640000/4944481, loss: 0.13953]\n",
      "   [epoch: 2, batch: 665600/4944481, loss: 0.13955]\n",
      "   [epoch: 2, batch: 691200/4944481, loss: 0.14051]\n",
      "   [epoch: 2, batch: 716800/4944481, loss: 0.14232]\n",
      "   [epoch: 2, batch: 742400/4944481, loss: 0.15132]\n",
      "   [epoch: 2, batch: 768000/4944481, loss: 0.17012]\n",
      "   [epoch: 2, batch: 793600/4944481, loss: 0.15118]\n",
      "   [epoch: 2, batch: 819200/4944481, loss: 0.14684]\n",
      "   [epoch: 2, batch: 844800/4944481, loss: 0.14504]\n",
      "   [epoch: 2, batch: 870400/4944481, loss: 0.14743]\n",
      "   [epoch: 2, batch: 896000/4944481, loss: 0.13895]\n",
      "   [epoch: 2, batch: 921600/4944481, loss: 0.14011]\n",
      "   [epoch: 2, batch: 947200/4944481, loss: 0.14979]\n",
      "   [epoch: 2, batch: 972800/4944481, loss: 0.14454]\n",
      "   [epoch: 2, batch: 998400/4944481, loss: 0.14585]\n",
      "   [epoch: 2, batch: 1024000/4944481, loss: 0.14478]\n",
      "   [epoch: 2, batch: 1049600/4944481, loss: 0.14865]\n",
      "   [epoch: 2, batch: 1075200/4944481, loss: 0.14812]\n",
      "   [epoch: 2, batch: 1100800/4944481, loss: 0.14598]\n",
      "   [epoch: 2, batch: 1126400/4944481, loss: 0.14743]\n",
      "   [epoch: 2, batch: 1152000/4944481, loss: 0.14052]\n",
      "   [epoch: 2, batch: 1177600/4944481, loss: 0.15095]\n",
      "   [epoch: 2, batch: 1203200/4944481, loss: 0.14755]\n",
      "   [epoch: 2, batch: 1228800/4944481, loss: 0.13941]\n",
      "   [epoch: 2, batch: 1254400/4944481, loss: 0.14336]\n",
      "   [epoch: 2, batch: 1280000/4944481, loss: 0.13846]\n",
      "   [epoch: 2, batch: 1305600/4944481, loss: 0.14628]\n",
      "   [epoch: 2, batch: 1331200/4944481, loss: 0.17401]\n",
      "   [epoch: 2, batch: 1356800/4944481, loss: 0.16835]\n",
      "   [epoch: 2, batch: 1382400/4944481, loss: 0.14668]\n",
      "   [epoch: 2, batch: 1408000/4944481, loss: 0.14458]\n",
      "   [epoch: 2, batch: 1433600/4944481, loss: 0.17006]\n",
      "   [epoch: 2, batch: 1459200/4944481, loss: 0.17231]\n",
      "   [epoch: 2, batch: 1484800/4944481, loss: 0.15328]\n",
      "   [epoch: 2, batch: 1510400/4944481, loss: 0.14740]\n",
      "   [epoch: 2, batch: 1536000/4944481, loss: 0.14534]\n",
      "   [epoch: 2, batch: 1561600/4944481, loss: 0.14673]\n",
      "   [epoch: 2, batch: 1587200/4944481, loss: 0.14384]\n",
      "   [epoch: 2, batch: 1612800/4944481, loss: 0.14352]\n",
      "   [epoch: 2, batch: 1638400/4944481, loss: 0.14627]\n",
      "   [epoch: 2, batch: 1664000/4944481, loss: 0.14005]\n",
      "   [epoch: 2, batch: 1689600/4944481, loss: 0.14614]\n",
      "   [epoch: 2, batch: 1715200/4944481, loss: 0.14188]\n",
      "   [epoch: 2, batch: 1740800/4944481, loss: 0.15038]\n",
      "   [epoch: 2, batch: 1766400/4944481, loss: 0.14259]\n",
      "   [epoch: 2, batch: 1792000/4944481, loss: 0.14437]\n",
      "   [epoch: 2, batch: 1817600/4944481, loss: 0.13549]\n",
      "   [epoch: 2, batch: 1843200/4944481, loss: 0.13643]\n",
      "   [epoch: 2, batch: 1868800/4944481, loss: 0.13909]\n",
      "   [epoch: 2, batch: 1894400/4944481, loss: 0.14374]\n",
      "   [epoch: 2, batch: 1920000/4944481, loss: 0.14448]\n",
      "   [epoch: 2, batch: 1945600/4944481, loss: 0.23849]\n",
      "   [epoch: 2, batch: 1971200/4944481, loss: 0.30434]\n",
      "   [epoch: 2, batch: 1996800/4944481, loss: 0.20037]\n",
      "   [epoch: 2, batch: 2022400/4944481, loss: 0.22263]\n",
      "   [epoch: 2, batch: 2048000/4944481, loss: 0.18014]\n",
      "   [epoch: 2, batch: 2073600/4944481, loss: 0.17104]\n",
      "   [epoch: 2, batch: 2099200/4944481, loss: 0.16586]\n",
      "   [epoch: 2, batch: 2124800/4944481, loss: 0.16327]\n",
      "   [epoch: 2, batch: 2150400/4944481, loss: 0.16482]\n",
      "   [epoch: 2, batch: 2176000/4944481, loss: 0.16572]\n",
      "   [epoch: 2, batch: 2201600/4944481, loss: 0.17047]\n",
      "   [epoch: 2, batch: 2227200/4944481, loss: 0.18751]\n",
      "   [epoch: 2, batch: 2252800/4944481, loss: 0.16739]\n",
      "   [epoch: 2, batch: 2278400/4944481, loss: 0.16908]\n",
      "   [epoch: 2, batch: 2304000/4944481, loss: 0.16266]\n",
      "   [epoch: 2, batch: 2329600/4944481, loss: 0.16594]\n",
      "   [epoch: 2, batch: 2355200/4944481, loss: 0.16566]\n",
      "   [epoch: 2, batch: 2380800/4944481, loss: 0.16312]\n",
      "   [epoch: 2, batch: 2406400/4944481, loss: 0.16806]\n",
      "   [epoch: 2, batch: 2432000/4944481, loss: 0.15565]\n",
      "   [epoch: 2, batch: 2457600/4944481, loss: 0.17402]\n",
      "   [epoch: 2, batch: 2483200/4944481, loss: 0.15301]\n",
      "   [epoch: 2, batch: 2508800/4944481, loss: 0.16053]\n",
      "   [epoch: 2, batch: 2534400/4944481, loss: 0.16665]\n",
      "   [epoch: 2, batch: 2560000/4944481, loss: 0.19832]\n",
      "   [epoch: 2, batch: 2585600/4944481, loss: 0.16735]\n",
      "   [epoch: 2, batch: 2611200/4944481, loss: 0.16408]\n",
      "   [epoch: 2, batch: 2636800/4944481, loss: 0.16095]\n",
      "   [epoch: 2, batch: 2662400/4944481, loss: 0.16064]\n",
      "   [epoch: 2, batch: 2688000/4944481, loss: 0.15499]\n",
      "   [epoch: 2, batch: 2713600/4944481, loss: 0.17700]\n",
      "   [epoch: 2, batch: 2739200/4944481, loss: 0.16711]\n",
      "   [epoch: 2, batch: 2764800/4944481, loss: 0.15712]\n",
      "   [epoch: 2, batch: 2790400/4944481, loss: 0.16661]\n",
      "   [epoch: 2, batch: 2816000/4944481, loss: 0.15615]\n",
      "   [epoch: 2, batch: 2841600/4944481, loss: 0.15924]\n",
      "   [epoch: 2, batch: 2867200/4944481, loss: 0.18980]\n",
      "   [epoch: 2, batch: 2892800/4944481, loss: 0.15821]\n",
      "   [epoch: 2, batch: 2918400/4944481, loss: 0.17768]\n",
      "   [epoch: 2, batch: 2944000/4944481, loss: 0.15607]\n",
      "   [epoch: 2, batch: 2969600/4944481, loss: 0.15291]\n",
      "   [epoch: 2, batch: 2995200/4944481, loss: 0.15578]\n",
      "   [epoch: 2, batch: 3020800/4944481, loss: 0.15860]\n",
      "   [epoch: 2, batch: 3046400/4944481, loss: 0.15686]\n",
      "   [epoch: 2, batch: 3072000/4944481, loss: 0.14896]\n",
      "   [epoch: 2, batch: 3097600/4944481, loss: 0.15848]\n",
      "   [epoch: 2, batch: 3123200/4944481, loss: 0.16838]\n",
      "   [epoch: 2, batch: 3148800/4944481, loss: 0.15670]\n",
      "   [epoch: 2, batch: 3174400/4944481, loss: 0.15389]\n",
      "   [epoch: 2, batch: 3200000/4944481, loss: 0.16284]\n",
      "   [epoch: 2, batch: 3225600/4944481, loss: 0.15627]\n",
      "   [epoch: 2, batch: 3251200/4944481, loss: 0.15231]\n",
      "   [epoch: 2, batch: 3276800/4944481, loss: 0.16287]\n",
      "   [epoch: 2, batch: 3302400/4944481, loss: 0.15268]\n",
      "   [epoch: 2, batch: 3328000/4944481, loss: 0.15403]\n",
      "   [epoch: 2, batch: 3353600/4944481, loss: 0.15652]\n",
      "   [epoch: 2, batch: 3379200/4944481, loss: 0.15575]\n",
      "   [epoch: 2, batch: 3404800/4944481, loss: 0.15915]\n",
      "   [epoch: 2, batch: 3430400/4944481, loss: 0.17168]\n",
      "   [epoch: 2, batch: 3456000/4944481, loss: 0.15685]\n",
      "   [epoch: 2, batch: 3481600/4944481, loss: 0.15650]\n",
      "   [epoch: 2, batch: 3507200/4944481, loss: 0.16097]\n",
      "   [epoch: 2, batch: 3532800/4944481, loss: 0.18654]\n",
      "   [epoch: 2, batch: 3558400/4944481, loss: 0.16150]\n",
      "   [epoch: 2, batch: 3584000/4944481, loss: 0.16723]\n",
      "   [epoch: 2, batch: 3609600/4944481, loss: 0.15130]\n",
      "   [epoch: 2, batch: 3635200/4944481, loss: 0.15953]\n",
      "   [epoch: 2, batch: 3660800/4944481, loss: 0.16338]\n",
      "   [epoch: 2, batch: 3686400/4944481, loss: 0.15777]\n",
      "   [epoch: 2, batch: 3712000/4944481, loss: 0.17799]\n",
      "   [epoch: 2, batch: 3737600/4944481, loss: 0.15633]\n",
      "   [epoch: 2, batch: 3763200/4944481, loss: 0.15171]\n",
      "   [epoch: 2, batch: 3788800/4944481, loss: 0.15261]\n",
      "   [epoch: 2, batch: 3814400/4944481, loss: 0.15298]\n",
      "   [epoch: 2, batch: 3840000/4944481, loss: 0.14427]\n",
      "   [epoch: 2, batch: 3865600/4944481, loss: 0.16322]\n",
      "   [epoch: 2, batch: 3891200/4944481, loss: 0.14437]\n",
      "   [epoch: 2, batch: 3916800/4944481, loss: 0.16128]\n",
      "   [epoch: 2, batch: 3942400/4944481, loss: 0.14914]\n",
      "   [epoch: 2, batch: 3968000/4944481, loss: 0.14055]\n",
      "   [epoch: 2, batch: 3993600/4944481, loss: 0.15217]\n",
      "   [epoch: 2, batch: 4019200/4944481, loss: 0.19677]\n",
      "   [epoch: 2, batch: 4044800/4944481, loss: 0.16884]\n",
      "   [epoch: 2, batch: 4070400/4944481, loss: 0.15864]\n",
      "   [epoch: 2, batch: 4096000/4944481, loss: 0.15237]\n",
      "   [epoch: 2, batch: 4121600/4944481, loss: 0.19414]\n",
      "   [epoch: 2, batch: 4147200/4944481, loss: 0.18892]\n",
      "   [epoch: 2, batch: 4172800/4944481, loss: 0.16313]\n",
      "   [epoch: 2, batch: 4198400/4944481, loss: 0.16580]\n",
      "   [epoch: 2, batch: 4224000/4944481, loss: 0.15396]\n",
      "   [epoch: 2, batch: 4249600/4944481, loss: 0.15942]\n",
      "   [epoch: 2, batch: 4275200/4944481, loss: 0.15447]\n",
      "   [epoch: 2, batch: 4300800/4944481, loss: 0.16228]\n",
      "   [epoch: 2, batch: 4326400/4944481, loss: 0.14999]\n",
      "   [epoch: 2, batch: 4352000/4944481, loss: 0.15906]\n",
      "   [epoch: 2, batch: 4377600/4944481, loss: 0.15669]\n",
      "   [epoch: 2, batch: 4403200/4944481, loss: 0.15679]\n",
      "   [epoch: 2, batch: 4428800/4944481, loss: 0.14918]\n",
      "   [epoch: 2, batch: 4454400/4944481, loss: 0.15011]\n",
      "   [epoch: 2, batch: 4480000/4944481, loss: 0.15754]\n",
      "   [epoch: 2, batch: 4505600/4944481, loss: 0.15164]\n",
      "   [epoch: 2, batch: 4531200/4944481, loss: 0.15997]\n",
      "   [epoch: 2, batch: 4556800/4944481, loss: 0.15500]\n",
      "   [epoch: 2, batch: 4582400/4944481, loss: 0.15637]\n",
      "   [epoch: 2, batch: 4608000/4944481, loss: 0.16064]\n",
      "   [epoch: 2, batch: 4633600/4944481, loss: 0.14811]\n",
      "   [epoch: 2, batch: 4659200/4944481, loss: 0.15209]\n",
      "   [epoch: 2, batch: 4684800/4944481, loss: 0.14605]\n",
      "   [epoch: 2, batch: 4710400/4944481, loss: 0.14774]\n",
      "   [epoch: 2, batch: 4736000/4944481, loss: 0.14513]\n",
      "   [epoch: 2, batch: 4761600/4944481, loss: 0.14324]\n",
      "   [epoch: 2, batch: 4787200/4944481, loss: 0.18509]\n",
      "   [epoch: 2, batch: 4812800/4944481, loss: 0.15590]\n",
      "   [epoch: 2, batch: 4838400/4944481, loss: 0.16718]\n",
      "   [epoch: 2, batch: 4864000/4944481, loss: 0.16439]\n",
      "   [epoch: 2, batch: 4889600/4944481, loss: 0.14441]\n",
      "   [epoch: 2, batch: 4915200/4944481, loss: 0.14172]\n",
      "   [epoch: 2, batch: 4940800/4944481, loss: 0.14755]\n",
      "   Validation Accuracy: 94.95856290738587 %\n",
      "   [epoch: 3, batch:     0/4944481, loss: 0.17356]\n",
      "   [epoch: 3, batch: 25600/4944481, loss: 0.14605]\n",
      "   [epoch: 3, batch: 51200/4944481, loss: 0.14008]\n",
      "   [epoch: 3, batch: 76800/4944481, loss: 0.13507]\n",
      "   [epoch: 3, batch: 102400/4944481, loss: 0.17293]\n",
      "   [epoch: 3, batch: 128000/4944481, loss: 0.15508]\n",
      "   [epoch: 3, batch: 153600/4944481, loss: 0.15038]\n",
      "   [epoch: 3, batch: 179200/4944481, loss: 0.15130]\n",
      "   [epoch: 3, batch: 204800/4944481, loss: 0.14162]\n",
      "   [epoch: 3, batch: 230400/4944481, loss: 0.17405]\n",
      "   [epoch: 3, batch: 256000/4944481, loss: 0.14490]\n",
      "   [epoch: 3, batch: 281600/4944481, loss: 0.13985]\n",
      "   [epoch: 3, batch: 307200/4944481, loss: 0.14780]\n",
      "   [epoch: 3, batch: 332800/4944481, loss: 0.14578]\n",
      "   [epoch: 3, batch: 358400/4944481, loss: 0.15134]\n",
      "   [epoch: 3, batch: 384000/4944481, loss: 0.14874]\n",
      "   [epoch: 3, batch: 409600/4944481, loss: 0.14778]\n",
      "   [epoch: 3, batch: 435200/4944481, loss: 0.15098]\n",
      "   [epoch: 3, batch: 460800/4944481, loss: 0.14133]\n",
      "   [epoch: 3, batch: 486400/4944481, loss: 0.14039]\n",
      "   [epoch: 3, batch: 512000/4944481, loss: 0.14289]\n",
      "   [epoch: 3, batch: 537600/4944481, loss: 0.14219]\n",
      "   [epoch: 3, batch: 563200/4944481, loss: 0.14699]\n",
      "   [epoch: 3, batch: 588800/4944481, loss: 0.16508]\n",
      "   [epoch: 3, batch: 614400/4944481, loss: 0.14314]\n",
      "   [epoch: 3, batch: 640000/4944481, loss: 0.17638]\n",
      "   [epoch: 3, batch: 665600/4944481, loss: 0.14987]\n",
      "   [epoch: 3, batch: 691200/4944481, loss: 0.14401]\n",
      "   [epoch: 3, batch: 716800/4944481, loss: 0.14910]\n",
      "   [epoch: 3, batch: 742400/4944481, loss: 0.14260]\n",
      "   [epoch: 3, batch: 768000/4944481, loss: 0.14338]\n",
      "   [epoch: 3, batch: 793600/4944481, loss: 0.14546]\n",
      "   [epoch: 3, batch: 819200/4944481, loss: 0.16654]\n",
      "   [epoch: 3, batch: 844800/4944481, loss: 0.15551]\n",
      "   [epoch: 3, batch: 870400/4944481, loss: 0.16205]\n",
      "   [epoch: 3, batch: 896000/4944481, loss: 0.15761]\n",
      "   [epoch: 3, batch: 921600/4944481, loss: 0.15808]\n",
      "   [epoch: 3, batch: 947200/4944481, loss: 0.15146]\n",
      "   [epoch: 3, batch: 972800/4944481, loss: 0.14030]\n",
      "   [epoch: 3, batch: 998400/4944481, loss: 0.14284]\n",
      "   [epoch: 3, batch: 1024000/4944481, loss: 0.14903]\n",
      "   [epoch: 3, batch: 1049600/4944481, loss: 0.14830]\n",
      "   [epoch: 3, batch: 1075200/4944481, loss: 0.14862]\n",
      "   [epoch: 3, batch: 1100800/4944481, loss: 0.14483]\n",
      "   [epoch: 3, batch: 1126400/4944481, loss: 0.13867]\n",
      "   [epoch: 3, batch: 1152000/4944481, loss: 0.14707]\n",
      "   [epoch: 3, batch: 1177600/4944481, loss: 0.13958]\n",
      "   [epoch: 3, batch: 1203200/4944481, loss: 0.14566]\n",
      "   [epoch: 3, batch: 1228800/4944481, loss: 0.15959]\n",
      "   [epoch: 3, batch: 1254400/4944481, loss: 0.14775]\n",
      "   [epoch: 3, batch: 1280000/4944481, loss: 0.15719]\n",
      "   [epoch: 3, batch: 1305600/4944481, loss: 0.14820]\n",
      "   [epoch: 3, batch: 1331200/4944481, loss: 0.14785]\n",
      "   [epoch: 3, batch: 1356800/4944481, loss: 0.14346]\n",
      "   [epoch: 3, batch: 1382400/4944481, loss: 0.14120]\n",
      "   [epoch: 3, batch: 1408000/4944481, loss: 0.14116]\n",
      "   [epoch: 3, batch: 1433600/4944481, loss: 0.14409]\n",
      "   [epoch: 3, batch: 1459200/4944481, loss: 0.14017]\n",
      "   [epoch: 3, batch: 1484800/4944481, loss: 0.17142]\n",
      "   [epoch: 3, batch: 1510400/4944481, loss: 0.15404]\n",
      "   [epoch: 3, batch: 1536000/4944481, loss: 0.15560]\n",
      "   [epoch: 3, batch: 1561600/4944481, loss: 0.15143]\n",
      "   [epoch: 3, batch: 1587200/4944481, loss: 0.17289]\n",
      "   [epoch: 3, batch: 1612800/4944481, loss: 0.15280]\n",
      "   [epoch: 3, batch: 1638400/4944481, loss: 0.15100]\n",
      "   [epoch: 3, batch: 1664000/4944481, loss: 0.15311]\n",
      "   [epoch: 3, batch: 1689600/4944481, loss: 0.14451]\n",
      "   [epoch: 3, batch: 1715200/4944481, loss: 0.14322]\n",
      "   [epoch: 3, batch: 1740800/4944481, loss: 0.14075]\n",
      "   [epoch: 3, batch: 1766400/4944481, loss: 0.16357]\n",
      "   [epoch: 3, batch: 1792000/4944481, loss: 0.15694]\n",
      "   [epoch: 3, batch: 1817600/4944481, loss: 0.14291]\n",
      "   [epoch: 3, batch: 1843200/4944481, loss: 0.14116]\n",
      "   [epoch: 3, batch: 1868800/4944481, loss: 0.14202]\n",
      "   [epoch: 3, batch: 1894400/4944481, loss: 0.13826]\n",
      "   [epoch: 3, batch: 1920000/4944481, loss: 0.14244]\n",
      "   [epoch: 3, batch: 1945600/4944481, loss: 0.14282]\n",
      "   [epoch: 3, batch: 1971200/4944481, loss: 0.14210]\n",
      "   [epoch: 3, batch: 1996800/4944481, loss: 0.16783]\n",
      "   [epoch: 3, batch: 2022400/4944481, loss: 0.14760]\n",
      "   [epoch: 3, batch: 2048000/4944481, loss: 0.14530]\n",
      "   [epoch: 3, batch: 2073600/4944481, loss: 0.14536]\n",
      "   [epoch: 3, batch: 2099200/4944481, loss: 0.13773]\n",
      "   [epoch: 3, batch: 2124800/4944481, loss: 0.14543]\n",
      "   [epoch: 3, batch: 2150400/4944481, loss: 0.14088]\n",
      "   [epoch: 3, batch: 2176000/4944481, loss: 0.27754]\n",
      "   [epoch: 3, batch: 2201600/4944481, loss: 0.20778]\n",
      "   [epoch: 3, batch: 2227200/4944481, loss: 0.16446]\n",
      "   [epoch: 3, batch: 2252800/4944481, loss: 0.16743]\n",
      "   [epoch: 3, batch: 2278400/4944481, loss: 0.15688]\n",
      "   [epoch: 3, batch: 2304000/4944481, loss: 0.16011]\n",
      "   [epoch: 3, batch: 2329600/4944481, loss: 0.15780]\n",
      "   [epoch: 3, batch: 2355200/4944481, loss: 0.15206]\n",
      "   [epoch: 3, batch: 2380800/4944481, loss: 0.15281]\n",
      "   [epoch: 3, batch: 2406400/4944481, loss: 0.14573]\n",
      "   [epoch: 3, batch: 2432000/4944481, loss: 0.14901]\n",
      "   [epoch: 3, batch: 2457600/4944481, loss: 0.15667]\n",
      "   [epoch: 3, batch: 2483200/4944481, loss: 0.14812]\n",
      "   [epoch: 3, batch: 2508800/4944481, loss: 0.16838]\n",
      "   [epoch: 3, batch: 2534400/4944481, loss: 0.15714]\n",
      "   [epoch: 3, batch: 2560000/4944481, loss: 0.14679]\n",
      "   [epoch: 3, batch: 2585600/4944481, loss: 0.14702]\n",
      "   [epoch: 3, batch: 2611200/4944481, loss: 0.14820]\n",
      "   [epoch: 3, batch: 2636800/4944481, loss: 0.14618]\n",
      "   [epoch: 3, batch: 2662400/4944481, loss: 0.14694]\n",
      "   [epoch: 3, batch: 2688000/4944481, loss: 0.14443]\n",
      "   [epoch: 3, batch: 2713600/4944481, loss: 0.14167]\n",
      "   [epoch: 3, batch: 2739200/4944481, loss: 0.14618]\n",
      "   [epoch: 3, batch: 2764800/4944481, loss: 0.14262]\n",
      "   [epoch: 3, batch: 2790400/4944481, loss: 0.14853]\n",
      "   [epoch: 3, batch: 2816000/4944481, loss: 0.14328]\n",
      "   [epoch: 3, batch: 2841600/4944481, loss: 0.16408]\n",
      "   [epoch: 3, batch: 2867200/4944481, loss: 0.15143]\n",
      "   [epoch: 3, batch: 2892800/4944481, loss: 0.14625]\n",
      "   [epoch: 3, batch: 2918400/4944481, loss: 0.15503]\n",
      "   [epoch: 3, batch: 2944000/4944481, loss: 0.18084]\n",
      "   [epoch: 3, batch: 2969600/4944481, loss: 0.17245]\n",
      "   [epoch: 3, batch: 2995200/4944481, loss: 0.20904]\n",
      "   [epoch: 3, batch: 3020800/4944481, loss: 0.15968]\n",
      "   [epoch: 3, batch: 3046400/4944481, loss: 0.15455]\n",
      "   [epoch: 3, batch: 3072000/4944481, loss: 0.15341]\n",
      "   [epoch: 3, batch: 3097600/4944481, loss: 0.15097]\n",
      "   [epoch: 3, batch: 3123200/4944481, loss: 0.14361]\n",
      "   [epoch: 3, batch: 3148800/4944481, loss: 0.15200]\n",
      "   [epoch: 3, batch: 3174400/4944481, loss: 0.26211]\n",
      "   [epoch: 3, batch: 3200000/4944481, loss: 0.17236]\n",
      "   [epoch: 3, batch: 3225600/4944481, loss: 0.15736]\n",
      "   [epoch: 3, batch: 3251200/4944481, loss: 0.14377]\n",
      "   [epoch: 3, batch: 3276800/4944481, loss: 0.15214]\n",
      "   [epoch: 3, batch: 3302400/4944481, loss: 0.15062]\n",
      "   [epoch: 3, batch: 3328000/4944481, loss: 0.15077]\n",
      "   [epoch: 3, batch: 3353600/4944481, loss: 0.15362]\n",
      "   [epoch: 3, batch: 3379200/4944481, loss: 0.15163]\n",
      "   [epoch: 3, batch: 3404800/4944481, loss: 0.14500]\n",
      "   [epoch: 3, batch: 3430400/4944481, loss: 0.14278]\n",
      "   [epoch: 3, batch: 3456000/4944481, loss: 0.14418]\n",
      "   [epoch: 3, batch: 3481600/4944481, loss: 0.14252]\n",
      "   [epoch: 3, batch: 3507200/4944481, loss: 0.15485]\n",
      "   [epoch: 3, batch: 3532800/4944481, loss: 0.15231]\n",
      "   [epoch: 3, batch: 3558400/4944481, loss: 0.14970]\n",
      "   [epoch: 3, batch: 3584000/4944481, loss: 0.14490]\n",
      "   [epoch: 3, batch: 3609600/4944481, loss: 0.15379]\n",
      "   [epoch: 3, batch: 3635200/4944481, loss: 0.16089]\n",
      "   [epoch: 3, batch: 3660800/4944481, loss: 0.14766]\n",
      "   [epoch: 3, batch: 3686400/4944481, loss: 0.13828]\n",
      "   [epoch: 3, batch: 3712000/4944481, loss: 0.14047]\n",
      "   [epoch: 3, batch: 3737600/4944481, loss: 0.14396]\n",
      "   [epoch: 3, batch: 3763200/4944481, loss: 0.14349]\n",
      "   [epoch: 3, batch: 3788800/4944481, loss: 0.14577]\n",
      "   [epoch: 3, batch: 3814400/4944481, loss: 0.14957]\n",
      "   [epoch: 3, batch: 3840000/4944481, loss: 0.15682]\n",
      "   [epoch: 3, batch: 3865600/4944481, loss: 0.15207]\n",
      "   [epoch: 3, batch: 3891200/4944481, loss: 0.14234]\n",
      "   [epoch: 3, batch: 3916800/4944481, loss: 0.14650]\n",
      "   [epoch: 3, batch: 3942400/4944481, loss: 0.14470]\n",
      "   [epoch: 3, batch: 3968000/4944481, loss: 0.14253]\n",
      "   [epoch: 3, batch: 3993600/4944481, loss: 0.14049]\n",
      "   [epoch: 3, batch: 4019200/4944481, loss: 0.16700]\n",
      "   [epoch: 3, batch: 4044800/4944481, loss: 0.14802]\n",
      "   [epoch: 3, batch: 4070400/4944481, loss: 0.14860]\n",
      "   [epoch: 3, batch: 4096000/4944481, loss: 0.14748]\n",
      "   [epoch: 3, batch: 4121600/4944481, loss: 0.14200]\n",
      "   [epoch: 3, batch: 4147200/4944481, loss: 0.14879]\n",
      "   [epoch: 3, batch: 4172800/4944481, loss: 0.14337]\n",
      "   [epoch: 3, batch: 4198400/4944481, loss: 0.13919]\n",
      "   [epoch: 3, batch: 4224000/4944481, loss: 0.14652]\n",
      "   [epoch: 3, batch: 4249600/4944481, loss: 0.13688]\n",
      "   [epoch: 3, batch: 4275200/4944481, loss: 0.14438]\n",
      "   [epoch: 3, batch: 4300800/4944481, loss: 0.14177]\n",
      "   [epoch: 3, batch: 4326400/4944481, loss: 0.14147]\n",
      "   [epoch: 3, batch: 4352000/4944481, loss: 0.14872]\n",
      "   [epoch: 3, batch: 4377600/4944481, loss: 0.14763]\n",
      "   [epoch: 3, batch: 4403200/4944481, loss: 0.19775]\n",
      "   [epoch: 3, batch: 4428800/4944481, loss: 0.16267]\n",
      "   [epoch: 3, batch: 4454400/4944481, loss: 0.15248]\n",
      "   [epoch: 3, batch: 4480000/4944481, loss: 0.14402]\n",
      "   [epoch: 3, batch: 4505600/4944481, loss: 0.14369]\n",
      "   [epoch: 3, batch: 4531200/4944481, loss: 0.15032]\n",
      "   [epoch: 3, batch: 4556800/4944481, loss: 0.14448]\n",
      "   [epoch: 3, batch: 4582400/4944481, loss: 0.14545]\n",
      "   [epoch: 3, batch: 4608000/4944481, loss: 0.15814]\n",
      "   [epoch: 3, batch: 4633600/4944481, loss: 0.14339]\n",
      "   [epoch: 3, batch: 4659200/4944481, loss: 0.14337]\n",
      "   [epoch: 3, batch: 4684800/4944481, loss: 0.13628]\n",
      "   [epoch: 3, batch: 4710400/4944481, loss: 0.14225]\n",
      "   [epoch: 3, batch: 4736000/4944481, loss: 0.13947]\n",
      "   [epoch: 3, batch: 4761600/4944481, loss: 0.14428]\n",
      "   [epoch: 3, batch: 4787200/4944481, loss: 0.13340]\n",
      "   [epoch: 3, batch: 4812800/4944481, loss: 0.13968]\n",
      "   [epoch: 3, batch: 4838400/4944481, loss: 0.14596]\n",
      "   [epoch: 3, batch: 4864000/4944481, loss: 0.14155]\n",
      "   [epoch: 3, batch: 4889600/4944481, loss: 0.13666]\n",
      "   [epoch: 3, batch: 4915200/4944481, loss: 0.14116]\n",
      "   [epoch: 3, batch: 4940800/4944481, loss: 0.14037]\n",
      "   Validation Accuracy: 95.32679149670452 %\n",
      "   best updated!\n",
      "   [epoch: 4, batch:     0/4944481, loss: 0.14439]\n",
      "   [epoch: 4, batch: 25600/4944481, loss: 0.14040]\n",
      "   [epoch: 4, batch: 51200/4944481, loss: 0.13920]\n",
      "   [epoch: 4, batch: 76800/4944481, loss: 0.19868]\n",
      "   [epoch: 4, batch: 102400/4944481, loss: 0.14511]\n",
      "   [epoch: 4, batch: 128000/4944481, loss: 0.14844]\n",
      "   [epoch: 4, batch: 153600/4944481, loss: 0.14785]\n",
      "   [epoch: 4, batch: 179200/4944481, loss: 0.15121]\n",
      "   [epoch: 4, batch: 204800/4944481, loss: 0.14962]\n",
      "   [epoch: 4, batch: 230400/4944481, loss: 0.14491]\n",
      "   [epoch: 4, batch: 256000/4944481, loss: 0.15036]\n",
      "   [epoch: 4, batch: 281600/4944481, loss: 0.14667]\n",
      "   [epoch: 4, batch: 307200/4944481, loss: 0.15073]\n",
      "   [epoch: 4, batch: 332800/4944481, loss: 0.14279]\n",
      "   [epoch: 4, batch: 358400/4944481, loss: 0.14194]\n",
      "   [epoch: 4, batch: 384000/4944481, loss: 0.13967]\n",
      "   [epoch: 4, batch: 409600/4944481, loss: 0.13786]\n",
      "   [epoch: 4, batch: 435200/4944481, loss: 0.14361]\n",
      "   [epoch: 4, batch: 460800/4944481, loss: 0.14574]\n",
      "   [epoch: 4, batch: 486400/4944481, loss: 0.14370]\n",
      "   [epoch: 4, batch: 512000/4944481, loss: 0.14615]\n",
      "   [epoch: 4, batch: 537600/4944481, loss: 0.14053]\n",
      "   [epoch: 4, batch: 563200/4944481, loss: 0.14406]\n",
      "   [epoch: 4, batch: 588800/4944481, loss: 0.14801]\n",
      "   [epoch: 4, batch: 614400/4944481, loss: 0.14727]\n",
      "   [epoch: 4, batch: 640000/4944481, loss: 0.14866]\n",
      "   [epoch: 4, batch: 665600/4944481, loss: 0.14764]\n",
      "   [epoch: 4, batch: 691200/4944481, loss: 0.14330]\n",
      "   [epoch: 4, batch: 716800/4944481, loss: 0.14537]\n",
      "   [epoch: 4, batch: 742400/4944481, loss: 0.14222]\n",
      "   [epoch: 4, batch: 768000/4944481, loss: 0.15732]\n",
      "   [epoch: 4, batch: 793600/4944481, loss: 0.14407]\n",
      "   [epoch: 4, batch: 819200/4944481, loss: 0.14842]\n",
      "   [epoch: 4, batch: 844800/4944481, loss: 0.14455]\n",
      "   [epoch: 4, batch: 870400/4944481, loss: 0.13995]\n",
      "   [epoch: 4, batch: 896000/4944481, loss: 0.14349]\n",
      "   [epoch: 4, batch: 921600/4944481, loss: 0.13911]\n",
      "   [epoch: 4, batch: 947200/4944481, loss: 0.14126]\n",
      "   [epoch: 4, batch: 972800/4944481, loss: 0.14201]\n",
      "   [epoch: 4, batch: 998400/4944481, loss: 0.15027]\n",
      "   [epoch: 4, batch: 1024000/4944481, loss: 0.14916]\n",
      "   [epoch: 4, batch: 1049600/4944481, loss: 0.14539]\n",
      "   [epoch: 4, batch: 1075200/4944481, loss: 0.13678]\n",
      "   [epoch: 4, batch: 1100800/4944481, loss: 0.13522]\n",
      "   [epoch: 4, batch: 1126400/4944481, loss: 0.14430]\n",
      "   [epoch: 4, batch: 1152000/4944481, loss: 0.15045]\n",
      "   [epoch: 4, batch: 1177600/4944481, loss: 0.13462]\n",
      "   [epoch: 4, batch: 1203200/4944481, loss: 0.13730]\n",
      "   [epoch: 4, batch: 1228800/4944481, loss: 0.14336]\n",
      "   [epoch: 4, batch: 1254400/4944481, loss: 0.14156]\n",
      "   [epoch: 4, batch: 1280000/4944481, loss: 0.14612]\n",
      "   [epoch: 4, batch: 1305600/4944481, loss: 0.15477]\n",
      "   [epoch: 4, batch: 1331200/4944481, loss: 0.25347]\n",
      "   [epoch: 4, batch: 1356800/4944481, loss: 0.31538]\n",
      "   [epoch: 4, batch: 1382400/4944481, loss: 0.18864]\n",
      "   [epoch: 4, batch: 1408000/4944481, loss: 0.18251]\n",
      "   [epoch: 4, batch: 1433600/4944481, loss: 0.16889]\n",
      "   [epoch: 4, batch: 1459200/4944481, loss: 0.16618]\n",
      "   [epoch: 4, batch: 1484800/4944481, loss: 0.15991]\n",
      "   [epoch: 4, batch: 1510400/4944481, loss: 0.15001]\n",
      "   [epoch: 4, batch: 1536000/4944481, loss: 0.15968]\n",
      "   [epoch: 4, batch: 1561600/4944481, loss: 0.15764]\n",
      "   [epoch: 4, batch: 1587200/4944481, loss: 0.15161]\n",
      "   [epoch: 4, batch: 1612800/4944481, loss: 0.15125]\n",
      "   [epoch: 4, batch: 1638400/4944481, loss: 0.14546]\n",
      "   [epoch: 4, batch: 1664000/4944481, loss: 0.14831]\n",
      "   [epoch: 4, batch: 1689600/4944481, loss: 0.14856]\n",
      "   [epoch: 4, batch: 1715200/4944481, loss: 0.13979]\n",
      "   [epoch: 4, batch: 1740800/4944481, loss: 0.14976]\n",
      "   [epoch: 4, batch: 1766400/4944481, loss: 0.14164]\n",
      "   [epoch: 4, batch: 1792000/4944481, loss: 0.14016]\n",
      "   [epoch: 4, batch: 1817600/4944481, loss: 0.14484]\n",
      "   [epoch: 4, batch: 1843200/4944481, loss: 0.14398]\n",
      "   [epoch: 4, batch: 1868800/4944481, loss: 0.16794]\n",
      "   [epoch: 4, batch: 1894400/4944481, loss: 0.30210]\n",
      "   [epoch: 4, batch: 1920000/4944481, loss: 0.18844]\n",
      "   [epoch: 4, batch: 1945600/4944481, loss: 0.17046]\n",
      "   [epoch: 4, batch: 1971200/4944481, loss: 0.16522]\n",
      "   [epoch: 4, batch: 1996800/4944481, loss: 0.16351]\n",
      "   [epoch: 4, batch: 2022400/4944481, loss: 0.16710]\n",
      "   [epoch: 4, batch: 2048000/4944481, loss: 0.16534]\n",
      "   [epoch: 4, batch: 2073600/4944481, loss: 0.15318]\n",
      "   [epoch: 4, batch: 2099200/4944481, loss: 0.15953]\n",
      "   [epoch: 4, batch: 2124800/4944481, loss: 0.14872]\n",
      "   [epoch: 4, batch: 2150400/4944481, loss: 0.14768]\n",
      "   [epoch: 4, batch: 2176000/4944481, loss: 0.15053]\n",
      "   [epoch: 4, batch: 2201600/4944481, loss: 0.15131]\n",
      "   [epoch: 4, batch: 2227200/4944481, loss: 0.14854]\n",
      "   [epoch: 4, batch: 2252800/4944481, loss: 0.14608]\n",
      "   [epoch: 4, batch: 2278400/4944481, loss: 0.15176]\n",
      "   [epoch: 4, batch: 2304000/4944481, loss: 0.14800]\n",
      "   [epoch: 4, batch: 2329600/4944481, loss: 0.14734]\n",
      "   [epoch: 4, batch: 2355200/4944481, loss: 0.14197]\n",
      "   [epoch: 4, batch: 2380800/4944481, loss: 0.14479]\n",
      "   [epoch: 4, batch: 2406400/4944481, loss: 0.15037]\n",
      "   [epoch: 4, batch: 2432000/4944481, loss: 0.14166]\n",
      "   [epoch: 4, batch: 2457600/4944481, loss: 0.15072]\n",
      "   [epoch: 4, batch: 2483200/4944481, loss: 0.15265]\n",
      "   [epoch: 4, batch: 2508800/4944481, loss: 0.15200]\n",
      "   [epoch: 4, batch: 2534400/4944481, loss: 0.14150]\n",
      "   [epoch: 4, batch: 2560000/4944481, loss: 0.15219]\n",
      "   [epoch: 4, batch: 2585600/4944481, loss: 0.15613]\n",
      "   [epoch: 4, batch: 2611200/4944481, loss: 0.16007]\n",
      "   [epoch: 4, batch: 2636800/4944481, loss: 0.14616]\n",
      "   [epoch: 4, batch: 2662400/4944481, loss: 0.19365]\n",
      "   [epoch: 4, batch: 2688000/4944481, loss: 0.18240]\n",
      "   [epoch: 4, batch: 2713600/4944481, loss: 0.17283]\n",
      "   [epoch: 4, batch: 2739200/4944481, loss: 0.14988]\n",
      "   [epoch: 4, batch: 2764800/4944481, loss: 0.14932]\n",
      "   [epoch: 4, batch: 2790400/4944481, loss: 0.15221]\n",
      "   [epoch: 4, batch: 2816000/4944481, loss: 0.15640]\n",
      "   [epoch: 4, batch: 2841600/4944481, loss: 0.24542]\n",
      "   [epoch: 4, batch: 2867200/4944481, loss: 0.17719]\n",
      "   [epoch: 4, batch: 2892800/4944481, loss: 0.16000]\n",
      "   [epoch: 4, batch: 2918400/4944481, loss: 0.15779]\n",
      "   [epoch: 4, batch: 2944000/4944481, loss: 0.16080]\n",
      "   [epoch: 4, batch: 2969600/4944481, loss: 0.15145]\n",
      "   [epoch: 4, batch: 2995200/4944481, loss: 0.15328]\n",
      "   [epoch: 4, batch: 3020800/4944481, loss: 0.15468]\n",
      "   [epoch: 4, batch: 3046400/4944481, loss: 0.14494]\n",
      "   [epoch: 4, batch: 3072000/4944481, loss: 0.14902]\n",
      "   [epoch: 4, batch: 3097600/4944481, loss: 0.15584]\n",
      "   [epoch: 4, batch: 3123200/4944481, loss: 0.14952]\n",
      "   [epoch: 4, batch: 3148800/4944481, loss: 0.14763]\n",
      "   [epoch: 4, batch: 3174400/4944481, loss: 0.14704]\n",
      "   [epoch: 4, batch: 3200000/4944481, loss: 0.15151]\n",
      "   [epoch: 4, batch: 3225600/4944481, loss: 0.14655]\n",
      "   [epoch: 4, batch: 3251200/4944481, loss: 0.14418]\n",
      "   [epoch: 4, batch: 3276800/4944481, loss: 0.14650]\n",
      "   [epoch: 4, batch: 3302400/4944481, loss: 0.14447]\n",
      "   [epoch: 4, batch: 3328000/4944481, loss: 0.14243]\n",
      "   [epoch: 4, batch: 3353600/4944481, loss: 0.14112]\n",
      "   [epoch: 4, batch: 3379200/4944481, loss: 0.14215]\n",
      "   [epoch: 4, batch: 3404800/4944481, loss: 0.14633]\n",
      "   [epoch: 4, batch: 3430400/4944481, loss: 0.13702]\n",
      "   [epoch: 4, batch: 3456000/4944481, loss: 0.14199]\n",
      "   [epoch: 4, batch: 3481600/4944481, loss: 0.14619]\n",
      "   [epoch: 4, batch: 3507200/4944481, loss: 0.14317]\n",
      "   [epoch: 4, batch: 3532800/4944481, loss: 0.14172]\n",
      "   [epoch: 4, batch: 3558400/4944481, loss: 0.14223]\n",
      "   [epoch: 4, batch: 3584000/4944481, loss: 0.13938]\n",
      "   [epoch: 4, batch: 3609600/4944481, loss: 0.13557]\n",
      "   [epoch: 4, batch: 3635200/4944481, loss: 0.13900]\n",
      "   [epoch: 4, batch: 3660800/4944481, loss: 0.14649]\n",
      "   [epoch: 4, batch: 3686400/4944481, loss: 0.14074]\n",
      "   [epoch: 4, batch: 3712000/4944481, loss: 0.13800]\n",
      "   [epoch: 4, batch: 3737600/4944481, loss: 0.16423]\n",
      "   [epoch: 4, batch: 3763200/4944481, loss: 0.17643]\n",
      "   [epoch: 4, batch: 3788800/4944481, loss: 0.14974]\n",
      "   [epoch: 4, batch: 3814400/4944481, loss: 0.14687]\n",
      "   [epoch: 4, batch: 3840000/4944481, loss: 0.15991]\n",
      "   [epoch: 4, batch: 3865600/4944481, loss: 0.13897]\n",
      "   [epoch: 4, batch: 3891200/4944481, loss: 0.13415]\n",
      "   [epoch: 4, batch: 3916800/4944481, loss: 0.14868]\n",
      "   [epoch: 4, batch: 3942400/4944481, loss: 0.15485]\n",
      "   [epoch: 4, batch: 3968000/4944481, loss: 0.14335]\n",
      "   [epoch: 4, batch: 3993600/4944481, loss: 0.14449]\n",
      "   [epoch: 4, batch: 4019200/4944481, loss: 0.14853]\n",
      "   [epoch: 4, batch: 4044800/4944481, loss: 0.14724]\n",
      "   [epoch: 4, batch: 4070400/4944481, loss: 0.14150]\n",
      "   [epoch: 4, batch: 4096000/4944481, loss: 0.14267]\n",
      "   [epoch: 4, batch: 4121600/4944481, loss: 0.14094]\n",
      "   [epoch: 4, batch: 4147200/4944481, loss: 0.14261]\n",
      "   [epoch: 4, batch: 4172800/4944481, loss: 0.14197]\n",
      "   [epoch: 4, batch: 4198400/4944481, loss: 0.13889]\n",
      "   [epoch: 4, batch: 4224000/4944481, loss: 0.14307]\n",
      "   [epoch: 4, batch: 4249600/4944481, loss: 0.14173]\n",
      "   [epoch: 4, batch: 4275200/4944481, loss: 0.14343]\n",
      "   [epoch: 4, batch: 4300800/4944481, loss: 0.14454]\n",
      "   [epoch: 4, batch: 4326400/4944481, loss: 0.13711]\n",
      "   [epoch: 4, batch: 4352000/4944481, loss: 0.14658]\n",
      "   [epoch: 4, batch: 4377600/4944481, loss: 0.14330]\n",
      "   [epoch: 4, batch: 4403200/4944481, loss: 0.13826]\n",
      "   [epoch: 4, batch: 4428800/4944481, loss: 0.14325]\n",
      "   [epoch: 4, batch: 4454400/4944481, loss: 0.14367]\n",
      "   [epoch: 4, batch: 4480000/4944481, loss: 0.16147]\n",
      "   [epoch: 4, batch: 4505600/4944481, loss: 0.15843]\n",
      "   [epoch: 4, batch: 4531200/4944481, loss: 0.15060]\n",
      "   [epoch: 4, batch: 4556800/4944481, loss: 0.14622]\n",
      "   [epoch: 4, batch: 4582400/4944481, loss: 0.14796]\n",
      "   [epoch: 4, batch: 4608000/4944481, loss: 0.14011]\n",
      "   [epoch: 4, batch: 4633600/4944481, loss: 0.13693]\n",
      "   [epoch: 4, batch: 4659200/4944481, loss: 0.14163]\n",
      "   [epoch: 4, batch: 4684800/4944481, loss: 0.14595]\n",
      "   [epoch: 4, batch: 4710400/4944481, loss: 0.15210]\n",
      "   [epoch: 4, batch: 4736000/4944481, loss: 0.16476]\n",
      "   [epoch: 4, batch: 4761600/4944481, loss: 0.14194]\n",
      "   [epoch: 4, batch: 4787200/4944481, loss: 0.13626]\n",
      "   [epoch: 4, batch: 4812800/4944481, loss: 0.13881]\n",
      "   [epoch: 4, batch: 4838400/4944481, loss: 0.13967]\n",
      "   [epoch: 4, batch: 4864000/4944481, loss: 0.14211]\n",
      "   [epoch: 4, batch: 4889600/4944481, loss: 0.14782]\n",
      "   [epoch: 4, batch: 4915200/4944481, loss: 0.14583]\n",
      "   [epoch: 4, batch: 4940800/4944481, loss: 0.14766]\n",
      "   Validation Accuracy: 95.16315457045762 %\n"
     ]
    }
   ],
   "source": [
    "train(model, 5, test_optimizer, test_criterion, train_load, valid_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been saving the best model as a pt file, we now load it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/myang/model_training/best_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ensure that it is the right one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Validation Accuracy: 95.36254664544165 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9536254664544166"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, train_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and before we start, we take a loop at its results in more detail with `sklearn`'s `classification_report` function. We can see that it performs fairly well. The 0 classification, which is benign, has a 0.97 f1 score across the full dataset. We can see that when detecting any of our 13 different intrusion types, the performance varies, and seems to correlate linearly with how well represented it is within the data. For example, class 4, which is represented in 87 instances out of approximately 5 million, isn't ever caught, which is fairly expected. Meanwhile, class 14 and 9 are essentially perfect. The performance is overall good, but that could be problematic for reasons described [here](https://whugimy.github.io/projects/ersp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97   2746934\n",
      "           1       0.72      0.20      0.31    160639\n",
      "           2       0.00      0.00      0.00       611\n",
      "           3       0.00      0.00      0.00       230\n",
      "           4       0.00      0.00      0.00        87\n",
      "           5       0.74      0.52      0.61    139890\n",
      "           6       1.00      1.00      1.00    461912\n",
      "           7       1.00      0.99      1.00    576191\n",
      "           8       0.71      0.87      0.78    193354\n",
      "           9       1.00      1.00      1.00    187589\n",
      "          10       0.96      0.99      0.98    286191\n",
      "          11       0.97      0.99      0.98     41508\n",
      "          12       0.86      0.98      0.92     10990\n",
      "          13       0.72      0.99      0.83      1730\n",
      "          14       1.00      1.00      1.00    686012\n",
      "\n",
      "    accuracy                           0.95   5493868\n",
      "   macro avg       0.71      0.70      0.69   5493868\n",
      "weighted avg       0.95      0.95      0.95   5493868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = model.pred(X)\n",
    "print(classification_report(Y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again ensure the model is the right one again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170015\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can begin working with TRUSTEE. We first convert our dataframes into PyTorch `tensor` objects. TRUSTEE is largely built on `sklearn`'s decision tree training framework (not the same process, but same classes and formats), so we need to use tensors instead of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustee import ClassificationTrustee\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X = df.drop('Label', axis=1)\n",
    "\n",
    "x_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(Y.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load a sample data point in our tensor, we see that it's still the normalized datawhich we want! The FFN doesn't know data is being normalized before being put in, so this way the inputs for TRUSTEE are the same as that of the FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5356e-02, -2.1717e-02, -2.7047e-02, -2.3092e-02, -1.5337e-02,\n",
       "        -3.7221e-01,  2.2031e+00,  2.4280e-01, -5.4623e-01, -4.1921e-01,\n",
       "         2.1418e+00,  1.1738e-01, -6.0950e-01, -4.4034e-03, -2.6216e-01,\n",
       "        -1.8986e-01, -2.9473e-03, -8.7419e-03, -3.0266e-03, -1.4896e-02,\n",
       "        -2.0126e-01, -3.0395e-03, -8.4495e-03, -3.1752e-03, -2.3742e-01,\n",
       "        -1.4929e-01, -2.1273e-01, -2.1053e-01, -6.5273e-02, -1.6745e-01,\n",
       "        -1.6794e-02, -2.4769e-02, -3.4850e-02, -2.3577e-01, -2.4631e-01,\n",
       "         2.3738e+00, -4.6699e-01,  1.0238e-01, -4.7545e-01, -1.4637e-01,\n",
       "        -5.4206e-02, -1.6745e-01, -5.0728e-01, -8.2245e-01, -8.5239e-01,\n",
       "        -2.0755e-01, -1.6794e-02, -5.0729e-01,  6.4583e-01,  3.6519e-01,\n",
       "         2.4280e-01,  1.1738e-01, -2.1717e-02, -2.3092e-02, -2.7047e-02,\n",
       "        -1.5337e-02, -6.8569e-01, -3.2391e-01, -2.0935e-02, -1.5090e+00,\n",
       "        -5.7687e-02, -4.6765e-02, -6.5431e-02, -4.6213e-02, -1.4954e-02,\n",
       "        -1.4701e-03, -7.4057e-03, -1.3927e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at this exact point in the dataframe (also normalized) to look at the feature names as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flow Duration     -0.015356\n",
       "Tot Fwd Pkts      -0.021717\n",
       "Tot Bwd Pkts      -0.027047\n",
       "TotLen Fwd Pkts   -0.023092\n",
       "TotLen Bwd Pkts   -0.015337\n",
       "                     ...   \n",
       "Active Min        -0.046213\n",
       "Idle Mean         -0.014954\n",
       "Idle Std          -0.001470\n",
       "Idle Max          -0.007406\n",
       "Idle Min          -0.139268\n",
       "Name: 1, Length: 68, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can begin the training process. TRUSTEE trains lots of decision trees then chooses the best ones in a fairly complex way. If possible, it would be best to read about it in the [paper](). However, in short, TRUSTEE optimizes trees for two factors: Fidelity and Agreement.\n",
    "\n",
    "Fidelity refers to the similarity of the tree and the black-box model. We optimize the trees such that they maximize fidelity, and are as close a representation to the black-box as possible.\n",
    "\n",
    "Agreement refers to the similarity of the tree to other trees we train. We train a large distribution of trees, all of which undergo the same training process, but have some randomization such that they may each differ in some way. Each tree is given an agreement score based on how similar it is to other trees, in the hopes that choosing the tree with the highest agreement to output means we've chosen the tree that best represents the full distribution. That being said, the distribution could have multiple peaks; and we're not entirely sure if this is the best methodology.\n",
    "\n",
    "The verbose training output is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6923942204254726\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 3845707 points from training dataset with (3845707, 3845707) entries\n",
      "Student model 0-0 trained with depth 30 and 1074 leaves:\n",
      "Student model score: 0.999006463933117\n",
      "Student model 0-0 fidelity: 0.999006463933117\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 3845707 points from training dataset with (4999420, 4999420) entries\n",
      "Student model 0-1 trained with depth 27 and 1000 leaves:\n",
      "Student model score: 0.9990827176930533\n",
      "Student model 0-1 fidelity: 0.9990827176930533\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 3845707 points from training dataset with (6153133, 6153133) entries\n",
      "Student model 0-2 trained with depth 25 and 985 leaves:\n",
      "Student model score: 0.9994271461974965\n",
      "Student model 0-2 fidelity: 0.9994271461974965\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 3845707 points from training dataset with (7306846, 7306846) entries\n",
      "Student model 0-3 trained with depth 29 and 972 leaves:\n",
      "Student model score: 0.9994074702154223\n",
      "Student model 0-3 fidelity: 0.9994074702154223\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 3845707 points from training dataset with (8460559, 8460559) entries\n",
      "Student model 0-4 trained with depth 30 and 937 leaves:\n",
      "Student model score: 0.999236365418187\n",
      "Student model 0-4 fidelity: 0.999236365418187\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 3845707 points from training dataset with (9614272, 9614272) entries\n",
      "Student model 1-0 trained with depth 25 and 920 leaves:\n",
      "Student model score: 0.9990369839107639\n",
      "Student model 1-0 fidelity: 0.9990369839107639\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 3845707 points from training dataset with (10767985, 10767985) entries\n",
      "Student model 1-1 trained with depth 25 and 931 leaves:\n",
      "Student model score: 0.9994697955344286\n",
      "Student model 1-1 fidelity: 0.9994697955344286\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 3845707 points from training dataset with (11921698, 11921698) entries\n",
      "Student model 1-2 trained with depth 26 and 947 leaves:\n",
      "Student model score: 0.999110678583459\n",
      "Student model 1-2 fidelity: 0.999110678583459\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 3845707 points from training dataset with (13075411, 13075411) entries\n",
      "Student model 1-3 trained with depth 29 and 911 leaves:\n",
      "Student model score: 0.9993908143914285\n",
      "Student model 1-3 fidelity: 0.9993908143914285\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 3845707 points from training dataset with (14229124, 14229124) entries\n",
      "Student model 1-4 trained with depth 27 and 936 leaves:\n",
      "Student model score: 0.9991821755322964\n",
      "Student model 1-4 fidelity: 0.9991821755322964\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 3845707 points from training dataset with (15382837, 15382837) entries\n",
      "Student model 2-0 trained with depth 29 and 905 leaves:\n",
      "Student model score: 0.9995496418906812\n",
      "Student model 2-0 fidelity: 0.9995496418906812\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 3845707 points from training dataset with (16536550, 16536550) entries\n",
      "Student model 2-1 trained with depth 29 and 892 leaves:\n",
      "Student model score: 0.9995247091804303\n",
      "Student model 2-1 fidelity: 0.9995247091804303\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 3845707 points from training dataset with (17690263, 17690263) entries\n",
      "Student model 2-2 trained with depth 26 and 915 leaves:\n",
      "Student model score: 0.9994358553698208\n",
      "Student model 2-2 fidelity: 0.9994358553698208\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 3845707 points from training dataset with (18843976, 18843976) entries\n",
      "Student model 2-3 trained with depth 27 and 910 leaves:\n",
      "Student model score: 0.9993441920770558\n",
      "Student model 2-3 fidelity: 0.9993441920770558\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 3845707 points from training dataset with (19997689, 19997689) entries\n",
      "Student model 2-4 trained with depth 28 and 922 leaves:\n",
      "Student model score: 0.99937216697001\n",
      "Student model 2-4 fidelity: 0.99937216697001\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 3845707 points from training dataset with (21151402, 21151402) entries\n",
      "Student model 3-0 trained with depth 26 and 919 leaves:\n",
      "Student model score: 0.9993884983070388\n",
      "Student model 3-0 fidelity: 0.9993884983070388\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 3845707 points from training dataset with (22305115, 22305115) entries\n",
      "Student model 3-1 trained with depth 26 and 887 leaves:\n",
      "Student model score: 0.9995564048279125\n",
      "Student model 3-1 fidelity: 0.9995564048279125\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 3845707 points from training dataset with (23458828, 23458828) entries\n",
      "Student model 3-2 trained with depth 26 and 870 leaves:\n",
      "Student model score: 0.9994626230402751\n",
      "Student model 3-2 fidelity: 0.9994626230402751\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 3845707 points from training dataset with (24612541, 24612541) entries\n",
      "Student model 3-3 trained with depth 27 and 891 leaves:\n",
      "Student model score: 0.9994460077039258\n",
      "Student model 3-3 fidelity: 0.9994460077039258\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 3845707 points from training dataset with (25766254, 25766254) entries\n",
      "Student model 3-4 trained with depth 27 and 914 leaves:\n",
      "Student model score: 0.999424260147077\n",
      "Student model 3-4 fidelity: 0.999424260147077\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 3845707 points from training dataset with (26919967, 26919967) entries\n",
      "Student model 4-0 trained with depth 25 and 927 leaves:\n",
      "Student model score: 0.9994888055894698\n",
      "Student model 4-0 fidelity: 0.9994888055894698\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 3845707 points from training dataset with (28073680, 28073680) entries\n",
      "Student model 4-1 trained with depth 26 and 911 leaves:\n",
      "Student model score: 0.9995354222329912\n",
      "Student model 4-1 fidelity: 0.9995354222329912\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 3845707 points from training dataset with (29227393, 29227393) entries\n",
      "Student model 4-2 trained with depth 26 and 885 leaves:\n",
      "Student model score: 0.9993197326646364\n",
      "Student model 4-2 fidelity: 0.9993197326646364\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 3845707 points from training dataset with (30381106, 30381106) entries\n",
      "Student model 4-3 trained with depth 32 and 912 leaves:\n",
      "Student model score: 0.999675443111881\n",
      "Student model 4-3 fidelity: 0.999675443111881\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 3845707 points from training dataset with (31534819, 31534819) entries\n",
      "Student model 4-4 trained with depth 26 and 903 leaves:\n",
      "Student model score: 0.9995839976780935\n",
      "Student model 4-4 fidelity: 0.9995839976780935\n"
     ]
    }
   ],
   "source": [
    "trustee = ClassificationTrustee(expert=model)\n",
    "trustee.fit(X, Y, num_iter=5, num_stability_iter=5, samples_size=1.0, verbose=True, predict_method_name='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the TRUSTEE model's explain function to get:\n",
    "1. `dt`: the full decision tree\n",
    "2. `pruned_dt`: the pruned decision tree for easier reading (we only really care about the top features, reasoning denoted on website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt, pruned_dt, agreement, reward = trustee.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the decision tree on our training dataset to see how it did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_y_pred = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the decision tree model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trustee_decision_tree_model.pkl', 'wb') as f:\n",
    "    pickle.dump(dt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the full classification report for the TRUSTEE tree model. It performed noticably worse than the FNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97   2472823\n",
      "           1       0.72      0.20      0.31    144311\n",
      "           2       0.00      0.00      0.00       551\n",
      "           3       0.00      0.00      0.00       206\n",
      "           4       0.00      0.00      0.00        75\n",
      "           5       0.74      0.52      0.61    125805\n",
      "           6       1.00      1.00      1.00    415856\n",
      "           7       1.00      0.99      1.00    518214\n",
      "           8       0.71      0.87      0.78    173820\n",
      "           9       1.00      1.00      1.00    168771\n",
      "          10       0.96      0.99      0.98    257658\n",
      "          11       0.97      0.99      0.98     37349\n",
      "          12       0.86      0.98      0.92      9897\n",
      "          13       0.72      0.99      0.83      1573\n",
      "          14       1.00      1.00      1.00    617572\n",
      "\n",
      "    accuracy                           0.95   4944481\n",
      "   macro avg       0.71      0.70      0.69   4944481\n",
      "weighted avg       0.95      0.95      0.95   4944481\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, dt_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try the same with the pruned decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pruned_dt_y_pred = pruned_dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as expected, with a shorter tree, it performs a bit worse than the full DT, and significantly worse than the FNN,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93   2472823\n",
      "           1       0.00      0.00      0.00    144311\n",
      "           2       0.00      0.00      0.00       551\n",
      "           3       0.00      0.00      0.00       206\n",
      "           4       0.00      0.00      0.00        75\n",
      "           5       0.32      0.55      0.41    125805\n",
      "           6       0.98      0.94      0.96    415856\n",
      "           7       0.98      0.97      0.97    518214\n",
      "           8       0.71      0.78      0.74    173820\n",
      "           9       0.40      0.50      0.45    168771\n",
      "          10       0.68      0.98      0.80    257658\n",
      "          11       0.00      0.00      0.00     37349\n",
      "          12       0.00      0.00      0.00      9897\n",
      "          13       0.00      0.00      0.00      1573\n",
      "          14       0.99      1.00      1.00    617572\n",
      "\n",
      "    accuracy                           0.87   4944481\n",
      "   macro avg       0.40      0.44      0.42   4944481\n",
      "weighted avg       0.86      0.87      0.86   4944481\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang/model_training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, pruned_dt_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `graphviz` to look at an image of the tree. We input our features and parameters as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree.export_graphviz(pruned_dt, 'prunedgraphvizoutput.gv', feature_names=X_train.columns, class_names=['Benign', 'Infilteration', 'Brute Force -Web',\n",
    "       'Brute Force -XSS', 'SQL Injection', 'DoS attacks-SlowHTTPTest',\n",
    "       'DoS attacks-Hulk', 'DDoS attacks-LOIC-HTTP', 'FTP-BruteForce',\n",
    "       'SSH-Bruteforce', 'Bot', 'DoS attacks-GoldenEye',\n",
    "       'DoS attacks-Slowloris', 'DDOS attack-LOIC-UDP',\n",
    "       'DDOS attack-HOIC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, below, we can see our decision tree! It would appear that Packet Length is the topmost decision split. Upon further discussion with Roman, it's kind of hard to say how impactful this feature is. If all malicious data had the same packet length, it could be problematic in some cases, but there are also some cases where that's okay. If you were to suddenly get a burst of data all with the same size, it could be indicative of an attack, but if your network is expecting all packets to be of some size, that feature is useless and the model we trained wouldn't help you at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/model_training/venv/lib/python3.10/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         proc \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(cmd, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:501\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m--> 501\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    502\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:969\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    967\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 969\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    970\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    971\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    972\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    973\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    974\u001b[0m                         errread, errwrite,\n\u001b[1;32m    975\u001b[0m                         restore_signals,\n\u001b[1;32m    976\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    977\u001b[0m                         start_new_session)\n\u001b[1;32m    978\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1845\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1845\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1846\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 19\u001b[0m\n\u001b[1;32m      4\u001b[0m dot_data \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mexport_graphviz(\n\u001b[1;32m      5\u001b[0m     pruned_dt,\n\u001b[1;32m      6\u001b[0m     filled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     feature_names\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m graph \u001b[39m=\u001b[39m graphviz\u001b[39m.\u001b[39mSource(dot_data)\n\u001b[0;32m---> 19\u001b[0m graph\u001b[39m.\u001b[39;49mrender(\u001b[39m\"\u001b[39;49m\u001b[39mdt_explanation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/model_training/venv/lib/python3.10/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m deprecated\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe signature of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m will be reduced\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00msupported_number\u001b[39m}\u001b[39;00m\u001b[39m positional args\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(supported)\u001b[39m}\u001b[39;00m\u001b[39m: pass \u001b[39m\u001b[39m{\u001b[39;00mwanted\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m as keyword arg(s)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[39m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/model_training/venv/lib/python3.10/site-packages/graphviz/rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(filename, directory\u001b[39m=\u001b[39mdirectory, skip_existing\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[39m.\u001b[39mappend(filepath)\n\u001b[0;32m--> 122\u001b[0m rendered \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m cleanup:\n\u001b[1;32m    125\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mdelete \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m, filepath)\n",
      "File \u001b[0;32m~/model_training/venv/lib/python3.10/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m deprecated\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe signature of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m will be reduced\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00msupported_number\u001b[39m}\u001b[39;00m\u001b[39m positional args\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(supported)\u001b[39m}\u001b[39;00m\u001b[39m: pass \u001b[39m\u001b[39m{\u001b[39;00mwanted\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m as keyword arg(s)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[39m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/model_training/venv/lib/python3.10/site-packages/graphviz/backend/rendering.py:324\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mFileExistsError(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39moutput file exists: \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mfspath(outfile)\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    322\u001b[0m cmd \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args\n\u001b[0;32m--> 324\u001b[0m execute\u001b[39m.\u001b[39;49mrun_check(cmd,\n\u001b[1;32m    325\u001b[0m                   cwd\u001b[39m=\u001b[39;49mfilepath\u001b[39m.\u001b[39;49mparent \u001b[39mif\u001b[39;49;00m filepath\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49mparts \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    326\u001b[0m                   quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[1;32m    327\u001b[0m                   capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m~/model_training/venv/lib/python3.10/site-packages/graphviz/backend/execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m==\u001b[39m errno\u001b[39m.\u001b[39mENOENT:\n\u001b[0;32m---> 84\u001b[0m         \u001b[39mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m quiet \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    pruned_dt,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    "    class_names=['Benign', 'Infilteration', 'Brute Force -Web',\n",
    "       'Brute Force -XSS', 'SQL Injection', 'DoS attacks-SlowHTTPTest',\n",
    "       'DoS attacks-Hulk', 'DDoS attacks-LOIC-HTTP', 'FTP-BruteForce',\n",
    "       'SSH-Bruteforce', 'Bot', 'DoS attacks-GoldenEye',\n",
    "       'DoS attacks-Slowloris', 'DDOS attack-LOIC-UDP',\n",
    "       'DDOS attack-HOIC'],\n",
    "    feature_names=X_train.columns\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"dt_explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trust Report...\n",
      "Preparing data...\n",
      "Splitting dataset for training and testing...\n",
      "X size: 5493868; y size: 5493868\n",
      "Done!\n",
      "Done!\n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.8% Complete\n",
      "Collecting blackbox information...\n",
      "Done!\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.6% Complete\n",
      "Collecting trustee information...\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6925509301266558\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 30 and 896 leaves:\n",
      "Student model score: 0.9987394489992671\n",
      "Student model 0-0 fidelity: 0.9987394489992671\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 24 and 829 leaves:\n",
      "Student model score: 0.999022821919968\n",
      "Student model 0-1 fidelity: 0.999022821919968\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 27 and 816 leaves:\n",
      "Student model score: 0.9990619302016596\n",
      "Student model 0-2 fidelity: 0.9990619302016596\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 26 and 822 leaves:\n",
      "Student model score: 0.9990627412349178\n",
      "Student model 0-3 fidelity: 0.9990627412349178\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 27 and 829 leaves:\n",
      "Student model score: 0.9992599408788577\n",
      "Student model 0-4 fidelity: 0.9992599408788577\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 27 and 850 leaves:\n",
      "Student model score: 0.9993154092778135\n",
      "Student model 1-0 fidelity: 0.9993154092778135\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 27 and 822 leaves:\n",
      "Student model score: 0.999295594256556\n",
      "Student model 1-1 fidelity: 0.999295594256556\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 27 and 808 leaves:\n",
      "Student model score: 0.999136696715626\n",
      "Student model 1-2 fidelity: 0.999136696715626\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 26 and 814 leaves:\n",
      "Student model score: 0.9991602791508051\n",
      "Student model 1-3 fidelity: 0.9991602791508051\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 27 and 813 leaves:\n",
      "Student model score: 0.9995339234568887\n",
      "Student model 1-4 fidelity: 0.9995339234568887\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 26 and 740 leaves:\n",
      "Student model score: 0.9992100276219684\n",
      "Student model 2-0 fidelity: 0.9992100276219684\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 27 and 781 leaves:\n",
      "Student model score: 0.9994192743128719\n",
      "Student model 2-1 fidelity: 0.9994192743128719\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 25 and 807 leaves:\n",
      "Student model score: 0.9992194654797281\n",
      "Student model 2-2 fidelity: 0.9992194654797281\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 25 and 755 leaves:\n",
      "Student model score: 0.9991794437668293\n",
      "Student model 2-3 fidelity: 0.9991794437668293\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 28 and 781 leaves:\n",
      "Student model score: 0.9993091500745889\n",
      "Student model 2-4 fidelity: 0.9993091500745889\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 26 and 785 leaves:\n",
      "Student model score: 0.9993929631069319\n",
      "Student model 3-0 fidelity: 0.9993929631069319\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 27 and 776 leaves:\n",
      "Student model score: 0.9995136599891298\n",
      "Student model 3-1 fidelity: 0.9995136599891298\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 27 and 784 leaves:\n",
      "Student model score: 0.9993542687493925\n",
      "Student model 3-2 fidelity: 0.9993542687493925\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 23 and 765 leaves:\n",
      "Student model score: 0.99932772083134\n",
      "Student model 3-3 fidelity: 0.99932772083134\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 25 and 778 leaves:\n",
      "Student model score: 0.9992858106093224\n",
      "Student model 3-4 fidelity: 0.9992858106093224\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 26 and 757 leaves:\n",
      "Student model score: 0.9992847043128767\n",
      "Student model 4-0 fidelity: 0.9992847043128767\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 26 and 779 leaves:\n",
      "Student model score: 0.9994538653374544\n",
      "Student model 4-1 fidelity: 0.9994538653374544\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 25 and 784 leaves:\n",
      "Student model score: 0.9992361442040257\n",
      "Student model 4-2 fidelity: 0.9992361442040257\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 29 and 799 leaves:\n",
      "Student model score: 0.9993898088173916\n",
      "Student model 4-3 fidelity: 0.9993898088173916\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 26 and 774 leaves:\n",
      "Student model score: 0.9994423518706487\n",
      "Student model 4-4 fidelity: 0.9994423518706487\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999910194403345, 0.9994538653374544)\n",
      "Top-k Prunned explanation size: 29\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000    855297\n",
      "           1      0.992     0.992     0.992     13385\n",
      "           5      1.000     1.000     1.000     29826\n",
      "           6      1.000     1.000     1.000    138687\n",
      "           7      1.000     1.000     1.000    172485\n",
      "           8      1.000     1.000     1.000     69893\n",
      "           9      1.000     1.000     1.000     56122\n",
      "          10      0.998     0.998     0.998     88944\n",
      "          11      0.998     0.998     0.998     12706\n",
      "          12      0.995     0.997     0.996      3763\n",
      "          13      1.000     1.000     1.000       737\n",
      "          14      1.000     1.000     1.000    206316\n",
      "\n",
      "    accuracy                          1.000   1648161\n",
      "   macro avg      0.998     0.999     0.999   1648161\n",
      "weighted avg      1.000     1.000     1.000   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.923     0.951    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.976     0.977     0.976    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.912   1648161\n",
      "   macro avg      0.537     0.603     0.557   1648161\n",
      "weighted avg      0.919     0.912     0.912   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.712     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.716     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.977     85913\n",
      "          11      0.972     0.992     0.982     12439\n",
      "          12      0.854     0.985     0.914      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.917     0.927    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.976     0.974     0.975    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.875   1648161\n",
      "   macro avg      0.400     0.444     0.417   1648161\n",
      "weighted avg      0.860     0.875     0.865   1648161\n",
      "\n",
      "Done!\n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.4% Complete\n",
      "Collecting stability analysis information...\n",
      "Iteration 0/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6925292419480198\n",
      "Initializing Trustee outer-loop with 1 iterations\n",
      "########## Outer-loop Iteration 0/1 ##########\n",
      "Initializing Trustee inner-loop with 1 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 31 and 917 leaves:\n",
      "Student model score: 0.9986160479623892\n",
      "Student model 0-0 fidelity: 0.9986160479623892\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 26 and 865 leaves:\n",
      "Student model score: 0.9990996503889776\n",
      "Student model 0-1 fidelity: 0.9990996503889776\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 30 and 839 leaves:\n",
      "Student model score: 0.9990445568230083\n",
      "Student model 0-2 fidelity: 0.9990445568230083\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 25 and 811 leaves:\n",
      "Student model score: 0.9987974131844496\n",
      "Student model 0-3 fidelity: 0.9987974131844496\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 29 and 814 leaves:\n",
      "Student model score: 0.9991417576796433\n",
      "Student model 0-4 fidelity: 0.9991417576796433\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.9991417576796433)\n",
      "Top-k Prunned explanation size: 29\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000    855297\n",
      "           1      0.992     0.991     0.992     13385\n",
      "           5      1.000     1.000     1.000     29826\n",
      "           6      1.000     1.000     1.000    138687\n",
      "           7      1.000     1.000     1.000    172485\n",
      "           8      1.000     1.000     1.000     69893\n",
      "           9      1.000     1.000     1.000     56122\n",
      "          10      0.999     0.998     0.999     88944\n",
      "          11      0.999     0.998     0.998     12706\n",
      "          12      0.995     0.996     0.996      3763\n",
      "          13      1.000     1.000     1.000       737\n",
      "          14      1.000     1.000     1.000    206316\n",
      "\n",
      "    accuracy                          1.000   1648161\n",
      "   macro avg      0.999     0.999     0.999   1648161\n",
      "weighted avg      1.000     1.000     1.000   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.923     0.951    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.976     0.977     0.976    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.912   1648161\n",
      "   macro avg      0.537     0.603     0.557   1648161\n",
      "weighted avg      0.919     0.912     0.912   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.991     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.716     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.973     0.993     0.983     12439\n",
      "          12      0.854     0.984     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.917     0.927    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.976     0.974     0.975    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.875   1648161\n",
      "   macro avg      0.400     0.444     0.417   1648161\n",
      "weighted avg      0.860     0.875     0.865   1648161\n",
      "\n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.5% Complete\n",
      "Iteration 1/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6928331509075151\n",
      "Initializing Trustee outer-loop with 1 iterations\n",
      "########## Outer-loop Iteration 0/1 ##########\n",
      "Initializing Trustee inner-loop with 1 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 30 and 917 leaves:\n",
      "Student model score: 0.9986382772228056\n",
      "Student model 0-0 fidelity: 0.9986382772228056\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 29 and 878 leaves:\n",
      "Student model score: 0.9987883519318131\n",
      "Student model 0-1 fidelity: 0.9987883519318131\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 31 and 833 leaves:\n",
      "Student model score: 0.9992139110306608\n",
      "Student model 0-2 fidelity: 0.9992139110306608\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 26 and 804 leaves:\n",
      "Student model score: 0.9992708129661145\n",
      "Student model 0-3 fidelity: 0.9992708129661145\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 32 and 802 leaves:\n",
      "Student model score: 0.9988619820873876\n",
      "Student model 0-4 fidelity: 0.9988619820873876\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.9992708129661145)\n",
      "Top-k Prunned explanation size: 31\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000    855297\n",
      "           1      0.992     0.991     0.992     13385\n",
      "           5      1.000     1.000     1.000     29826\n",
      "           6      1.000     1.000     1.000    138687\n",
      "           7      1.000     1.000     1.000    172485\n",
      "           8      1.000     1.000     1.000     69893\n",
      "           9      1.000     1.000     1.000     56122\n",
      "          10      0.998     0.999     0.999     88944\n",
      "          11      0.998     0.999     0.998     12706\n",
      "          12      0.997     0.995     0.996      3763\n",
      "          13      1.000     0.997     0.999       737\n",
      "          14      1.000     1.000     1.000    206316\n",
      "\n",
      "    accuracy                          1.000   1648161\n",
      "   macro avg      0.999     0.998     0.999   1648161\n",
      "weighted avg      1.000     1.000     1.000   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.922     0.954    855297\n",
      "           1      0.910     0.376     0.532     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.976     0.977     0.976    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.915   1648161\n",
      "   macro avg      0.613     0.634     0.602   1648161\n",
      "weighted avg      0.930     0.915     0.917   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.712     0.197     0.309     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.716     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.977     85913\n",
      "          11      0.972     0.993     0.982     12439\n",
      "          12      0.856     0.984     0.915      3267\n",
      "          13      0.701     0.996     0.823       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.943     0.914     0.928    823804\n",
      "           1      0.635     0.073     0.131     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.976     0.974     0.975    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.876   1648161\n",
      "   macro avg      0.443     0.448     0.426   1648161\n",
      "weighted avg      0.881     0.876     0.869   1648161\n",
      "\n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.6% Complete\n",
      "Iteration 2/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6927831292025199\n",
      "Initializing Trustee outer-loop with 1 iterations\n",
      "########## Outer-loop Iteration 0/1 ##########\n",
      "Initializing Trustee inner-loop with 1 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 26 and 913 leaves:\n",
      "Student model score: 0.9987120051977375\n",
      "Student model 0-0 fidelity: 0.9987120051977375\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 27 and 835 leaves:\n",
      "Student model score: 0.9989467043289187\n",
      "Student model 0-1 fidelity: 0.9989467043289187\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 28 and 860 leaves:\n",
      "Student model score: 0.9991294315322604\n",
      "Student model 0-2 fidelity: 0.9991294315322604\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 27 and 828 leaves:\n",
      "Student model score: 0.99924828893925\n",
      "Student model 0-3 fidelity: 0.99924828893925\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 26 and 826 leaves:\n",
      "Student model score: 0.9991385892340396\n",
      "Student model 0-4 fidelity: 0.9991385892340396\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.99924828893925)\n",
      "Top-k Prunned explanation size: 29\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000    855297\n",
      "           1      0.994     0.993     0.993     13385\n",
      "           5      1.000     1.000     1.000     29826\n",
      "           6      1.000     1.000     1.000    138687\n",
      "           7      1.000     1.000     1.000    172485\n",
      "           8      1.000     1.000     1.000     69893\n",
      "           9      1.000     1.000     1.000     56122\n",
      "          10      0.999     0.999     0.999     88944\n",
      "          11      0.998     0.998     0.998     12706\n",
      "          12      0.998     0.995     0.996      3763\n",
      "          13      1.000     1.000     1.000       737\n",
      "          14      1.000     1.000     1.000    206316\n",
      "\n",
      "    accuracy                          1.000   1648161\n",
      "   macro avg      0.999     0.999     0.999   1648161\n",
      "weighted avg      1.000     1.000     1.000   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.923     0.951    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.976     0.977     0.976    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.912   1648161\n",
      "   macro avg      0.537     0.603     0.557   1648161\n",
      "weighted avg      0.919     0.912     0.912   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.991     0.972    823804\n",
      "           1      0.712     0.198     0.309     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.973     0.993     0.983     12439\n",
      "          12      0.857     0.984     0.916      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.917     0.927    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.976     0.974     0.975    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.875   1648161\n",
      "   macro avg      0.400     0.444     0.417   1648161\n",
      "weighted avg      0.860     0.875     0.865   1648161\n",
      "\n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.7% Complete\n",
      "Iteration 3/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.692442902481811\n",
      "Initializing Trustee outer-loop with 1 iterations\n",
      "########## Outer-loop Iteration 0/1 ##########\n",
      "Initializing Trustee inner-loop with 1 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 29 and 877 leaves:\n",
      "Student model score: 0.9990794930325118\n",
      "Student model 0-0 fidelity: 0.9990794930325118\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 29 and 874 leaves:\n",
      "Student model score: 0.9991059111428838\n",
      "Student model 0-1 fidelity: 0.9991059111428838\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 25 and 845 leaves:\n",
      "Student model score: 0.999331411475931\n",
      "Student model 0-2 fidelity: 0.999331411475931\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 27 and 833 leaves:\n",
      "Student model score: 0.9991976532892791\n",
      "Student model 0-3 fidelity: 0.9991976532892791\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 26 and 807 leaves:\n",
      "Student model score: 0.9995155496648663\n",
      "Student model 0-4 fidelity: 0.9995155496648663\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.9995155496648663)\n",
      "Top-k Prunned explanation size: 29\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000    855297\n",
      "           1      0.993     0.993     0.993     13385\n",
      "           5      1.000     1.000     1.000     29826\n",
      "           6      1.000     1.000     1.000    138687\n",
      "           7      1.000     1.000     1.000    172485\n",
      "           8      1.000     1.000     1.000     69893\n",
      "           9      1.000     1.000     1.000     56122\n",
      "          10      0.998     0.999     0.998     88944\n",
      "          11      0.999     0.998     0.998     12706\n",
      "          12      0.997     0.997     0.997      3763\n",
      "          13      1.000     0.999     0.999       737\n",
      "          14      1.000     1.000     1.000    206316\n",
      "\n",
      "    accuracy                          1.000   1648161\n",
      "   macro avg      0.999     0.999     0.999   1648161\n",
      "weighted avg      1.000     1.000     1.000   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.923     0.951    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.976     0.977     0.976    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.912   1648161\n",
      "   macro avg      0.537     0.603     0.557   1648161\n",
      "weighted avg      0.919     0.912     0.912   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.712     0.198     0.309     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.960     0.995     0.977     85913\n",
      "          11      0.973     0.993     0.983     12439\n",
      "          12      0.854     0.984     0.915      3267\n",
      "          13      0.700     0.996     0.822       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.917     0.927    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.976     0.974     0.975    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.875   1648161\n",
      "   macro avg      0.400     0.444     0.417   1648161\n",
      "weighted avg      0.860     0.875     0.865   1648161\n",
      "\n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.9% Complete\n",
      "Iteration 4/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6928970906442692\n",
      "Initializing Trustee outer-loop with 1 iterations\n",
      "########## Outer-loop Iteration 0/1 ##########\n",
      "Initializing Trustee inner-loop with 1 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 28 and 930 leaves:\n",
      "Student model score: 0.9988939658565325\n",
      "Student model 0-0 fidelity: 0.9988939658565325\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 26 and 848 leaves:\n",
      "Student model score: 0.9990707136211429\n",
      "Student model 0-1 fidelity: 0.9990707136211429\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 23 and 841 leaves:\n",
      "Student model score: 0.9991347186985093\n",
      "Student model 0-2 fidelity: 0.9991347186985093\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 27 and 822 leaves:\n",
      "Student model score: 0.998837099195593\n",
      "Student model 0-3 fidelity: 0.998837099195593\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 24 and 795 leaves:\n",
      "Student model score: 0.9992130241934162\n",
      "Student model 0-4 fidelity: 0.9992130241934162\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.9992130241934162)\n",
      "Top-k Prunned explanation size: 29\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000    855297\n",
      "           1      0.992     0.992     0.992     13385\n",
      "           5      1.000     1.000     1.000     29826\n",
      "           6      1.000     1.000     1.000    138687\n",
      "           7      1.000     1.000     1.000    172485\n",
      "           8      1.000     1.000     1.000     69893\n",
      "           9      1.000     1.000     1.000     56122\n",
      "          10      0.999     0.998     0.999     88944\n",
      "          11      0.997     0.998     0.998     12706\n",
      "          12      0.997     0.995     0.996      3763\n",
      "          13      1.000     1.000     1.000       737\n",
      "          14      1.000     1.000     1.000    206316\n",
      "\n",
      "    accuracy                          1.000   1648161\n",
      "   macro avg      0.999     0.999     0.999   1648161\n",
      "weighted avg      1.000     1.000     1.000   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.923     0.951    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.976     0.977     0.976    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.912   1648161\n",
      "   macro avg      0.537     0.603     0.557   1648161\n",
      "weighted avg      0.919     0.912     0.912   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.712     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.971     0.993     0.982     12439\n",
      "          12      0.856     0.984     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.917     0.927    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.976     0.974     0.975    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.875   1648161\n",
      "   macro avg      0.400     0.444     0.417   1648161\n",
      "weighted avg      0.860     0.875     0.865   1648161\n",
      "\n",
      "Progress |----------------------------------------------------------------------------------------------------| 1.0% Complete\n",
      "Done!\n",
      "Collecting branch analysis information...\n",
      "Iteration 1/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.1% Complete\n",
      "Iteration 2/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.2% Complete\n",
      "Iteration 3/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.4% Complete\n",
      "Iteration 4/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.5% Complete\n",
      "Iteration 5/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.6% Complete\n",
      "Iteration 6/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.7% Complete\n",
      "Iteration 7/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 1.9% Complete\n",
      "Iteration 8/779\n",
      "Progress |---------------------------------------------------------------------------------------------------| 2.0% Complete\n",
      "Iteration 9/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.1% Complete\n",
      "Iteration 10/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.2% Complete\n",
      "Iteration 11/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.4% Complete\n",
      "Iteration 12/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.5% Complete\n",
      "Iteration 13/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.6% Complete\n",
      "Iteration 14/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.7% Complete\n",
      "Iteration 15/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 2.9% Complete\n",
      "Iteration 16/779\n",
      "Progress |--------------------------------------------------------------------------------------------------| 3.0% Complete\n",
      "Iteration 17/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.1% Complete\n",
      "Iteration 18/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.2% Complete\n",
      "Iteration 19/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.4% Complete\n",
      "Iteration 20/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.5% Complete\n",
      "Iteration 21/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.6% Complete\n",
      "Iteration 22/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.7% Complete\n",
      "Iteration 23/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 3.9% Complete\n",
      "Iteration 24/779\n",
      "Progress |-------------------------------------------------------------------------------------------------| 4.0% Complete\n",
      "Iteration 25/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.1% Complete\n",
      "Iteration 26/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.2% Complete\n",
      "Iteration 27/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.4% Complete\n",
      "Iteration 28/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.5% Complete\n",
      "Iteration 29/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.6% Complete\n",
      "Iteration 30/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.7% Complete\n",
      "Iteration 31/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 4.9% Complete\n",
      "Iteration 32/779\n",
      "Progress |------------------------------------------------------------------------------------------------| 5.0% Complete\n",
      "Iteration 33/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.1% Complete\n",
      "Iteration 34/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.2% Complete\n",
      "Iteration 35/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.4% Complete\n",
      "Iteration 36/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.5% Complete\n",
      "Iteration 37/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.6% Complete\n",
      "Iteration 38/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.7% Complete\n",
      "Iteration 39/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 5.9% Complete\n",
      "Iteration 40/779\n",
      "Progress |-----------------------------------------------------------------------------------------------| 6.0% Complete\n",
      "Iteration 41/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.1% Complete\n",
      "Iteration 42/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.2% Complete\n",
      "Iteration 43/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.4% Complete\n",
      "Iteration 44/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.5% Complete\n",
      "Iteration 45/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.6% Complete\n",
      "Iteration 46/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.7% Complete\n",
      "Iteration 47/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 6.8% Complete\n",
      "Iteration 48/779\n",
      "Progress |----------------------------------------------------------------------------------------------| 7.0% Complete\n",
      "Iteration 49/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.1% Complete\n",
      "Iteration 50/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.2% Complete\n",
      "Iteration 51/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.3% Complete\n",
      "Iteration 52/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.5% Complete\n",
      "Iteration 53/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.6% Complete\n",
      "Iteration 54/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.7% Complete\n",
      "Iteration 55/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 7.8% Complete\n",
      "Iteration 56/779\n",
      "Progress |---------------------------------------------------------------------------------------------| 8.0% Complete\n",
      "Iteration 57/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.1% Complete\n",
      "Iteration 58/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.2% Complete\n",
      "Iteration 59/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.3% Complete\n",
      "Iteration 60/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.5% Complete\n",
      "Iteration 61/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.6% Complete\n",
      "Iteration 62/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.7% Complete\n",
      "Iteration 63/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 8.8% Complete\n",
      "Iteration 64/779\n",
      "Progress |--------------------------------------------------------------------------------------------| 9.0% Complete\n",
      "Iteration 65/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.1% Complete\n",
      "Iteration 66/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.2% Complete\n",
      "Iteration 67/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.3% Complete\n",
      "Iteration 68/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.5% Complete\n",
      "Iteration 69/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.6% Complete\n",
      "Iteration 70/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.7% Complete\n",
      "Iteration 71/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 9.8% Complete\n",
      "Iteration 72/779\n",
      "Progress |-------------------------------------------------------------------------------------------| 10.0% Complete\n",
      "Iteration 73/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.1% Complete\n",
      "Iteration 74/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.2% Complete\n",
      "Iteration 75/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.3% Complete\n",
      "Iteration 76/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.5% Complete\n",
      "Iteration 77/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.6% Complete\n",
      "Iteration 78/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.7% Complete\n",
      "Iteration 79/779\n",
      "Progress |------------------------------------------------------------------------------------------| 10.8% Complete\n",
      "Iteration 80/779\n",
      "Progress |------------------------------------------------------------------------------------------| 11.0% Complete\n",
      "Iteration 81/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.1% Complete\n",
      "Iteration 82/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.2% Complete\n",
      "Iteration 83/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.3% Complete\n",
      "Iteration 84/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.5% Complete\n",
      "Iteration 85/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.6% Complete\n",
      "Iteration 86/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.7% Complete\n",
      "Iteration 87/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 11.8% Complete\n",
      "Iteration 88/779\n",
      "Progress |-----------------------------------------------------------------------------------------| 12.0% Complete\n",
      "Iteration 89/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.1% Complete\n",
      "Iteration 90/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.2% Complete\n",
      "Iteration 91/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.3% Complete\n",
      "Iteration 92/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.5% Complete\n",
      "Iteration 93/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.6% Complete\n",
      "Iteration 94/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.7% Complete\n",
      "Iteration 95/779\n",
      "Progress |----------------------------------------------------------------------------------------| 12.8% Complete\n",
      "Iteration 96/779\n",
      "Progress |----------------------------------------------------------------------------------------| 13.0% Complete\n",
      "Iteration 97/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.1% Complete\n",
      "Iteration 98/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.2% Complete\n",
      "Iteration 99/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.3% Complete\n",
      "Iteration 100/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.4% Complete\n",
      "Iteration 101/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.6% Complete\n",
      "Iteration 102/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.7% Complete\n",
      "Iteration 103/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.8% Complete\n",
      "Iteration 104/779\n",
      "Progress |---------------------------------------------------------------------------------------| 13.9% Complete\n",
      "Iteration 105/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.1% Complete\n",
      "Iteration 106/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.2% Complete\n",
      "Iteration 107/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.3% Complete\n",
      "Iteration 108/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.4% Complete\n",
      "Iteration 109/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.6% Complete\n",
      "Iteration 110/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.7% Complete\n",
      "Iteration 111/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.8% Complete\n",
      "Iteration 112/779\n",
      "Progress |--------------------------------------------------------------------------------------| 14.9% Complete\n",
      "Iteration 113/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.1% Complete\n",
      "Iteration 114/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.2% Complete\n",
      "Iteration 115/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.3% Complete\n",
      "Iteration 116/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.4% Complete\n",
      "Iteration 117/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.6% Complete\n",
      "Iteration 118/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.7% Complete\n",
      "Iteration 119/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.8% Complete\n",
      "Iteration 120/779\n",
      "Progress |-------------------------------------------------------------------------------------| 15.9% Complete\n",
      "Iteration 121/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.1% Complete\n",
      "Iteration 122/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.2% Complete\n",
      "Iteration 123/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.3% Complete\n",
      "Iteration 124/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.4% Complete\n",
      "Iteration 125/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.6% Complete\n",
      "Iteration 126/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.7% Complete\n",
      "Iteration 127/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.8% Complete\n",
      "Iteration 128/779\n",
      "Progress |------------------------------------------------------------------------------------| 16.9% Complete\n",
      "Iteration 129/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.1% Complete\n",
      "Iteration 130/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.2% Complete\n",
      "Iteration 131/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.3% Complete\n",
      "Iteration 132/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.4% Complete\n",
      "Iteration 133/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.6% Complete\n",
      "Iteration 134/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.7% Complete\n",
      "Iteration 135/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.8% Complete\n",
      "Iteration 136/779\n",
      "Progress |-----------------------------------------------------------------------------------| 17.9% Complete\n",
      "Iteration 137/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.1% Complete\n",
      "Iteration 138/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.2% Complete\n",
      "Iteration 139/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.3% Complete\n",
      "Iteration 140/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.4% Complete\n",
      "Iteration 141/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.6% Complete\n",
      "Iteration 142/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.7% Complete\n",
      "Iteration 143/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.8% Complete\n",
      "Iteration 144/779\n",
      "Progress |----------------------------------------------------------------------------------| 18.9% Complete\n",
      "Iteration 145/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.1% Complete\n",
      "Iteration 146/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.2% Complete\n",
      "Iteration 147/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.3% Complete\n",
      "Iteration 148/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.4% Complete\n",
      "Iteration 149/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.6% Complete\n",
      "Iteration 150/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.7% Complete\n",
      "Iteration 151/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.8% Complete\n",
      "Iteration 152/779\n",
      "Progress |---------------------------------------------------------------------------------| 19.9% Complete\n",
      "Iteration 153/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.0% Complete\n",
      "Iteration 154/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.2% Complete\n",
      "Iteration 155/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.3% Complete\n",
      "Iteration 156/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.4% Complete\n",
      "Iteration 157/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.5% Complete\n",
      "Iteration 158/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.7% Complete\n",
      "Iteration 159/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.8% Complete\n",
      "Iteration 160/779\n",
      "Progress |--------------------------------------------------------------------------------| 20.9% Complete\n",
      "Iteration 161/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.0% Complete\n",
      "Iteration 162/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.2% Complete\n",
      "Iteration 163/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.3% Complete\n",
      "Iteration 164/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.4% Complete\n",
      "Iteration 165/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.5% Complete\n",
      "Iteration 166/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.7% Complete\n",
      "Iteration 167/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.8% Complete\n",
      "Iteration 168/779\n",
      "Progress |-------------------------------------------------------------------------------| 21.9% Complete\n",
      "Iteration 169/779\n",
      "Progress |------------------------------------------------------------------------------| 22.0% Complete\n",
      "Iteration 170/779\n",
      "Progress |------------------------------------------------------------------------------| 22.2% Complete\n",
      "Iteration 171/779\n",
      "Progress |------------------------------------------------------------------------------| 22.3% Complete\n",
      "Iteration 172/779\n",
      "Progress |------------------------------------------------------------------------------| 22.4% Complete\n",
      "Iteration 173/779\n",
      "Progress |------------------------------------------------------------------------------| 22.5% Complete\n",
      "Iteration 174/779\n",
      "Progress |------------------------------------------------------------------------------| 22.7% Complete\n",
      "Iteration 175/779\n",
      "Progress |------------------------------------------------------------------------------| 22.8% Complete\n",
      "Iteration 176/779\n",
      "Progress |------------------------------------------------------------------------------| 22.9% Complete\n",
      "Iteration 177/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.0% Complete\n",
      "Iteration 178/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.2% Complete\n",
      "Iteration 179/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.3% Complete\n",
      "Iteration 180/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.4% Complete\n",
      "Iteration 181/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.5% Complete\n",
      "Iteration 182/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.7% Complete\n",
      "Iteration 183/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.8% Complete\n",
      "Iteration 184/779\n",
      "Progress |-----------------------------------------------------------------------------| 23.9% Complete\n",
      "Iteration 185/779\n",
      "Progress |----------------------------------------------------------------------------| 24.0% Complete\n",
      "Iteration 186/779\n",
      "Progress |----------------------------------------------------------------------------| 24.2% Complete\n",
      "Iteration 187/779\n",
      "Progress |----------------------------------------------------------------------------| 24.3% Complete\n",
      "Iteration 188/779\n",
      "Progress |----------------------------------------------------------------------------| 24.4% Complete\n",
      "Iteration 189/779\n",
      "Progress |----------------------------------------------------------------------------| 24.5% Complete\n",
      "Iteration 190/779\n",
      "Progress |----------------------------------------------------------------------------| 24.7% Complete\n",
      "Iteration 191/779\n",
      "Progress |----------------------------------------------------------------------------| 24.8% Complete\n",
      "Iteration 192/779\n",
      "Progress |----------------------------------------------------------------------------| 24.9% Complete\n",
      "Iteration 193/779\n",
      "Progress |---------------------------------------------------------------------------| 25.0% Complete\n",
      "Iteration 194/779\n",
      "Progress |---------------------------------------------------------------------------| 25.2% Complete\n",
      "Iteration 195/779\n",
      "Progress |---------------------------------------------------------------------------| 25.3% Complete\n",
      "Iteration 196/779\n",
      "Progress |---------------------------------------------------------------------------| 25.4% Complete\n",
      "Iteration 197/779\n",
      "Progress |---------------------------------------------------------------------------| 25.5% Complete\n",
      "Iteration 198/779\n",
      "Progress |---------------------------------------------------------------------------| 25.7% Complete\n",
      "Iteration 199/779\n",
      "Progress |---------------------------------------------------------------------------| 25.8% Complete\n",
      "Iteration 200/779\n",
      "Progress |---------------------------------------------------------------------------| 25.9% Complete\n",
      "Iteration 201/779\n",
      "Progress |--------------------------------------------------------------------------| 26.0% Complete\n",
      "Iteration 202/779\n",
      "Progress |--------------------------------------------------------------------------| 26.2% Complete\n",
      "Iteration 203/779\n",
      "Progress |--------------------------------------------------------------------------| 26.3% Complete\n",
      "Iteration 204/779\n",
      "Progress |--------------------------------------------------------------------------| 26.4% Complete\n",
      "Iteration 205/779\n",
      "Progress |--------------------------------------------------------------------------| 26.5% Complete\n",
      "Iteration 206/779\n",
      "Progress |--------------------------------------------------------------------------| 26.7% Complete\n",
      "Iteration 207/779\n",
      "Progress |--------------------------------------------------------------------------| 26.8% Complete\n",
      "Iteration 208/779\n",
      "Progress |--------------------------------------------------------------------------| 26.9% Complete\n",
      "Iteration 209/779\n",
      "Progress |-------------------------------------------------------------------------| 27.0% Complete\n",
      "Iteration 210/779\n",
      "Progress |-------------------------------------------------------------------------| 27.1% Complete\n",
      "Iteration 211/779\n",
      "Progress |-------------------------------------------------------------------------| 27.3% Complete\n",
      "Iteration 212/779\n",
      "Progress |-------------------------------------------------------------------------| 27.4% Complete\n",
      "Iteration 213/779\n",
      "Progress |-------------------------------------------------------------------------| 27.5% Complete\n",
      "Iteration 214/779\n",
      "Progress |-------------------------------------------------------------------------| 27.6% Complete\n",
      "Iteration 215/779\n",
      "Progress |-------------------------------------------------------------------------| 27.8% Complete\n",
      "Iteration 216/779\n",
      "Progress |-------------------------------------------------------------------------| 27.9% Complete\n",
      "Iteration 217/779\n",
      "Progress |------------------------------------------------------------------------| 28.0% Complete\n",
      "Iteration 218/779\n",
      "Progress |------------------------------------------------------------------------| 28.1% Complete\n",
      "Iteration 219/779\n",
      "Progress |------------------------------------------------------------------------| 28.3% Complete\n",
      "Iteration 220/779\n",
      "Progress |------------------------------------------------------------------------| 28.4% Complete\n",
      "Iteration 221/779\n",
      "Progress |------------------------------------------------------------------------| 28.5% Complete\n",
      "Iteration 222/779\n",
      "Progress |------------------------------------------------------------------------| 28.6% Complete\n",
      "Iteration 223/779\n",
      "Progress |------------------------------------------------------------------------| 28.8% Complete\n",
      "Iteration 224/779\n",
      "Progress |------------------------------------------------------------------------| 28.9% Complete\n",
      "Iteration 225/779\n",
      "Progress |-----------------------------------------------------------------------| 29.0% Complete\n",
      "Iteration 226/779\n",
      "Progress |-----------------------------------------------------------------------| 29.1% Complete\n",
      "Iteration 227/779\n",
      "Progress |-----------------------------------------------------------------------| 29.3% Complete\n",
      "Iteration 228/779\n",
      "Progress |-----------------------------------------------------------------------| 29.4% Complete\n",
      "Iteration 229/779\n",
      "Progress |-----------------------------------------------------------------------| 29.5% Complete\n",
      "Iteration 230/779\n",
      "Progress |-----------------------------------------------------------------------| 29.6% Complete\n",
      "Iteration 231/779\n",
      "Progress |-----------------------------------------------------------------------| 29.8% Complete\n",
      "Iteration 232/779\n",
      "Progress |-----------------------------------------------------------------------| 29.9% Complete\n",
      "Iteration 233/779\n",
      "Progress |----------------------------------------------------------------------| 30.0% Complete\n",
      "Iteration 234/779\n",
      "Progress |----------------------------------------------------------------------| 30.1% Complete\n",
      "Iteration 235/779\n",
      "Progress |----------------------------------------------------------------------| 30.3% Complete\n",
      "Iteration 236/779\n",
      "Progress |----------------------------------------------------------------------| 30.4% Complete\n",
      "Iteration 237/779\n",
      "Progress |----------------------------------------------------------------------| 30.5% Complete\n",
      "Iteration 238/779\n",
      "Progress |----------------------------------------------------------------------| 30.6% Complete\n",
      "Iteration 239/779\n",
      "Progress |----------------------------------------------------------------------| 30.8% Complete\n",
      "Iteration 240/779\n",
      "Progress |----------------------------------------------------------------------| 30.9% Complete\n",
      "Iteration 241/779\n",
      "Progress |---------------------------------------------------------------------| 31.0% Complete\n",
      "Iteration 242/779\n",
      "Progress |---------------------------------------------------------------------| 31.1% Complete\n",
      "Iteration 243/779\n",
      "Progress |---------------------------------------------------------------------| 31.3% Complete\n",
      "Iteration 244/779\n",
      "Progress |---------------------------------------------------------------------| 31.4% Complete\n",
      "Iteration 245/779\n",
      "Progress |---------------------------------------------------------------------| 31.5% Complete\n",
      "Iteration 246/779\n",
      "Progress |---------------------------------------------------------------------| 31.6% Complete\n",
      "Iteration 247/779\n",
      "Progress |---------------------------------------------------------------------| 31.8% Complete\n",
      "Iteration 248/779\n",
      "Progress |---------------------------------------------------------------------| 31.9% Complete\n",
      "Iteration 249/779\n",
      "Progress |--------------------------------------------------------------------| 32.0% Complete\n",
      "Iteration 250/779\n",
      "Progress |--------------------------------------------------------------------| 32.1% Complete\n",
      "Iteration 251/779\n",
      "Progress |--------------------------------------------------------------------| 32.3% Complete\n",
      "Iteration 252/779\n",
      "Progress |--------------------------------------------------------------------| 32.4% Complete\n",
      "Iteration 253/779\n",
      "Progress |--------------------------------------------------------------------| 32.5% Complete\n",
      "Iteration 254/779\n",
      "Progress |--------------------------------------------------------------------| 32.6% Complete\n",
      "Iteration 255/779\n",
      "Progress |--------------------------------------------------------------------| 32.8% Complete\n",
      "Iteration 256/779\n",
      "Progress |--------------------------------------------------------------------| 32.9% Complete\n",
      "Iteration 257/779\n",
      "Progress |-------------------------------------------------------------------| 33.0% Complete\n",
      "Iteration 258/779\n",
      "Progress |-------------------------------------------------------------------| 33.1% Complete\n",
      "Iteration 259/779\n",
      "Progress |-------------------------------------------------------------------| 33.3% Complete\n",
      "Iteration 260/779\n",
      "Progress |-------------------------------------------------------------------| 33.4% Complete\n",
      "Iteration 261/779\n",
      "Progress |-------------------------------------------------------------------| 33.5% Complete\n",
      "Iteration 262/779\n",
      "Progress |-------------------------------------------------------------------| 33.6% Complete\n",
      "Iteration 263/779\n",
      "Progress |-------------------------------------------------------------------| 33.7% Complete\n",
      "Iteration 264/779\n",
      "Progress |-------------------------------------------------------------------| 33.9% Complete\n",
      "Iteration 265/779\n",
      "Progress |-------------------------------------------------------------------| 34.0% Complete\n",
      "Iteration 266/779\n",
      "Progress |------------------------------------------------------------------| 34.1% Complete\n",
      "Iteration 267/779\n",
      "Progress |------------------------------------------------------------------| 34.2% Complete\n",
      "Iteration 268/779\n",
      "Progress |------------------------------------------------------------------| 34.4% Complete\n",
      "Iteration 269/779\n",
      "Progress |------------------------------------------------------------------| 34.5% Complete\n",
      "Iteration 270/779\n",
      "Progress |------------------------------------------------------------------| 34.6% Complete\n",
      "Iteration 271/779\n",
      "Progress |------------------------------------------------------------------| 34.7% Complete\n",
      "Iteration 272/779\n",
      "Progress |------------------------------------------------------------------| 34.9% Complete\n",
      "Iteration 273/779\n",
      "Progress |------------------------------------------------------------------| 35.0% Complete\n",
      "Iteration 274/779\n",
      "Progress |-----------------------------------------------------------------| 35.1% Complete\n",
      "Iteration 275/779\n",
      "Progress |-----------------------------------------------------------------| 35.2% Complete\n",
      "Iteration 276/779\n",
      "Progress |-----------------------------------------------------------------| 35.4% Complete\n",
      "Iteration 277/779\n",
      "Progress |-----------------------------------------------------------------| 35.5% Complete\n",
      "Iteration 278/779\n",
      "Progress |-----------------------------------------------------------------| 35.6% Complete\n",
      "Iteration 279/779\n",
      "Progress |-----------------------------------------------------------------| 35.7% Complete\n",
      "Iteration 280/779\n",
      "Progress |-----------------------------------------------------------------| 35.9% Complete\n",
      "Iteration 281/779\n",
      "Progress |-----------------------------------------------------------------| 36.0% Complete\n",
      "Iteration 282/779\n",
      "Progress |----------------------------------------------------------------| 36.1% Complete\n",
      "Iteration 283/779\n",
      "Progress |----------------------------------------------------------------| 36.2% Complete\n",
      "Iteration 284/779\n",
      "Progress |----------------------------------------------------------------| 36.4% Complete\n",
      "Iteration 285/779\n",
      "Progress |----------------------------------------------------------------| 36.5% Complete\n",
      "Iteration 286/779\n",
      "Progress |----------------------------------------------------------------| 36.6% Complete\n",
      "Iteration 287/779\n",
      "Progress |----------------------------------------------------------------| 36.7% Complete\n",
      "Iteration 288/779\n",
      "Progress |----------------------------------------------------------------| 36.9% Complete\n",
      "Iteration 289/779\n",
      "Progress |----------------------------------------------------------------| 37.0% Complete\n",
      "Iteration 290/779\n",
      "Progress |---------------------------------------------------------------| 37.1% Complete\n",
      "Iteration 291/779\n",
      "Progress |---------------------------------------------------------------| 37.2% Complete\n",
      "Iteration 292/779\n",
      "Progress |---------------------------------------------------------------| 37.4% Complete\n",
      "Iteration 293/779\n",
      "Progress |---------------------------------------------------------------| 37.5% Complete\n",
      "Iteration 294/779\n",
      "Progress |---------------------------------------------------------------| 37.6% Complete\n",
      "Iteration 295/779\n",
      "Progress |---------------------------------------------------------------| 37.7% Complete\n",
      "Iteration 296/779\n",
      "Progress |---------------------------------------------------------------| 37.9% Complete\n",
      "Iteration 297/779\n",
      "Progress |---------------------------------------------------------------| 38.0% Complete\n",
      "Iteration 298/779\n",
      "Progress |--------------------------------------------------------------| 38.1% Complete\n",
      "Iteration 299/779\n",
      "Progress |--------------------------------------------------------------| 38.2% Complete\n",
      "Iteration 300/779\n",
      "Progress |--------------------------------------------------------------| 38.4% Complete\n",
      "Iteration 301/779\n",
      "Progress |--------------------------------------------------------------| 38.5% Complete\n",
      "Iteration 302/779\n",
      "Progress |--------------------------------------------------------------| 38.6% Complete\n",
      "Iteration 303/779\n",
      "Progress |--------------------------------------------------------------| 38.7% Complete\n",
      "Iteration 304/779\n",
      "Progress |--------------------------------------------------------------| 38.9% Complete\n",
      "Iteration 305/779\n",
      "Progress |--------------------------------------------------------------| 39.0% Complete\n",
      "Iteration 306/779\n",
      "Progress |-------------------------------------------------------------| 39.1% Complete\n",
      "Iteration 307/779\n",
      "Progress |-------------------------------------------------------------| 39.2% Complete\n",
      "Iteration 308/779\n",
      "Progress |-------------------------------------------------------------| 39.4% Complete\n",
      "Iteration 309/779\n",
      "Progress |-------------------------------------------------------------| 39.5% Complete\n",
      "Iteration 310/779\n",
      "Progress |-------------------------------------------------------------| 39.6% Complete\n",
      "Iteration 311/779\n",
      "Progress |-------------------------------------------------------------| 39.7% Complete\n",
      "Iteration 312/779\n",
      "Progress |-------------------------------------------------------------| 39.9% Complete\n",
      "Iteration 313/779\n",
      "Progress |-------------------------------------------------------------| 40.0% Complete\n",
      "Iteration 314/779\n",
      "Progress |------------------------------------------------------------| 40.1% Complete\n",
      "Iteration 315/779\n",
      "Progress |------------------------------------------------------------| 40.2% Complete\n",
      "Iteration 316/779\n",
      "Progress |------------------------------------------------------------| 40.3% Complete\n",
      "Iteration 317/779\n",
      "Progress |------------------------------------------------------------| 40.5% Complete\n",
      "Iteration 318/779\n",
      "Progress |------------------------------------------------------------| 40.6% Complete\n",
      "Iteration 319/779\n",
      "Progress |------------------------------------------------------------| 40.7% Complete\n",
      "Iteration 320/779\n",
      "Progress |------------------------------------------------------------| 40.8% Complete\n",
      "Iteration 321/779\n",
      "Progress |------------------------------------------------------------| 41.0% Complete\n",
      "Iteration 322/779\n",
      "Progress |-----------------------------------------------------------| 41.1% Complete\n",
      "Iteration 323/779\n",
      "Progress |-----------------------------------------------------------| 41.2% Complete\n",
      "Iteration 324/779\n",
      "Progress |-----------------------------------------------------------| 41.3% Complete\n",
      "Iteration 325/779\n",
      "Progress |-----------------------------------------------------------| 41.5% Complete\n",
      "Iteration 326/779\n",
      "Progress |-----------------------------------------------------------| 41.6% Complete\n",
      "Iteration 327/779\n",
      "Progress |-----------------------------------------------------------| 41.7% Complete\n",
      "Iteration 328/779\n",
      "Progress |-----------------------------------------------------------| 41.8% Complete\n",
      "Iteration 329/779\n",
      "Progress |-----------------------------------------------------------| 42.0% Complete\n",
      "Iteration 330/779\n",
      "Progress |----------------------------------------------------------| 42.1% Complete\n",
      "Iteration 331/779\n",
      "Progress |----------------------------------------------------------| 42.2% Complete\n",
      "Iteration 332/779\n",
      "Progress |----------------------------------------------------------| 42.3% Complete\n",
      "Iteration 333/779\n",
      "Progress |----------------------------------------------------------| 42.5% Complete\n",
      "Iteration 334/779\n",
      "Progress |----------------------------------------------------------| 42.6% Complete\n",
      "Iteration 335/779\n",
      "Progress |----------------------------------------------------------| 42.7% Complete\n",
      "Iteration 336/779\n",
      "Progress |----------------------------------------------------------| 42.8% Complete\n",
      "Iteration 337/779\n",
      "Progress |----------------------------------------------------------| 43.0% Complete\n",
      "Iteration 338/779\n",
      "Progress |---------------------------------------------------------| 43.1% Complete\n",
      "Iteration 339/779\n",
      "Progress |---------------------------------------------------------| 43.2% Complete\n",
      "Iteration 340/779\n",
      "Progress |---------------------------------------------------------| 43.3% Complete\n",
      "Iteration 341/779\n",
      "Progress |---------------------------------------------------------| 43.5% Complete\n",
      "Iteration 342/779\n",
      "Progress |---------------------------------------------------------| 43.6% Complete\n",
      "Iteration 343/779\n",
      "Progress |---------------------------------------------------------| 43.7% Complete\n",
      "Iteration 344/779\n",
      "Progress |---------------------------------------------------------| 43.8% Complete\n",
      "Iteration 345/779\n",
      "Progress |---------------------------------------------------------| 44.0% Complete\n",
      "Iteration 346/779\n",
      "Progress |--------------------------------------------------------| 44.1% Complete\n",
      "Iteration 347/779\n",
      "Progress |--------------------------------------------------------| 44.2% Complete\n",
      "Iteration 348/779\n",
      "Progress |--------------------------------------------------------| 44.3% Complete\n",
      "Iteration 349/779\n",
      "Progress |--------------------------------------------------------| 44.5% Complete\n",
      "Iteration 350/779\n",
      "Progress |--------------------------------------------------------| 44.6% Complete\n",
      "Iteration 351/779\n",
      "Progress |--------------------------------------------------------| 44.7% Complete\n",
      "Iteration 352/779\n",
      "Progress |--------------------------------------------------------| 44.8% Complete\n",
      "Iteration 353/779\n",
      "Progress |--------------------------------------------------------| 45.0% Complete\n",
      "Iteration 354/779\n",
      "Progress |-------------------------------------------------------| 45.1% Complete\n",
      "Iteration 355/779\n",
      "Progress |-------------------------------------------------------| 45.2% Complete\n",
      "Iteration 356/779\n",
      "Progress |-------------------------------------------------------| 45.3% Complete\n",
      "Iteration 357/779\n",
      "Progress |-------------------------------------------------------| 45.5% Complete\n",
      "Iteration 358/779\n",
      "Progress |-------------------------------------------------------| 45.6% Complete\n",
      "Iteration 359/779\n",
      "Progress |-------------------------------------------------------| 45.7% Complete\n",
      "Iteration 360/779\n",
      "Progress |-------------------------------------------------------| 45.8% Complete\n",
      "Iteration 361/779\n",
      "Progress |-------------------------------------------------------| 46.0% Complete\n",
      "Iteration 362/779\n",
      "Progress |------------------------------------------------------| 46.1% Complete\n",
      "Iteration 363/779\n",
      "Progress |------------------------------------------------------| 46.2% Complete\n",
      "Iteration 364/779\n",
      "Progress |------------------------------------------------------| 46.3% Complete\n",
      "Iteration 365/779\n",
      "Progress |------------------------------------------------------| 46.5% Complete\n",
      "Iteration 366/779\n",
      "Progress |------------------------------------------------------| 46.6% Complete\n",
      "Iteration 367/779\n",
      "Progress |------------------------------------------------------| 46.7% Complete\n",
      "Iteration 368/779\n",
      "Progress |------------------------------------------------------| 46.8% Complete\n",
      "Iteration 369/779\n",
      "Progress |------------------------------------------------------| 46.9% Complete\n",
      "Iteration 370/779\n",
      "Progress |-----------------------------------------------------| 47.1% Complete\n",
      "Iteration 371/779\n",
      "Progress |-----------------------------------------------------| 47.2% Complete\n",
      "Iteration 372/779\n",
      "Progress |-----------------------------------------------------| 47.3% Complete\n",
      "Iteration 373/779\n",
      "Progress |-----------------------------------------------------| 47.4% Complete\n",
      "Iteration 374/779\n",
      "Progress |-----------------------------------------------------| 47.6% Complete\n",
      "Iteration 375/779\n",
      "Progress |-----------------------------------------------------| 47.7% Complete\n",
      "Iteration 376/779\n",
      "Progress |-----------------------------------------------------| 47.8% Complete\n",
      "Iteration 377/779\n",
      "Progress |-----------------------------------------------------| 47.9% Complete\n",
      "Iteration 378/779\n",
      "Progress |----------------------------------------------------| 48.1% Complete\n",
      "Iteration 379/779\n",
      "Progress |----------------------------------------------------| 48.2% Complete\n",
      "Iteration 380/779\n",
      "Progress |----------------------------------------------------| 48.3% Complete\n",
      "Iteration 381/779\n",
      "Progress |----------------------------------------------------| 48.4% Complete\n",
      "Iteration 382/779\n",
      "Progress |----------------------------------------------------| 48.6% Complete\n",
      "Iteration 383/779\n",
      "Progress |----------------------------------------------------| 48.7% Complete\n",
      "Iteration 384/779\n",
      "Progress |----------------------------------------------------| 48.8% Complete\n",
      "Iteration 385/779\n",
      "Progress |----------------------------------------------------| 48.9% Complete\n",
      "Iteration 386/779\n",
      "Progress |---------------------------------------------------| 49.1% Complete\n",
      "Iteration 387/779\n",
      "Progress |---------------------------------------------------| 49.2% Complete\n",
      "Iteration 388/779\n",
      "Progress |---------------------------------------------------| 49.3% Complete\n",
      "Iteration 389/779\n",
      "Progress |---------------------------------------------------| 49.4% Complete\n",
      "Iteration 390/779\n",
      "Progress |---------------------------------------------------| 49.6% Complete\n",
      "Iteration 391/779\n",
      "Progress |---------------------------------------------------| 49.7% Complete\n",
      "Iteration 392/779\n",
      "Progress |---------------------------------------------------| 49.8% Complete\n",
      "Iteration 393/779\n",
      "Progress |---------------------------------------------------| 49.9% Complete\n",
      "Iteration 394/779\n",
      "Progress |--------------------------------------------------| 50.1% Complete\n",
      "Iteration 395/779\n",
      "Progress |--------------------------------------------------| 50.2% Complete\n",
      "Iteration 396/779\n",
      "Progress |--------------------------------------------------| 50.3% Complete\n",
      "Iteration 397/779\n",
      "Progress |--------------------------------------------------| 50.4% Complete\n",
      "Iteration 398/779\n",
      "Progress |--------------------------------------------------| 50.6% Complete\n",
      "Iteration 399/779\n",
      "Progress |--------------------------------------------------| 50.7% Complete\n",
      "Iteration 400/779\n",
      "Progress |--------------------------------------------------| 50.8% Complete\n",
      "Iteration 401/779\n",
      "Progress |--------------------------------------------------| 50.9% Complete\n",
      "Iteration 402/779\n",
      "Progress |-------------------------------------------------| 51.1% Complete\n",
      "Iteration 403/779\n",
      "Progress |-------------------------------------------------| 51.2% Complete\n",
      "Iteration 404/779\n",
      "Progress |-------------------------------------------------| 51.3% Complete\n",
      "Iteration 405/779\n",
      "Progress |-------------------------------------------------| 51.4% Complete\n",
      "Iteration 406/779\n",
      "Progress |-------------------------------------------------| 51.6% Complete\n",
      "Iteration 407/779\n",
      "Progress |-------------------------------------------------| 51.7% Complete\n",
      "Iteration 408/779\n",
      "Progress |-------------------------------------------------| 51.8% Complete\n",
      "Iteration 409/779\n",
      "Progress |-------------------------------------------------| 51.9% Complete\n",
      "Iteration 410/779\n",
      "Progress |------------------------------------------------| 52.1% Complete\n",
      "Iteration 411/779\n",
      "Progress |------------------------------------------------| 52.2% Complete\n",
      "Iteration 412/779\n",
      "Progress |------------------------------------------------| 52.3% Complete\n",
      "Iteration 413/779\n",
      "Progress |------------------------------------------------| 52.4% Complete\n",
      "Iteration 414/779\n",
      "Progress |------------------------------------------------| 52.6% Complete\n",
      "Iteration 415/779\n",
      "Progress |------------------------------------------------| 52.7% Complete\n",
      "Iteration 416/779\n",
      "Progress |------------------------------------------------| 52.8% Complete\n",
      "Iteration 417/779\n",
      "Progress |------------------------------------------------| 52.9% Complete\n",
      "Iteration 418/779\n",
      "Progress |-----------------------------------------------| 53.1% Complete\n",
      "Iteration 419/779\n",
      "Progress |-----------------------------------------------| 53.2% Complete\n",
      "Iteration 420/779\n",
      "Progress |-----------------------------------------------| 53.3% Complete\n",
      "Iteration 421/779\n",
      "Progress |-----------------------------------------------| 53.4% Complete\n",
      "Iteration 422/779\n",
      "Progress |-----------------------------------------------| 53.5% Complete\n",
      "Iteration 423/779\n",
      "Progress |-----------------------------------------------| 53.7% Complete\n",
      "Iteration 424/779\n",
      "Progress |-----------------------------------------------| 53.8% Complete\n",
      "Iteration 425/779\n",
      "Progress |-----------------------------------------------| 53.9% Complete\n",
      "Iteration 426/779\n",
      "Progress |----------------------------------------------| 54.0% Complete\n",
      "Iteration 427/779\n",
      "Progress |----------------------------------------------| 54.2% Complete\n",
      "Iteration 428/779\n",
      "Progress |----------------------------------------------| 54.3% Complete\n",
      "Iteration 429/779\n",
      "Progress |----------------------------------------------| 54.4% Complete\n",
      "Iteration 430/779\n",
      "Progress |----------------------------------------------| 54.5% Complete\n",
      "Iteration 431/779\n",
      "Progress |----------------------------------------------| 54.7% Complete\n",
      "Iteration 432/779\n",
      "Progress |----------------------------------------------| 54.8% Complete\n",
      "Iteration 433/779\n",
      "Progress |----------------------------------------------| 54.9% Complete\n",
      "Iteration 434/779\n",
      "Progress |---------------------------------------------| 55.0% Complete\n",
      "Iteration 435/779\n",
      "Progress |---------------------------------------------| 55.2% Complete\n",
      "Iteration 436/779\n",
      "Progress |---------------------------------------------| 55.3% Complete\n",
      "Iteration 437/779\n",
      "Progress |---------------------------------------------| 55.4% Complete\n",
      "Iteration 438/779\n",
      "Progress |---------------------------------------------| 55.5% Complete\n",
      "Iteration 439/779\n",
      "Progress |---------------------------------------------| 55.7% Complete\n",
      "Iteration 440/779\n",
      "Progress |---------------------------------------------| 55.8% Complete\n",
      "Iteration 441/779\n",
      "Progress |---------------------------------------------| 55.9% Complete\n",
      "Iteration 442/779\n",
      "Progress |--------------------------------------------| 56.0% Complete\n",
      "Iteration 443/779\n",
      "Progress |--------------------------------------------| 56.2% Complete\n",
      "Iteration 444/779\n",
      "Progress |--------------------------------------------| 56.3% Complete\n",
      "Iteration 445/779\n",
      "Progress |--------------------------------------------| 56.4% Complete\n",
      "Iteration 446/779\n",
      "Progress |--------------------------------------------| 56.5% Complete\n",
      "Iteration 447/779\n",
      "Progress |--------------------------------------------| 56.7% Complete\n",
      "Iteration 448/779\n",
      "Progress |--------------------------------------------| 56.8% Complete\n",
      "Iteration 449/779\n",
      "Progress |--------------------------------------------| 56.9% Complete\n",
      "Iteration 450/779\n",
      "Progress |-------------------------------------------| 57.0% Complete\n",
      "Iteration 451/779\n",
      "Progress |-------------------------------------------| 57.2% Complete\n",
      "Iteration 452/779\n",
      "Progress |-------------------------------------------| 57.3% Complete\n",
      "Iteration 453/779\n",
      "Progress |-------------------------------------------| 57.4% Complete\n",
      "Iteration 454/779\n",
      "Progress |-------------------------------------------| 57.5% Complete\n",
      "Iteration 455/779\n",
      "Progress |-------------------------------------------| 57.7% Complete\n",
      "Iteration 456/779\n",
      "Progress |-------------------------------------------| 57.8% Complete\n",
      "Iteration 457/779\n",
      "Progress |-------------------------------------------| 57.9% Complete\n",
      "Iteration 458/779\n",
      "Progress |------------------------------------------| 58.0% Complete\n",
      "Iteration 459/779\n",
      "Progress |------------------------------------------| 58.2% Complete\n",
      "Iteration 460/779\n",
      "Progress |------------------------------------------| 58.3% Complete\n",
      "Iteration 461/779\n",
      "Progress |------------------------------------------| 58.4% Complete\n",
      "Iteration 462/779\n",
      "Progress |------------------------------------------| 58.5% Complete\n",
      "Iteration 463/779\n",
      "Progress |------------------------------------------| 58.7% Complete\n",
      "Iteration 464/779\n",
      "Progress |------------------------------------------| 58.8% Complete\n",
      "Iteration 465/779\n",
      "Progress |------------------------------------------| 58.9% Complete\n",
      "Iteration 466/779\n",
      "Progress |-----------------------------------------| 59.0% Complete\n",
      "Iteration 467/779\n",
      "Progress |-----------------------------------------| 59.2% Complete\n",
      "Iteration 468/779\n",
      "Progress |-----------------------------------------| 59.3% Complete\n",
      "Iteration 469/779\n",
      "Progress |-----------------------------------------| 59.4% Complete\n",
      "Iteration 470/779\n",
      "Progress |-----------------------------------------| 59.5% Complete\n",
      "Iteration 471/779\n",
      "Progress |-----------------------------------------| 59.7% Complete\n",
      "Iteration 472/779\n",
      "Progress |-----------------------------------------| 59.8% Complete\n",
      "Iteration 473/779\n",
      "Progress |-----------------------------------------| 59.9% Complete\n",
      "Iteration 474/779\n",
      "Progress |----------------------------------------| 60.0% Complete\n",
      "Iteration 475/779\n",
      "Progress |----------------------------------------| 60.1% Complete\n",
      "Iteration 476/779\n",
      "Progress |----------------------------------------| 60.3% Complete\n",
      "Iteration 477/779\n",
      "Progress |----------------------------------------| 60.4% Complete\n",
      "Iteration 478/779\n",
      "Progress |----------------------------------------| 60.5% Complete\n",
      "Iteration 479/779\n",
      "Progress |----------------------------------------| 60.6% Complete\n",
      "Iteration 480/779\n",
      "Progress |----------------------------------------| 60.8% Complete\n",
      "Iteration 481/779\n",
      "Progress |----------------------------------------| 60.9% Complete\n",
      "Iteration 482/779\n",
      "Progress |---------------------------------------| 61.0% Complete\n",
      "Iteration 483/779\n",
      "Progress |---------------------------------------| 61.1% Complete\n",
      "Iteration 484/779\n",
      "Progress |---------------------------------------| 61.3% Complete\n",
      "Iteration 485/779\n",
      "Progress |---------------------------------------| 61.4% Complete\n",
      "Iteration 486/779\n",
      "Progress |---------------------------------------| 61.5% Complete\n",
      "Iteration 487/779\n",
      "Progress |---------------------------------------| 61.6% Complete\n",
      "Iteration 488/779\n",
      "Progress |---------------------------------------| 61.8% Complete\n",
      "Iteration 489/779\n",
      "Progress |---------------------------------------| 61.9% Complete\n",
      "Iteration 490/779\n",
      "Progress |--------------------------------------| 62.0% Complete\n",
      "Iteration 491/779\n",
      "Progress |--------------------------------------| 62.1% Complete\n",
      "Iteration 492/779\n",
      "Progress |--------------------------------------| 62.3% Complete\n",
      "Iteration 493/779\n",
      "Progress |--------------------------------------| 62.4% Complete\n",
      "Iteration 494/779\n",
      "Progress |--------------------------------------| 62.5% Complete\n",
      "Iteration 495/779\n",
      "Progress |--------------------------------------| 62.6% Complete\n",
      "Iteration 496/779\n",
      "Progress |--------------------------------------| 62.8% Complete\n",
      "Iteration 497/779\n",
      "Progress |--------------------------------------| 62.9% Complete\n",
      "Iteration 498/779\n",
      "Progress |-------------------------------------| 63.0% Complete\n",
      "Iteration 499/779\n",
      "Progress |-------------------------------------| 63.1% Complete\n",
      "Iteration 500/779\n",
      "Progress |-------------------------------------| 63.3% Complete\n",
      "Iteration 501/779\n",
      "Progress |-------------------------------------| 63.4% Complete\n",
      "Iteration 502/779\n",
      "Progress |-------------------------------------| 63.5% Complete\n",
      "Iteration 503/779\n",
      "Progress |-------------------------------------| 63.6% Complete\n",
      "Iteration 504/779\n",
      "Progress |-------------------------------------| 63.8% Complete\n",
      "Iteration 505/779\n",
      "Progress |-------------------------------------| 63.9% Complete\n",
      "Iteration 506/779\n",
      "Progress |------------------------------------| 64.0% Complete\n",
      "Iteration 507/779\n",
      "Progress |------------------------------------| 64.1% Complete\n",
      "Iteration 508/779\n",
      "Progress |------------------------------------| 64.3% Complete\n",
      "Iteration 509/779\n",
      "Progress |------------------------------------| 64.4% Complete\n",
      "Iteration 510/779\n",
      "Progress |------------------------------------| 64.5% Complete\n",
      "Iteration 511/779\n",
      "Progress |------------------------------------| 64.6% Complete\n",
      "Iteration 512/779\n",
      "Progress |------------------------------------| 64.8% Complete\n",
      "Iteration 513/779\n",
      "Progress |------------------------------------| 64.9% Complete\n",
      "Iteration 514/779\n",
      "Progress |-----------------------------------| 65.0% Complete\n",
      "Iteration 515/779\n",
      "Progress |-----------------------------------| 65.1% Complete\n",
      "Iteration 516/779\n",
      "Progress |-----------------------------------| 65.3% Complete\n",
      "Iteration 517/779\n",
      "Progress |-----------------------------------| 65.4% Complete\n",
      "Iteration 518/779\n",
      "Progress |-----------------------------------| 65.5% Complete\n",
      "Iteration 519/779\n",
      "Progress |-----------------------------------| 65.6% Complete\n",
      "Iteration 520/779\n",
      "Progress |-----------------------------------| 65.8% Complete\n",
      "Iteration 521/779\n",
      "Progress |-----------------------------------| 65.9% Complete\n",
      "Iteration 522/779\n",
      "Progress |----------------------------------| 66.0% Complete\n",
      "Iteration 523/779\n",
      "Progress |----------------------------------| 66.1% Complete\n",
      "Iteration 524/779\n",
      "Progress |----------------------------------| 66.3% Complete\n",
      "Iteration 525/779\n",
      "Progress |----------------------------------| 66.4% Complete\n",
      "Iteration 526/779\n",
      "Progress |----------------------------------| 66.5% Complete\n",
      "Iteration 527/779\n",
      "Progress |----------------------------------| 66.6% Complete\n",
      "Iteration 528/779\n",
      "Progress |----------------------------------| 66.7% Complete\n",
      "Iteration 529/779\n",
      "Progress |----------------------------------| 66.9% Complete\n",
      "Iteration 530/779\n",
      "Progress |----------------------------------| 67.0% Complete\n",
      "Iteration 531/779\n",
      "Progress |---------------------------------| 67.1% Complete\n",
      "Iteration 532/779\n",
      "Progress |---------------------------------| 67.2% Complete\n",
      "Iteration 533/779\n",
      "Progress |---------------------------------| 67.4% Complete\n",
      "Iteration 534/779\n",
      "Progress |---------------------------------| 67.5% Complete\n",
      "Iteration 535/779\n",
      "Progress |---------------------------------| 67.6% Complete\n",
      "Iteration 536/779\n",
      "Progress |---------------------------------| 67.7% Complete\n",
      "Iteration 537/779\n",
      "Progress |---------------------------------| 67.9% Complete\n",
      "Iteration 538/779\n",
      "Progress |---------------------------------| 68.0% Complete\n",
      "Iteration 539/779\n",
      "Progress |--------------------------------| 68.1% Complete\n",
      "Iteration 540/779\n",
      "Progress |--------------------------------| 68.2% Complete\n",
      "Iteration 541/779\n",
      "Progress |--------------------------------| 68.4% Complete\n",
      "Iteration 542/779\n",
      "Progress |--------------------------------| 68.5% Complete\n",
      "Iteration 543/779\n",
      "Progress |--------------------------------| 68.6% Complete\n",
      "Iteration 544/779\n",
      "Progress |--------------------------------| 68.7% Complete\n",
      "Iteration 545/779\n",
      "Progress |--------------------------------| 68.9% Complete\n",
      "Iteration 546/779\n",
      "Progress |--------------------------------| 69.0% Complete\n",
      "Iteration 547/779\n",
      "Progress |-------------------------------| 69.1% Complete\n",
      "Iteration 548/779\n",
      "Progress |-------------------------------| 69.2% Complete\n",
      "Iteration 549/779\n",
      "Progress |-------------------------------| 69.4% Complete\n",
      "Iteration 550/779\n",
      "Progress |-------------------------------| 69.5% Complete\n",
      "Iteration 551/779\n",
      "Progress |-------------------------------| 69.6% Complete\n",
      "Iteration 552/779\n",
      "Progress |-------------------------------| 69.7% Complete\n",
      "Iteration 553/779\n",
      "Progress |-------------------------------| 69.9% Complete\n",
      "Iteration 554/779\n",
      "Progress |-------------------------------| 70.0% Complete\n",
      "Iteration 555/779\n",
      "Progress |------------------------------| 70.1% Complete\n",
      "Iteration 556/779\n",
      "Progress |------------------------------| 70.2% Complete\n",
      "Iteration 557/779\n",
      "Progress |------------------------------| 70.4% Complete\n",
      "Iteration 558/779\n",
      "Progress |------------------------------| 70.5% Complete\n",
      "Iteration 559/779\n",
      "Progress |------------------------------| 70.6% Complete\n",
      "Iteration 560/779\n",
      "Progress |------------------------------| 70.7% Complete\n",
      "Iteration 561/779\n",
      "Progress |------------------------------| 70.9% Complete\n",
      "Iteration 562/779\n",
      "Progress |------------------------------| 71.0% Complete\n",
      "Iteration 563/779\n",
      "Progress |-----------------------------| 71.1% Complete\n",
      "Iteration 564/779\n",
      "Progress |-----------------------------| 71.2% Complete\n",
      "Iteration 565/779\n",
      "Progress |-----------------------------| 71.4% Complete\n",
      "Iteration 566/779\n",
      "Progress |-----------------------------| 71.5% Complete\n",
      "Iteration 567/779\n",
      "Progress |-----------------------------| 71.6% Complete\n",
      "Iteration 568/779\n",
      "Progress |-----------------------------| 71.7% Complete\n",
      "Iteration 569/779\n",
      "Progress |-----------------------------| 71.9% Complete\n",
      "Iteration 570/779\n",
      "Progress |-----------------------------| 72.0% Complete\n",
      "Iteration 571/779\n",
      "Progress |----------------------------| 72.1% Complete\n",
      "Iteration 572/779\n",
      "Progress |----------------------------| 72.2% Complete\n",
      "Iteration 573/779\n",
      "Progress |----------------------------| 72.4% Complete\n",
      "Iteration 574/779\n",
      "Progress |----------------------------| 72.5% Complete\n",
      "Iteration 575/779\n",
      "Progress |----------------------------| 72.6% Complete\n",
      "Iteration 576/779\n",
      "Progress |----------------------------| 72.7% Complete\n",
      "Iteration 577/779\n",
      "Progress |----------------------------| 72.9% Complete\n",
      "Iteration 578/779\n",
      "Progress |----------------------------| 73.0% Complete\n",
      "Iteration 579/779\n",
      "Progress |---------------------------| 73.1% Complete\n",
      "Iteration 580/779\n",
      "Progress |---------------------------| 73.2% Complete\n",
      "Iteration 581/779\n",
      "Progress |---------------------------| 73.3% Complete\n",
      "Iteration 582/779\n",
      "Progress |---------------------------| 73.5% Complete\n",
      "Iteration 583/779\n",
      "Progress |---------------------------| 73.6% Complete\n",
      "Iteration 584/779\n",
      "Progress |---------------------------| 73.7% Complete\n",
      "Iteration 585/779\n",
      "Progress |---------------------------| 73.8% Complete\n",
      "Iteration 586/779\n",
      "Progress |---------------------------| 74.0% Complete\n",
      "Iteration 587/779\n",
      "Progress |--------------------------| 74.1% Complete\n",
      "Iteration 588/779\n",
      "Progress |--------------------------| 74.2% Complete\n",
      "Iteration 589/779\n",
      "Progress |--------------------------| 74.3% Complete\n",
      "Iteration 590/779\n",
      "Progress |--------------------------| 74.5% Complete\n",
      "Iteration 591/779\n",
      "Progress |--------------------------| 74.6% Complete\n",
      "Iteration 592/779\n",
      "Progress |--------------------------| 74.7% Complete\n",
      "Iteration 593/779\n",
      "Progress |--------------------------| 74.8% Complete\n",
      "Iteration 594/779\n",
      "Progress |--------------------------| 75.0% Complete\n",
      "Iteration 595/779\n",
      "Progress |-------------------------| 75.1% Complete\n",
      "Iteration 596/779\n",
      "Progress |-------------------------| 75.2% Complete\n",
      "Iteration 597/779\n",
      "Progress |-------------------------| 75.3% Complete\n",
      "Iteration 598/779\n",
      "Progress |-------------------------| 75.5% Complete\n",
      "Iteration 599/779\n",
      "Progress |-------------------------| 75.6% Complete\n",
      "Iteration 600/779\n",
      "Progress |-------------------------| 75.7% Complete\n",
      "Iteration 601/779\n",
      "Progress |-------------------------| 75.8% Complete\n",
      "Iteration 602/779\n",
      "Progress |-------------------------| 76.0% Complete\n",
      "Iteration 603/779\n",
      "Progress |------------------------| 76.1% Complete\n",
      "Iteration 604/779\n",
      "Progress |------------------------| 76.2% Complete\n",
      "Iteration 605/779\n",
      "Progress |------------------------| 76.3% Complete\n",
      "Iteration 606/779\n",
      "Progress |------------------------| 76.5% Complete\n",
      "Iteration 607/779\n",
      "Progress |------------------------| 76.6% Complete\n",
      "Iteration 608/779\n",
      "Progress |------------------------| 76.7% Complete\n",
      "Iteration 609/779\n",
      "Progress |------------------------| 76.8% Complete\n",
      "Iteration 610/779\n",
      "Progress |------------------------| 77.0% Complete\n",
      "Iteration 611/779\n",
      "Progress |-----------------------| 77.1% Complete\n",
      "Iteration 612/779\n",
      "Progress |-----------------------| 77.2% Complete\n",
      "Iteration 613/779\n",
      "Progress |-----------------------| 77.3% Complete\n",
      "Iteration 614/779\n",
      "Progress |-----------------------| 77.5% Complete\n",
      "Iteration 615/779\n",
      "Progress |-----------------------| 77.6% Complete\n",
      "Iteration 616/779\n",
      "Progress |-----------------------| 77.7% Complete\n",
      "Iteration 617/779\n",
      "Progress |-----------------------| 77.8% Complete\n",
      "Iteration 618/779\n",
      "Progress |-----------------------| 78.0% Complete\n",
      "Iteration 619/779\n",
      "Progress |----------------------| 78.1% Complete\n",
      "Iteration 620/779\n",
      "Progress |----------------------| 78.2% Complete\n",
      "Iteration 621/779\n",
      "Progress |----------------------| 78.3% Complete\n",
      "Iteration 622/779\n",
      "Progress |----------------------| 78.5% Complete\n",
      "Iteration 623/779\n",
      "Progress |----------------------| 78.6% Complete\n",
      "Iteration 624/779\n",
      "Progress |----------------------| 78.7% Complete\n",
      "Iteration 625/779\n",
      "Progress |----------------------| 78.8% Complete\n",
      "Iteration 626/779\n",
      "Progress |----------------------| 79.0% Complete\n",
      "Iteration 627/779\n",
      "Progress |---------------------| 79.1% Complete\n",
      "Iteration 628/779\n",
      "Progress |---------------------| 79.2% Complete\n",
      "Iteration 629/779\n",
      "Progress |---------------------| 79.3% Complete\n",
      "Iteration 630/779\n",
      "Progress |---------------------| 79.5% Complete\n",
      "Iteration 631/779\n",
      "Progress |---------------------| 79.6% Complete\n",
      "Iteration 632/779\n",
      "Progress |---------------------| 79.7% Complete\n",
      "Iteration 633/779\n",
      "Progress |---------------------| 79.8% Complete\n",
      "Iteration 634/779\n",
      "Progress |---------------------| 80.0% Complete\n",
      "Iteration 635/779\n",
      "Progress |--------------------| 80.1% Complete\n",
      "Iteration 636/779\n",
      "Progress |--------------------| 80.2% Complete\n",
      "Iteration 637/779\n",
      "Progress |--------------------| 80.3% Complete\n",
      "Iteration 638/779\n",
      "Progress |--------------------| 80.4% Complete\n",
      "Iteration 639/779\n",
      "Progress |--------------------| 80.6% Complete\n",
      "Iteration 640/779\n",
      "Progress |--------------------| 80.7% Complete\n",
      "Iteration 641/779\n",
      "Progress |--------------------| 80.8% Complete\n",
      "Iteration 642/779\n",
      "Progress |--------------------| 80.9% Complete\n",
      "Iteration 643/779\n",
      "Progress |-------------------| 81.1% Complete\n",
      "Iteration 644/779\n",
      "Progress |-------------------| 81.2% Complete\n",
      "Iteration 645/779\n",
      "Progress |-------------------| 81.3% Complete\n",
      "Iteration 646/779\n",
      "Progress |-------------------| 81.4% Complete\n",
      "Iteration 647/779\n",
      "Progress |-------------------| 81.6% Complete\n",
      "Iteration 648/779\n",
      "Progress |-------------------| 81.7% Complete\n",
      "Iteration 649/779\n",
      "Progress |-------------------| 81.8% Complete\n",
      "Iteration 650/779\n",
      "Progress |-------------------| 81.9% Complete\n",
      "Iteration 651/779\n",
      "Progress |------------------| 82.1% Complete\n",
      "Iteration 652/779\n",
      "Progress |------------------| 82.2% Complete\n",
      "Iteration 653/779\n",
      "Progress |------------------| 82.3% Complete\n",
      "Iteration 654/779\n",
      "Progress |------------------| 82.4% Complete\n",
      "Iteration 655/779\n",
      "Progress |------------------| 82.6% Complete\n",
      "Iteration 656/779\n",
      "Progress |------------------| 82.7% Complete\n",
      "Iteration 657/779\n",
      "Progress |------------------| 82.8% Complete\n",
      "Iteration 658/779\n",
      "Progress |------------------| 82.9% Complete\n",
      "Iteration 659/779\n",
      "Progress |-----------------| 83.1% Complete\n",
      "Iteration 660/779\n",
      "Progress |-----------------| 83.2% Complete\n",
      "Iteration 661/779\n",
      "Progress |-----------------| 83.3% Complete\n",
      "Iteration 662/779\n",
      "Progress |-----------------| 83.4% Complete\n",
      "Iteration 663/779\n",
      "Progress |-----------------| 83.6% Complete\n",
      "Iteration 664/779\n",
      "Progress |-----------------| 83.7% Complete\n",
      "Iteration 665/779\n",
      "Progress |-----------------| 83.8% Complete\n",
      "Iteration 666/779\n",
      "Progress |-----------------| 83.9% Complete\n",
      "Iteration 667/779\n",
      "Progress |----------------| 84.1% Complete\n",
      "Iteration 668/779\n",
      "Progress |----------------| 84.2% Complete\n",
      "Iteration 669/779\n",
      "Progress |----------------| 84.3% Complete\n",
      "Iteration 670/779\n",
      "Progress |----------------| 84.4% Complete\n",
      "Iteration 671/779\n",
      "Progress |----------------| 84.6% Complete\n",
      "Iteration 672/779\n",
      "Progress |----------------| 84.7% Complete\n",
      "Iteration 673/779\n",
      "Progress |----------------| 84.8% Complete\n",
      "Iteration 674/779\n",
      "Progress |----------------| 84.9% Complete\n",
      "Iteration 675/779\n",
      "Progress |---------------| 85.1% Complete\n",
      "Iteration 676/779\n",
      "Progress |---------------| 85.2% Complete\n",
      "Iteration 677/779\n",
      "Progress |---------------| 85.3% Complete\n",
      "Iteration 678/779\n",
      "Progress |---------------| 85.4% Complete\n",
      "Iteration 679/779\n",
      "Progress |---------------| 85.6% Complete\n",
      "Iteration 680/779\n",
      "Progress |---------------| 85.7% Complete\n",
      "Iteration 681/779\n",
      "Progress |---------------| 85.8% Complete\n",
      "Iteration 682/779\n",
      "Progress |---------------| 85.9% Complete\n",
      "Iteration 683/779\n",
      "Progress |--------------| 86.1% Complete\n",
      "Iteration 684/779\n",
      "Progress |--------------| 86.2% Complete\n",
      "Iteration 685/779\n",
      "Progress |--------------| 86.3% Complete\n",
      "Iteration 686/779\n",
      "Progress |--------------| 86.4% Complete\n",
      "Iteration 687/779\n",
      "Progress |--------------| 86.6% Complete\n",
      "Iteration 688/779\n",
      "Progress |--------------| 86.7% Complete\n",
      "Iteration 689/779\n",
      "Progress |--------------| 86.8% Complete\n",
      "Iteration 690/779\n",
      "Progress |--------------| 86.9% Complete\n",
      "Iteration 691/779\n",
      "Progress |-------------| 87.0% Complete\n",
      "Iteration 692/779\n",
      "Progress |-------------| 87.2% Complete\n",
      "Iteration 693/779\n",
      "Progress |-------------| 87.3% Complete\n",
      "Iteration 694/779\n",
      "Progress |-------------| 87.4% Complete\n",
      "Iteration 695/779\n",
      "Progress |-------------| 87.5% Complete\n",
      "Iteration 696/779\n",
      "Progress |-------------| 87.7% Complete\n",
      "Iteration 697/779\n",
      "Progress |-------------| 87.8% Complete\n",
      "Iteration 698/779\n",
      "Progress |-------------| 87.9% Complete\n",
      "Iteration 699/779\n",
      "Progress |------------| 88.0% Complete\n",
      "Iteration 700/779\n",
      "Progress |------------| 88.2% Complete\n",
      "Iteration 701/779\n",
      "Progress |------------| 88.3% Complete\n",
      "Iteration 702/779\n",
      "Progress |------------| 88.4% Complete\n",
      "Iteration 703/779\n",
      "Progress |------------| 88.5% Complete\n",
      "Iteration 704/779\n",
      "Progress |------------| 88.7% Complete\n",
      "Iteration 705/779\n",
      "Progress |------------| 88.8% Complete\n",
      "Iteration 706/779\n",
      "Progress |------------| 88.9% Complete\n",
      "Iteration 707/779\n",
      "Progress |-----------| 89.0% Complete\n",
      "Iteration 708/779\n",
      "Progress |-----------| 89.2% Complete\n",
      "Iteration 709/779\n",
      "Progress |-----------| 89.3% Complete\n",
      "Iteration 710/779\n",
      "Progress |-----------| 89.4% Complete\n",
      "Iteration 711/779\n",
      "Progress |-----------| 89.5% Complete\n",
      "Iteration 712/779\n",
      "Progress |-----------| 89.7% Complete\n",
      "Iteration 713/779\n",
      "Progress |-----------| 89.8% Complete\n",
      "Iteration 714/779\n",
      "Progress |-----------| 89.9% Complete\n",
      "Iteration 715/779\n",
      "Progress |----------| 90.0% Complete\n",
      "Iteration 716/779\n",
      "Progress |----------| 90.2% Complete\n",
      "Iteration 717/779\n",
      "Progress |----------| 90.3% Complete\n",
      "Iteration 718/779\n",
      "Progress |----------| 90.4% Complete\n",
      "Iteration 719/779\n",
      "Progress |----------| 90.5% Complete\n",
      "Iteration 720/779\n",
      "Progress |----------| 90.7% Complete\n",
      "Iteration 721/779\n",
      "Progress |----------| 90.8% Complete\n",
      "Iteration 722/779\n",
      "Progress |----------| 90.9% Complete\n",
      "Iteration 723/779\n",
      "Progress |---------| 91.0% Complete\n",
      "Iteration 724/779\n",
      "Progress |---------| 91.2% Complete\n",
      "Iteration 725/779\n",
      "Progress |---------| 91.3% Complete\n",
      "Iteration 726/779\n",
      "Progress |---------| 91.4% Complete\n",
      "Iteration 727/779\n",
      "Progress |---------| 91.5% Complete\n",
      "Iteration 728/779\n",
      "Progress |---------| 91.7% Complete\n",
      "Iteration 729/779\n",
      "Progress |---------| 91.8% Complete\n",
      "Iteration 730/779\n",
      "Progress |---------| 91.9% Complete\n",
      "Iteration 731/779\n",
      "Progress |--------| 92.0% Complete\n",
      "Iteration 732/779\n",
      "Progress |--------| 92.2% Complete\n",
      "Iteration 733/779\n",
      "Progress |--------| 92.3% Complete\n",
      "Iteration 734/779\n",
      "Progress |--------| 92.4% Complete\n",
      "Iteration 735/779\n",
      "Progress |--------| 92.5% Complete\n",
      "Iteration 736/779\n",
      "Progress |--------| 92.7% Complete\n",
      "Iteration 737/779\n",
      "Progress |--------| 92.8% Complete\n",
      "Iteration 738/779\n",
      "Progress |--------| 92.9% Complete\n",
      "Iteration 739/779\n",
      "Progress |-------| 93.0% Complete\n",
      "Iteration 740/779\n",
      "Progress |-------| 93.2% Complete\n",
      "Iteration 741/779\n",
      "Progress |-------| 93.3% Complete\n",
      "Iteration 742/779\n",
      "Progress |-------| 93.4% Complete\n",
      "Iteration 743/779\n",
      "Progress |-------| 93.5% Complete\n",
      "Iteration 744/779\n",
      "Progress |-------| 93.6% Complete\n",
      "Iteration 745/779\n",
      "Progress |-------| 93.8% Complete\n",
      "Iteration 746/779\n",
      "Progress |-------| 93.9% Complete\n",
      "Iteration 747/779\n",
      "Progress |------| 94.0% Complete\n",
      "Iteration 748/779\n",
      "Progress |------| 94.1% Complete\n",
      "Iteration 749/779\n",
      "Progress |------| 94.3% Complete\n",
      "Iteration 750/779\n",
      "Progress |------| 94.4% Complete\n",
      "Iteration 751/779\n",
      "Progress |------| 94.5% Complete\n",
      "Iteration 752/779\n",
      "Progress |------| 94.6% Complete\n",
      "Iteration 753/779\n",
      "Progress |------| 94.8% Complete\n",
      "Iteration 754/779\n",
      "Progress |------| 94.9% Complete\n",
      "Iteration 755/779\n",
      "Progress |-----| 95.0% Complete\n",
      "Iteration 756/779\n",
      "Progress |-----| 95.1% Complete\n",
      "Iteration 757/779\n",
      "Progress |-----| 95.3% Complete\n",
      "Iteration 758/779\n",
      "Progress |-----| 95.4% Complete\n",
      "Iteration 759/779\n",
      "Progress |-----| 95.5% Complete\n",
      "Iteration 760/779\n",
      "Progress |-----| 95.6% Complete\n",
      "Iteration 761/779\n",
      "Progress |-----| 95.8% Complete\n",
      "Iteration 762/779\n",
      "Progress |-----| 95.9% Complete\n",
      "Iteration 763/779\n",
      "Progress |----| 96.0% Complete\n",
      "Iteration 764/779\n",
      "Progress |----| 96.1% Complete\n",
      "Iteration 765/779\n",
      "Progress |----| 96.3% Complete\n",
      "Iteration 766/779\n",
      "Progress |----| 96.4% Complete\n",
      "Iteration 767/779\n",
      "Progress |----| 96.5% Complete\n",
      "Iteration 768/779\n",
      "Progress |----| 96.6% Complete\n",
      "Iteration 769/779\n",
      "Progress |----| 96.8% Complete\n",
      "Iteration 770/779\n",
      "Progress |----| 96.9% Complete\n",
      "Iteration 771/779\n",
      "Progress |---| 97.0% Complete\n",
      "Iteration 772/779\n",
      "Progress |---| 97.1% Complete\n",
      "Iteration 773/779\n",
      "Progress |---| 97.3% Complete\n",
      "Iteration 774/779\n",
      "Progress |---| 97.4% Complete\n",
      "Iteration 775/779\n",
      "Progress |---| 97.5% Complete\n",
      "Iteration 776/779\n",
      "Progress |---| 97.6% Complete\n",
      "Iteration 777/779\n",
      "Progress |---| 97.8% Complete\n",
      "Iteration 778/779\n",
      "Progress |---| 97.9% Complete\n",
      "Done!\n",
      "Collecting top-k prunning information...\n",
      "Iteration 1/5\n",
      "Progress |--| 98.0% Complete\n",
      "Iteration 2/5\n",
      "Progress |--| 98.1% Complete\n",
      "Iteration 3/5\n",
      "Progress |--| 98.3% Complete\n",
      "Iteration 4/5\n",
      "Progress |--| 98.4% Complete\n",
      "Iteration 5/5\n",
      "Progress |--| 98.5% Complete\n",
      "Done!\n",
      "Collecting CCP prunning information...\n",
      "Iteration 0/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6929005101875226\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3504219188548001\n",
      "Student model 0-0 fidelity: 0.3504219188548001\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3502107684458513\n",
      "Student model 0-1 fidelity: 0.3502107684458513\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35028831184716575\n",
      "Student model 0-2 fidelity: 0.35028831184716575\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3503414064750534\n",
      "Student model 0-3 fidelity: 0.3503414064750534\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35059528783377236\n",
      "Student model 0-4 fidelity: 0.35059528783377236\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35021051858604274\n",
      "Student model 1-0 fidelity: 0.35021051858604274\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3503188582988946\n",
      "Student model 1-1 fidelity: 0.3503188582988946\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35058084511363347\n",
      "Student model 1-2 fidelity: 0.35058084511363347\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35065354315419\n",
      "Student model 1-3 fidelity: 0.35065354315419\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3504682536019494\n",
      "Student model 1-4 fidelity: 0.3504682536019494\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3501522070597813\n",
      "Student model 2-0 fidelity: 0.3501522070597813\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35072377824755646\n",
      "Student model 2-1 fidelity: 0.35072377824755646\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35018125671606265\n",
      "Student model 2-2 fidelity: 0.35018125671606265\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35048075463850625\n",
      "Student model 2-3 fidelity: 0.35048075463850625\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3499398053361573\n",
      "Student model 2-4 fidelity: 0.3499398053361573\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35043802879911873\n",
      "Student model 3-0 fidelity: 0.35043802879911873\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3506239326806495\n",
      "Student model 3-1 fidelity: 0.3506239326806495\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3508096105333192\n",
      "Student model 3-2 fidelity: 0.3508096105333192\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35021853849990814\n",
      "Student model 3-3 fidelity: 0.35021853849990814\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35055855961719623\n",
      "Student model 3-4 fidelity: 0.35055855961719623\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3504528934091759\n",
      "Student model 4-0 fidelity: 0.3504528934091759\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3503999753451134\n",
      "Student model 4-1 fidelity: 0.3503999753451134\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.350752215763813\n",
      "Student model 4-2 fidelity: 0.350752215763813\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3503295132052602\n",
      "Student model 4-3 fidelity: 0.3503295132052602\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3503568852885417\n",
      "Student model 4-4 fidelity: 0.3503568852885417\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999882223676527, 0.3508096105333192)\n",
      "Top-k Prunned explanation size: 11\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.515     1.000     0.680     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.800   1648161\n",
      "   macro avg      0.331     0.385     0.350   1648161\n",
      "weighted avg      0.713     0.800     0.750   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.515     1.000     0.680     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.800   1648161\n",
      "   macro avg      0.331     0.385     0.350   1648161\n",
      "weighted avg      0.713     0.800     0.750   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.822     0.988     0.897    173016\n",
      "           8      0.427     1.000     0.598     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.774   1648161\n",
      "   macro avg      0.257     0.308     0.273   1648161\n",
      "weighted avg      0.675     0.774     0.716   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.822     0.988     0.897    173016\n",
      "           8      0.427     1.000     0.598     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.774   1648161\n",
      "   macro avg      0.257     0.308     0.273   1648161\n",
      "weighted avg      0.675     0.774     0.716   1648161\n",
      "\n",
      "Progress |--| 98.6% Complete\n",
      "Iteration 1/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.692476799834841\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26706134356776046\n",
      "Student model 0-0 fidelity: 0.26706134356776046\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26675995416920545\n",
      "Student model 0-1 fidelity: 0.26675995416920545\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26690806666013195\n",
      "Student model 0-2 fidelity: 0.26690806666013195\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26679817416326823\n",
      "Student model 0-3 fidelity: 0.26679817416326823\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26723873450881885\n",
      "Student model 0-4 fidelity: 0.26723873450881885\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26687267609977877\n",
      "Student model 1-0 fidelity: 0.26687267609977877\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2666274365883092\n",
      "Student model 1-1 fidelity: 0.2666274365883092\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2671051737640581\n",
      "Student model 1-2 fidelity: 0.2671051737640581\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26688080881872717\n",
      "Student model 1-3 fidelity: 0.26688080881872717\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.266966796815595\n",
      "Student model 1-4 fidelity: 0.266966796815595\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2669363010221149\n",
      "Student model 2-0 fidelity: 0.2669363010221149\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26717210724200413\n",
      "Student model 2-1 fidelity: 0.26717210724200413\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2668788605761076\n",
      "Student model 2-2 fidelity: 0.2668788605761076\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.266975208478989\n",
      "Student model 2-3 fidelity: 0.266975208478989\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2668385674092974\n",
      "Student model 2-4 fidelity: 0.2668385674092974\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23735829545649131\n",
      "Student model 3-0 fidelity: 0.23735829545649131\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2669957739607735\n",
      "Student model 3-1 fidelity: 0.2669957739607735\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23779996687788488\n",
      "Student model 3-2 fidelity: 0.23779996687788488\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26689746189307123\n",
      "Student model 3-3 fidelity: 0.26689746189307123\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2667523812501194\n",
      "Student model 3-4 fidelity: 0.2667523812501194\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23773694894950181\n",
      "Student model 4-0 fidelity: 0.23773694894950181\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2673564339758237\n",
      "Student model 4-1 fidelity: 0.2673564339758237\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2671958313331469\n",
      "Student model 4-2 fidelity: 0.2671958313331469\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23792069060478885\n",
      "Student model 4-3 fidelity: 0.23792069060478885\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26698496152933254\n",
      "Student model 4-4 fidelity: 0.26698496152933254\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999943548480138, 0.26723873450881885)\n",
      "Top-k Prunned explanation size: 9\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.758   1648161\n",
      "   macro avg      0.247     0.302     0.267   1648161\n",
      "weighted avg      0.649     0.758     0.694   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.758   1648161\n",
      "   macro avg      0.247     0.302     0.267   1648161\n",
      "weighted avg      0.649     0.758     0.694   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.821     0.988     0.897    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.739   1648161\n",
      "   macro avg      0.196     0.241     0.212   1648161\n",
      "weighted avg      0.619     0.739     0.668   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.821     0.988     0.897    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.739   1648161\n",
      "   macro avg      0.196     0.241     0.212   1648161\n",
      "weighted avg      0.619     0.739     0.668   1648161\n",
      "\n",
      "Progress |--| 98.8% Complete\n",
      "Iteration 2/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6916080577529683\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23794555684830856\n",
      "Student model 0-0 fidelity: 0.23794555684830856\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23766131247886488\n",
      "Student model 0-1 fidelity: 0.23766131247886488\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23765640811217326\n",
      "Student model 0-2 fidelity: 0.23765640811217326\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23764707135091193\n",
      "Student model 0-3 fidelity: 0.23764707135091193\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23770121373780007\n",
      "Student model 0-4 fidelity: 0.23770121373780007\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23748209356607886\n",
      "Student model 1-0 fidelity: 0.23748209356607886\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23754662869077017\n",
      "Student model 1-1 fidelity: 0.23754662869077017\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23755595708303423\n",
      "Student model 1-2 fidelity: 0.23755595708303423\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23796696819036664\n",
      "Student model 1-3 fidelity: 0.23796696819036664\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23746305118226096\n",
      "Student model 1-4 fidelity: 0.23746305118226096\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2375770017442774\n",
      "Student model 2-0 fidelity: 0.2375770017442774\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2375842871545696\n",
      "Student model 2-1 fidelity: 0.2375842871545696\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23777988007221298\n",
      "Student model 2-2 fidelity: 0.23777988007221298\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23745805674522824\n",
      "Student model 2-3 fidelity: 0.23745805674522824\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23796259040114798\n",
      "Student model 2-4 fidelity: 0.23796259040114798\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23796173784894256\n",
      "Student model 3-0 fidelity: 0.23796173784894256\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23765760195010896\n",
      "Student model 3-1 fidelity: 0.23765760195010896\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23765999228331328\n",
      "Student model 3-2 fidelity: 0.23765999228331328\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.237564105836654\n",
      "Student model 3-3 fidelity: 0.237564105836654\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23757613382351628\n",
      "Student model 3-4 fidelity: 0.23757613382351628\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2374205726512376\n",
      "Student model 4-0 fidelity: 0.2374205726512376\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23724211457667557\n",
      "Student model 4-1 fidelity: 0.23724211457667557\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23753135852180252\n",
      "Student model 4-2 fidelity: 0.23753135852180252\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23781832339849665\n",
      "Student model 4-3 fidelity: 0.23781832339849665\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23773447599849706\n",
      "Student model 4-4 fidelity: 0.23773447599849706\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.23794555684830856)\n",
      "Top-k Prunned explanation size: 7\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.772     0.798    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.434     1.000     0.606    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.680   1648161\n",
      "   macro avg      0.217     0.290     0.238   1648161\n",
      "weighted avg      0.623     0.680     0.633   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.772     0.798    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.434     1.000     0.606    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.680   1648161\n",
      "   macro avg      0.217     0.290     0.238   1648161\n",
      "weighted avg      0.623     0.680     0.633   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.435     0.998     0.606    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.662   1648161\n",
      "   macro avg      0.171     0.231     0.189   1648161\n",
      "weighted avg      0.588     0.662     0.606   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.435     0.998     0.606    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.662   1648161\n",
      "   macro avg      0.171     0.231     0.189   1648161\n",
      "weighted avg      0.588     0.662     0.606   1648161\n",
      "\n",
      "Progress |--| 98.9% Complete\n",
      "Iteration 3/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6925548858052758\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05691611437989502\n",
      "Student model 0-0 fidelity: 0.05691611437989502\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05699346767366234\n",
      "Student model 0-1 fidelity: 0.05699346767366234\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05690376352942264\n",
      "Student model 0-2 fidelity: 0.05690376352942264\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0568974975425594\n",
      "Student model 0-3 fidelity: 0.0568974975425594\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056922109805027876\n",
      "Student model 0-4 fidelity: 0.056922109805027876\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05689830320951892\n",
      "Student model 1-0 fidelity: 0.05689830320951892\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05697344676463006\n",
      "Student model 1-1 fidelity: 0.05697344676463006\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696504295565555\n",
      "Student model 1-2 fidelity: 0.05696504295565555\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0569433122472936\n",
      "Student model 1-3 fidelity: 0.0569433122472936\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05688523204066758\n",
      "Student model 1-4 fidelity: 0.05688523204066758\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05700088427991479\n",
      "Student model 2-0 fidelity: 0.05700088427991479\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056961108815387705\n",
      "Student model 2-1 fidelity: 0.056961108815387705\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056851644562218806\n",
      "Student model 2-2 fidelity: 0.056851644562218806\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056906001208510464\n",
      "Student model 2-3 fidelity: 0.056906001208510464\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692425726065974\n",
      "Student model 2-4 fidelity: 0.05692425726065974\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05686445501135685\n",
      "Student model 3-0 fidelity: 0.05686445501135685\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056902599900239076\n",
      "Student model 3-1 fidelity: 0.056902599900239076\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05694635327593525\n",
      "Student model 3-2 fidelity: 0.05694635327593525\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056939913251084356\n",
      "Student model 3-3 fidelity: 0.056939913251084356\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05697559221165243\n",
      "Student model 3-4 fidelity: 0.05697559221165243\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05690635922869777\n",
      "Student model 4-0 fidelity: 0.05690635922869777\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05694456445597279\n",
      "Student model 4-1 fidelity: 0.05694456445597279\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05684178833635501\n",
      "Student model 4-2 fidelity: 0.05684178833635501\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692425726065974\n",
      "Student model 4-3 fidelity: 0.05692425726065974\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05689901934799894\n",
      "Student model 4-4 fidelity: 0.05689901934799894\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.05699346767366234)\n",
      "Top-k Prunned explanation size: 3\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Progress |-| 99.0% Complete\n",
      "Iteration 4/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6928455444843948\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05697666490369244\n",
      "Student model 0-0 fidelity: 0.05697666490369244\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692926766365309\n",
      "Student model 0-1 fidelity: 0.05692926766365309\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05693400923081605\n",
      "Student model 0-2 fidelity: 0.05693400923081605\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056957889763462916\n",
      "Student model 0-3 fidelity: 0.056957889763462916\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.057041970184651875\n",
      "Student model 0-4 fidelity: 0.057041970184651875\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05697201641998923\n",
      "Student model 1-0 fidelity: 0.05697201641998923\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696316533299863\n",
      "Student model 1-1 fidelity: 0.05696316533299863\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056995076179480304\n",
      "Student model 1-2 fidelity: 0.056995076179480304\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05695708497096414\n",
      "Student model 1-3 fidelity: 0.05695708497096414\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056993378310844584\n",
      "Student model 1-4 fidelity: 0.056993378310844584\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05700695978813219\n",
      "Student model 2-0 fidelity: 0.05700695978813219\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05691468253968254\n",
      "Student model 2-1 fidelity: 0.05691468253968254\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05701723301293826\n",
      "Student model 2-2 fidelity: 0.05701723301293826\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05698810564682319\n",
      "Student model 2-3 fidelity: 0.05698810564682319\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056924704636671576\n",
      "Student model 2-4 fidelity: 0.056924704636671576\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056915935401911595\n",
      "Student model 3-0 fidelity: 0.056915935401911595\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692586779723202\n",
      "Student model 3-1 fidelity: 0.05692586779723202\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.057069819629096456\n",
      "Student model 3-2 fidelity: 0.057069819629096456\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.057003028653802496\n",
      "Student model 3-3 fidelity: 0.057003028653802496\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05701919810617571\n",
      "Student model 3-4 fidelity: 0.05701919810617571\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696951322710789\n",
      "Student model 4-0 fidelity: 0.05696951322710789\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.057014195912113895\n",
      "Student model 4-1 fidelity: 0.057014195912113895\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05702071653907207\n",
      "Student model 4-2 fidelity: 0.05702071653907207\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05698524568478585\n",
      "Student model 4-3 fidelity: 0.05698524568478585\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.057030183466211394\n",
      "Student model 4-4 fidelity: 0.057030183466211394\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.057041970184651875)\n",
      "Top-k Prunned explanation size: 3\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Progress |-| 99.1% Complete\n",
      "Done!\n",
      "Collecting max depth prunning information...\n",
      "Iteration 1/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6929722216982417\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056943133358006215\n",
      "Student model 0-0 fidelity: 0.056943133358006215\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056995076179480304\n",
      "Student model 0-1 fidelity: 0.056995076179480304\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0569746088921849\n",
      "Student model 0-2 fidelity: 0.0569746088921849\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05704955877566389\n",
      "Student model 0-3 fidelity: 0.05704955877566389\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0570035647341712\n",
      "Student model 0-4 fidelity: 0.0570035647341712\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05698059792834867\n",
      "Student model 1-0 fidelity: 0.05698059792834867\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696915561879878\n",
      "Student model 1-1 fidelity: 0.05696915561879878\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05697550281970206\n",
      "Student model 1-2 fidelity: 0.05697550281970206\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05691548795439912\n",
      "Student model 1-3 fidelity: 0.05691548795439912\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056922467720136184\n",
      "Student model 1-4 fidelity: 0.056922467720136184\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05699185912066118\n",
      "Student model 2-0 fidelity: 0.05699185912066118\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05698765878758643\n",
      "Student model 2-1 fidelity: 0.05698765878758643\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696629466838688\n",
      "Student model 2-2 fidelity: 0.05696629466838688\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696861920196337\n",
      "Student model 2-3 fidelity: 0.05696861920196337\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05697156942963912\n",
      "Student model 2-4 fidelity: 0.05697156942963912\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692014123020082\n",
      "Student model 3-0 fidelity: 0.05692014123020082\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056936514044281246\n",
      "Student model 3-1 fidelity: 0.056936514044281246\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0569926634030598\n",
      "Student model 3-2 fidelity: 0.0569926634030598\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05698462004820801\n",
      "Student model 3-3 fidelity: 0.05698462004820801\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692577832421814\n",
      "Student model 3-4 fidelity: 0.05692577832421814\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0569637018031734\n",
      "Student model 4-0 fidelity: 0.0569637018031734\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696745694748939\n",
      "Student model 4-1 fidelity: 0.05696745694748939\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05691683028599135\n",
      "Student model 4-2 fidelity: 0.05691683028599135\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05694778428991037\n",
      "Student model 4-3 fidelity: 0.05694778428991037\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.057014195912113895\n",
      "Student model 4-4 fidelity: 0.057014195912113895\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.05704955877566389)\n",
      "Top-k Prunned explanation size: 3\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Progress |-| 99.3% Complete\n",
      "Iteration 2/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6927852410272713\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17028590283009096\n",
      "Student model 0-0 fidelity: 0.17028590283009096\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17056479979044836\n",
      "Student model 0-1 fidelity: 0.17056479979044836\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17030394809995045\n",
      "Student model 0-2 fidelity: 0.17030394809995045\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17029066541944282\n",
      "Student model 0-3 fidelity: 0.17029066541944282\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17017586686449013\n",
      "Student model 0-4 fidelity: 0.17017586686449013\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17021412702174643\n",
      "Student model 1-0 fidelity: 0.17021412702174643\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17021041310266602\n",
      "Student model 1-1 fidelity: 0.17021041310266602\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.1702523437092737\n",
      "Student model 1-2 fidelity: 0.1702523437092737\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17031534532111778\n",
      "Student model 1-3 fidelity: 0.17031534532111778\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.1702364018158938\n",
      "Student model 1-4 fidelity: 0.1702364018158938\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17029634806086993\n",
      "Student model 2-0 fidelity: 0.17029634806086993\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.1701550445411981\n",
      "Student model 2-1 fidelity: 0.1701550445411981\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.1701970980291341\n",
      "Student model 2-2 fidelity: 0.1701970980291341\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17007232205895548\n",
      "Student model 2-3 fidelity: 0.17007232205895548\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.1703866533297376\n",
      "Student model 2-4 fidelity: 0.1703866533297376\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17013191413361195\n",
      "Student model 3-0 fidelity: 0.17013191413361195\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17035933446844642\n",
      "Student model 3-1 fidelity: 0.17035933446844642\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17017908515074354\n",
      "Student model 3-2 fidelity: 0.17017908515074354\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17029083895654862\n",
      "Student model 3-3 fidelity: 0.17029083895654862\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17025088655322593\n",
      "Student model 3-4 fidelity: 0.17025088655322593\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17032533393678986\n",
      "Student model 4-0 fidelity: 0.17032533393678986\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17036445381723497\n",
      "Student model 4-1 fidelity: 0.17036445381723497\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17041707952126608\n",
      "Student model 4-2 fidelity: 0.17041707952126608\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17041446494961174\n",
      "Student model 4-3 fidelity: 0.17041446494961174\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 2 and 4 leaves:\n",
      "Student model score: 0.17044185195124759\n",
      "Student model 4-4 fidelity: 0.17044185195124759\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.17056479979044836)\n",
      "Top-k Prunned explanation size: 7\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.930     0.738     0.823    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.259     1.000     0.411    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.681     0.999     0.810    206316\n",
      "\n",
      "    accuracy                          0.613   1648161\n",
      "   macro avg      0.156     0.228     0.170   1648161\n",
      "weighted avg      0.595     0.613     0.571   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.930     0.738     0.823    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.259     1.000     0.411    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.681     0.999     0.810    206316\n",
      "\n",
      "    accuracy                          0.613   1648161\n",
      "   macro avg      0.156     0.228     0.170   1648161\n",
      "weighted avg      0.595     0.613     0.571   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.885     0.729     0.800    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.259     0.998     0.412    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.680     1.000     0.810    206068\n",
      "\n",
      "    accuracy                          0.594   1648161\n",
      "   macro avg      0.122     0.182     0.135   1648161\n",
      "weighted avg      0.555     0.594     0.544   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.885     0.729     0.800    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.259     0.998     0.412    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.680     1.000     0.810    206068\n",
      "\n",
      "    accuracy                          0.594   1648161\n",
      "   macro avg      0.122     0.182     0.135   1648161\n",
      "weighted avg      0.555     0.594     0.544   1648161\n",
      "\n",
      "Progress |-| 99.4% Complete\n",
      "Iteration 3/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6924772245930182\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2945705027756432\n",
      "Student model 0-0 fidelity: 0.2945705027756432\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29417912187680567\n",
      "Student model 0-1 fidelity: 0.29417912187680567\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2948537982079156\n",
      "Student model 0-2 fidelity: 0.2948537982079156\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2942265620899109\n",
      "Student model 0-3 fidelity: 0.2942265620899109\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29441742163761553\n",
      "Student model 0-4 fidelity: 0.29441742163761553\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29421718592761187\n",
      "Student model 1-0 fidelity: 0.29421718592761187\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.294909340867978\n",
      "Student model 1-1 fidelity: 0.294909340867978\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2947434338027202\n",
      "Student model 1-2 fidelity: 0.2947434338027202\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2945179219114149\n",
      "Student model 1-3 fidelity: 0.2945179219114149\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2941666913577585\n",
      "Student model 1-4 fidelity: 0.2941666913577585\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2946311941736382\n",
      "Student model 2-0 fidelity: 0.2946311941736382\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29500967151690555\n",
      "Student model 2-1 fidelity: 0.29500967151690555\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2942506193302863\n",
      "Student model 2-2 fidelity: 0.2942506193302863\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.294467052556369\n",
      "Student model 2-3 fidelity: 0.294467052556369\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2943862670297359\n",
      "Student model 2-4 fidelity: 0.2943862670297359\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29421076374480776\n",
      "Student model 3-0 fidelity: 0.29421076374480776\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2942365121865655\n",
      "Student model 3-1 fidelity: 0.2942365121865655\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2944612742771872\n",
      "Student model 3-2 fidelity: 0.2944612742771872\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2940309321252424\n",
      "Student model 3-3 fidelity: 0.2940309321252424\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29436696676319424\n",
      "Student model 3-4 fidelity: 0.29436696676319424\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29464671490396627\n",
      "Student model 4-0 fidelity: 0.29464671490396627\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2946834764835504\n",
      "Student model 4-1 fidelity: 0.2946834764835504\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2944317778366648\n",
      "Student model 4-2 fidelity: 0.2944317778366648\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.2943818317628412\n",
      "Student model 4-3 fidelity: 0.2943818317628412\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 3 and 8 leaves:\n",
      "Student model score: 0.29439935463764716\n",
      "Student model 4-4 fidelity: 0.29439935463764716\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.2948537982079156)\n",
      "Top-k Prunned explanation size: 15\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.932     0.767     0.842    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.434     1.000     0.606    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.725   1648161\n",
      "   macro avg      0.271     0.351     0.294   1648161\n",
      "weighted avg      0.708     0.725     0.694   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.932     0.767     0.842    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.434     1.000     0.606    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.725   1648161\n",
      "   macro avg      0.271     0.351     0.294   1648161\n",
      "weighted avg      0.708     0.725     0.694   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.888     0.759     0.818    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.435     0.998     0.606    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.706   1648161\n",
      "   macro avg      0.214     0.280     0.234   1648161\n",
      "weighted avg      0.668     0.706     0.667   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.888     0.759     0.818    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.435     0.998     0.606    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.706   1648161\n",
      "   macro avg      0.214     0.280     0.234   1648161\n",
      "weighted avg      0.668     0.706     0.667   1648161\n",
      "\n",
      "Progress |-| 99.5% Complete\n",
      "Iteration 4/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6925614310883454\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47735251818488855\n",
      "Student model 0-0 fidelity: 0.47735251818488855\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4760490944858236\n",
      "Student model 0-1 fidelity: 0.4760490944858236\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47633937599807025\n",
      "Student model 0-2 fidelity: 0.47633937599807025\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.477008741095242\n",
      "Student model 0-3 fidelity: 0.477008741095242\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47791358976798914\n",
      "Student model 0-4 fidelity: 0.47791358976798914\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47640437658708173\n",
      "Student model 1-0 fidelity: 0.47640437658708173\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4764859327634367\n",
      "Student model 1-1 fidelity: 0.4764859327634367\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47611552870735535\n",
      "Student model 1-2 fidelity: 0.47611552870735535\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4769594221562048\n",
      "Student model 1-3 fidelity: 0.4769594221562048\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.476638739974528\n",
      "Student model 1-4 fidelity: 0.476638739974528\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4762708520193825\n",
      "Student model 2-0 fidelity: 0.4762708520193825\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47631710035948577\n",
      "Student model 2-1 fidelity: 0.47631710035948577\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4768227824849756\n",
      "Student model 2-2 fidelity: 0.4768227824849756\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47626136758065307\n",
      "Student model 2-3 fidelity: 0.47626136758065307\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47600486037087403\n",
      "Student model 2-4 fidelity: 0.47600486037087403\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47630754199978903\n",
      "Student model 3-0 fidelity: 0.47630754199978903\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47692659846915614\n",
      "Student model 3-1 fidelity: 0.47692659846915614\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47766163178737403\n",
      "Student model 3-2 fidelity: 0.47766163178737403\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47728836283548276\n",
      "Student model 3-3 fidelity: 0.47728836283548276\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47572940650362744\n",
      "Student model 3-4 fidelity: 0.47572940650362744\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4766341319639724\n",
      "Student model 4-0 fidelity: 0.4766341319639724\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.477385256160447\n",
      "Student model 4-1 fidelity: 0.477385256160447\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47740249946520413\n",
      "Student model 4-2 fidelity: 0.47740249946520413\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.4770177939531038\n",
      "Student model 4-3 fidelity: 0.4770177939531038\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 4 and 16 leaves:\n",
      "Student model score: 0.47769740471274785\n",
      "Student model 4-4 fidelity: 0.47769740471274785\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999936653301305, 0.47766163178737403)\n",
      "Top-k Prunned explanation size: 23\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.928     0.951     0.939    855297\n",
      "           1      0.949     0.017     0.033     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.971     0.964     0.968    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.515     1.000     0.680     69893\n",
      "           9      0.668     0.501     0.573     56122\n",
      "          10      0.993     0.460     0.629     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      1.000     0.999     0.999    206316\n",
      "\n",
      "    accuracy                          0.888   1648161\n",
      "   macro avg      0.570     0.490     0.477   1648161\n",
      "weighted avg      0.880     0.888     0.871   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     0.951     0.938    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.515     1.000     0.680     69893\n",
      "           9      0.668     0.501     0.573     56122\n",
      "          10      0.993     0.460     0.629     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.886   1648161\n",
      "   macro avg      0.491     0.487     0.473   1648161\n",
      "weighted avg      0.871     0.886     0.869   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.893     0.951     0.921    823804\n",
      "           1      0.743     0.004     0.007     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.973     0.965     0.969    138781\n",
      "           7      0.821     0.988     0.897    173016\n",
      "           8      0.427     1.000     0.598     57846\n",
      "           9      0.668     0.501     0.572     56190\n",
      "          10      1.000     0.480     0.648     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      1.000     1.000     1.000    206068\n",
      "\n",
      "    accuracy                          0.863   1648161\n",
      "   macro avg      0.435     0.393     0.374   1648161\n",
      "weighted avg      0.851     0.863     0.836   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.890     0.950     0.919    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.821     0.988     0.897    173016\n",
      "           8      0.427     1.000     0.598     57846\n",
      "           9      0.668     0.501     0.572     56190\n",
      "          10      1.000     0.480     0.648     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.860   1648161\n",
      "   macro avg      0.385     0.391     0.373   1648161\n",
      "weighted avg      0.827     0.860     0.833   1648161\n",
      "\n",
      "Progress |-| 99.6% Complete\n",
      "Iteration 5/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6928676832143966\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7171088255003378\n",
      "Student model 0-0 fidelity: 0.7171088255003378\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7157366118473835\n",
      "Student model 0-1 fidelity: 0.7157366118473835\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7169639009372538\n",
      "Student model 0-2 fidelity: 0.7169639009372538\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7161167677264618\n",
      "Student model 0-3 fidelity: 0.7161167677264618\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7172036496915527\n",
      "Student model 0-4 fidelity: 0.7172036496915527\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7160738587807152\n",
      "Student model 1-0 fidelity: 0.7160738587807152\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7163108892455421\n",
      "Student model 1-1 fidelity: 0.7163108892455421\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7156142720134836\n",
      "Student model 1-2 fidelity: 0.7156142720134836\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7177388597555483\n",
      "Student model 1-3 fidelity: 0.7177388597555483\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7156039586128767\n",
      "Student model 1-4 fidelity: 0.7156039586128767\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7174988783548749\n",
      "Student model 2-0 fidelity: 0.7174988783548749\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7167867706225507\n",
      "Student model 2-1 fidelity: 0.7167867706225507\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7162842045514234\n",
      "Student model 2-2 fidelity: 0.7162842045514234\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7165903477379456\n",
      "Student model 2-3 fidelity: 0.7165903477379456\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7162556920638922\n",
      "Student model 2-4 fidelity: 0.7162556920638922\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7158335450437457\n",
      "Student model 3-0 fidelity: 0.7158335450437457\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7182230223230691\n",
      "Student model 3-1 fidelity: 0.7182230223230691\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.715681352624521\n",
      "Student model 3-2 fidelity: 0.715681352624521\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7155741961077257\n",
      "Student model 3-3 fidelity: 0.7155741961077257\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7151996067289302\n",
      "Student model 3-4 fidelity: 0.7151996067289302\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7159552808916064\n",
      "Student model 4-0 fidelity: 0.7159552808916064\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7168059502486174\n",
      "Student model 4-1 fidelity: 0.7168059502486174\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.716699093592636\n",
      "Student model 4-2 fidelity: 0.716699093592636\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 5 and 30 leaves:\n",
      "Student model score: 0.7175543484575727\n",
      "Student model 4-3 fidelity: 0.7175543484575727\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 5 and 29 leaves:\n",
      "Student model score: 0.7161164676875802\n",
      "Student model 4-4 fidelity: 0.7161164676875802\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999890354889119, 0.7175543484575727)\n",
      "Top-k Prunned explanation size: 27\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     0.949     0.968    855297\n",
      "           1      0.982     0.024     0.048     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.998     0.966     0.982    138687\n",
      "           7      0.942     0.977     0.959    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      1.000     0.501     0.668     56122\n",
      "          10      0.698     0.974     0.813     88944\n",
      "          11      0.554     0.736     0.632     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      1.000     1.000     1.000       737\n",
      "          14      1.000     0.999     0.999    206316\n",
      "\n",
      "    accuracy                          0.934   1648161\n",
      "   macro avg      0.798     0.754     0.718   1648161\n",
      "weighted avg      0.954     0.934     0.934   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.916     0.947    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.417     1.000     0.588     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.942     0.977     0.959    172485\n",
      "           8      1.000     0.916     0.956     69893\n",
      "           9      0.402     0.501     0.446     56122\n",
      "          10      0.697     0.974     0.812     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.993     0.999     0.996    206316\n",
      "\n",
      "    accuracy                          0.908   1648161\n",
      "   macro avg      0.534     0.602     0.556   1648161\n",
      "weighted avg      0.916     0.908     0.908   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.945     0.945     0.945    823804\n",
      "           1      0.792     0.005     0.011     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      1.000     0.968     0.984    138781\n",
      "           7      0.942     0.974     0.958    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      1.000     0.501     0.667     56190\n",
      "          10      0.677     0.977     0.800     85913\n",
      "          11      0.552     0.748     0.635     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      1.000     1.000     1.000    206068\n",
      "\n",
      "    accuracy                          0.897   1648161\n",
      "   macro avg      0.576     0.563     0.532   1648161\n",
      "weighted avg      0.911     0.897     0.887   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.910     0.924    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.324     0.554     0.409     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.942     0.974     0.958    173016\n",
      "           8      0.708     0.784     0.744     57846\n",
      "           9      0.402     0.501     0.446     56190\n",
      "          10      0.678     0.981     0.802     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.993     1.000     0.997    206068\n",
      "\n",
      "    accuracy                          0.871   1648161\n",
      "   macro avg      0.398     0.443     0.416   1648161\n",
      "weighted avg      0.857     0.871     0.861   1648161\n",
      "\n",
      "Progress |-| 99.8% Complete\n",
      "Done!\n",
      "Collecting max leaves prunning information...\n",
      "Iteration 2/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6924165401249804\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056981759904385504\n",
      "Student model 0-0 fidelity: 0.056981759904385504\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05695377625552539\n",
      "Student model 0-1 fidelity: 0.05695377625552539\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05690671724654948\n",
      "Student model 0-2 fidelity: 0.05690671724654948\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05695583304805221\n",
      "Student model 0-3 fidelity: 0.05695583304805221\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.0568781579868928\n",
      "Student model 0-4 fidelity: 0.0568781579868928\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056938213674008274\n",
      "Student model 1-0 fidelity: 0.056938213674008274\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056912176729410006\n",
      "Student model 1-1 fidelity: 0.056912176729410006\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05696155589099244\n",
      "Student model 1-2 fidelity: 0.05696155589099244\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05691092378136456\n",
      "Student model 1-3 fidelity: 0.05691092378136456\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056951361661544984\n",
      "Student model 1-4 fidelity: 0.056951361661544984\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05698801627526714\n",
      "Student model 2-0 fidelity: 0.05698801627526714\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05702384259202691\n",
      "Student model 2-1 fidelity: 0.05702384259202691\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05693114644682576\n",
      "Student model 2-2 fidelity: 0.05693114644682576\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05689508047071897\n",
      "Student model 2-3 fidelity: 0.05689508047071897\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05691853052553877\n",
      "Student model 2-4 fidelity: 0.05691853052553877\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692953606518796\n",
      "Student model 3-0 fidelity: 0.05692953606518796\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05689472237694884\n",
      "Student model 3-1 fidelity: 0.05689472237694884\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05690680675064747\n",
      "Student model 3-2 fidelity: 0.05690680675064747\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05692524148307105\n",
      "Student model 3-3 fidelity: 0.05692524148307105\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056923720404630736\n",
      "Student model 3-4 fidelity: 0.056923720404630736\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05694018160054673\n",
      "Student model 4-0 fidelity: 0.05694018160054673\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05694143388068304\n",
      "Student model 4-1 fidelity: 0.05694143388068304\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.056925330956960306\n",
      "Student model 4-2 fidelity: 0.056925330956960306\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05689624425933689\n",
      "Student model 4-3 fidelity: 0.05689624425933689\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 1 and 2 leaves:\n",
      "Student model score: 0.05694519054959441\n",
      "Student model 4-4 fidelity: 0.05694519054959441\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.056981759904385504)\n",
      "Top-k Prunned explanation size: 3\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.519     1.000     0.683    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.000     0.000     0.000    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.000     0.000     0.000    206316\n",
      "\n",
      "    accuracy                          0.519   1648161\n",
      "   macro avg      0.043     0.083     0.057   1648161\n",
      "weighted avg      0.269     0.519     0.355   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     1.000     0.667    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.000     0.000     0.000    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.000     0.000     0.000    206068\n",
      "\n",
      "    accuracy                          0.500   1648161\n",
      "   macro avg      0.033     0.067     0.044   1648161\n",
      "weighted avg      0.250     0.500     0.333   1648161\n",
      "\n",
      "Progress |-| 99.9% Complete\n",
      "Iteration 3/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.692792865538789\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16804477794150682\n",
      "Student model 0-0 fidelity: 0.16804477794150682\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16804896713026682\n",
      "Student model 0-1 fidelity: 0.16804896713026682\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16797863321560602\n",
      "Student model 0-2 fidelity: 0.16797863321560602\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16802875416167073\n",
      "Student model 0-3 fidelity: 0.16802875416167073\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16802093023304124\n",
      "Student model 0-4 fidelity: 0.16802093023304124\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16791502095529198\n",
      "Student model 1-0 fidelity: 0.16791502095529198\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16795069300307633\n",
      "Student model 1-1 fidelity: 0.16795069300307633\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.1679413515079721\n",
      "Student model 1-2 fidelity: 0.1679413515079721\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16805848420988678\n",
      "Student model 1-3 fidelity: 0.16805848420988678\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.1678992381643858\n",
      "Student model 1-4 fidelity: 0.1678992381643858\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16798278465189778\n",
      "Student model 2-0 fidelity: 0.16798278465189778\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16812015009187795\n",
      "Student model 2-1 fidelity: 0.16812015009187795\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16791094810397558\n",
      "Student model 2-2 fidelity: 0.16791094810397558\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16789395244057034\n",
      "Student model 2-3 fidelity: 0.16789395244057034\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16801025672248027\n",
      "Student model 2-4 fidelity: 0.16801025672248027\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16810694613638968\n",
      "Student model 3-0 fidelity: 0.16810694613638968\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16821332120524934\n",
      "Student model 3-1 fidelity: 0.16821332120524934\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16794601235661022\n",
      "Student model 3-2 fidelity: 0.16794601235661022\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16808576306877818\n",
      "Student model 3-3 fidelity: 0.16808576306877818\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16814115042904065\n",
      "Student model 3-4 fidelity: 0.16814115042904065\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.1677874643006262\n",
      "Student model 4-0 fidelity: 0.1677874643006262\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.1680208549672159\n",
      "Student model 4-1 fidelity: 0.1680208549672159\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.1680674291562858\n",
      "Student model 4-2 fidelity: 0.1680674291562858\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.16778119313826564\n",
      "Student model 4-3 fidelity: 0.16778119313826564\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 2 and 3 leaves:\n",
      "Student model score: 0.1680594331635339\n",
      "Student model 4-4 fidelity: 0.1680594331635339\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999993324943093, 0.16805848420988678)\n",
      "Top-k Prunned explanation size: 5\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.772     0.798    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.259     1.000     0.411    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.600   1648161\n",
      "   macro avg      0.162     0.211     0.168   1648161\n",
      "weighted avg      0.563     0.600     0.558   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.772     0.798    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.000     0.000     0.000    138687\n",
      "           7      0.259     1.000     0.411    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.600   1648161\n",
      "   macro avg      0.162     0.211     0.168   1648161\n",
      "weighted avg      0.563     0.600     0.558   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.259     0.998     0.412    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.582   1648161\n",
      "   macro avg      0.127     0.168     0.133   1648161\n",
      "weighted avg      0.529     0.582     0.532   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.000     0.000     0.000    138781\n",
      "           7      0.259     0.998     0.412    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.582   1648161\n",
      "   macro avg      0.127     0.168     0.133   1648161\n",
      "weighted avg      0.529     0.582     0.532   1648161\n",
      "\n",
      "Progress || 100.0% Complete\n",
      "Iteration 4/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6928167203762143\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2375751889268224\n",
      "Student model 0-0 fidelity: 0.2375751889268224\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23773879496044611\n",
      "Student model 0-1 fidelity: 0.23773879496044611\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23777450135912406\n",
      "Student model 0-2 fidelity: 0.23777450135912406\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23764851249575092\n",
      "Student model 0-3 fidelity: 0.23764851249575092\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2373788299379157\n",
      "Student model 0-4 fidelity: 0.2373788299379157\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23737881178044853\n",
      "Student model 1-0 fidelity: 0.23737881178044853\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2377954435221414\n",
      "Student model 1-1 fidelity: 0.2377954435221414\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23728591738849678\n",
      "Student model 1-2 fidelity: 0.23728591738849678\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23745469650688864\n",
      "Student model 1-3 fidelity: 0.23745469650688864\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23768229155790302\n",
      "Student model 1-4 fidelity: 0.23768229155790302\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2375056812061993\n",
      "Student model 2-0 fidelity: 0.2375056812061993\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23774233919804033\n",
      "Student model 2-1 fidelity: 0.23774233919804033\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23744870831099862\n",
      "Student model 2-2 fidelity: 0.23744870831099862\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23748194661970698\n",
      "Student model 2-3 fidelity: 0.23748194661970698\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23747593253559804\n",
      "Student model 2-4 fidelity: 0.23747593253559804\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23736895178273487\n",
      "Student model 3-0 fidelity: 0.23736895178273487\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23789892875386062\n",
      "Student model 3-1 fidelity: 0.23789892875386062\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.237826105441395\n",
      "Student model 3-2 fidelity: 0.237826105441395\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23769781777863042\n",
      "Student model 3-3 fidelity: 0.23769781777863042\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2375315950336022\n",
      "Student model 3-4 fidelity: 0.2375315950336022\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23791288037783642\n",
      "Student model 4-0 fidelity: 0.23791288037783642\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23767145143913074\n",
      "Student model 4-1 fidelity: 0.23767145143913074\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2373476717424915\n",
      "Student model 4-2 fidelity: 0.2373476717424915\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.23751032203959388\n",
      "Student model 4-3 fidelity: 0.23751032203959388\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 3 and 4 leaves:\n",
      "Student model score: 0.2376356275558457\n",
      "Student model 4-4 fidelity: 0.2376356275558457\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (1.0, 0.23777450135912406)\n",
      "Top-k Prunned explanation size: 7\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.772     0.798    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.434     1.000     0.606    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.680   1648161\n",
      "   macro avg      0.217     0.290     0.238   1648161\n",
      "weighted avg      0.623     0.680     0.633   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.772     0.798    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.434     1.000     0.606    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.680   1648161\n",
      "   macro avg      0.217     0.290     0.238   1648161\n",
      "weighted avg      0.623     0.680     0.633   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.435     0.998     0.606    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.662   1648161\n",
      "   macro avg      0.171     0.231     0.189   1648161\n",
      "weighted avg      0.588     0.662     0.606   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.435     0.998     0.606    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.662   1648161\n",
      "   macro avg      0.171     0.231     0.189   1648161\n",
      "weighted avg      0.588     0.662     0.606   1648161\n",
      "\n",
      "Progress || 100.0% Complete\n",
      "Iteration 5/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.692662060301736\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26724518440728245\n",
      "Student model 0-0 fidelity: 0.26724518440728245\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2672659434882091\n",
      "Student model 0-1 fidelity: 0.2672659434882091\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26712082726583336\n",
      "Student model 0-2 fidelity: 0.26712082726583336\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2675468299935315\n",
      "Student model 0-3 fidelity: 0.2675468299935315\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2672700065599334\n",
      "Student model 0-4 fidelity: 0.2672700065599334\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.267300686837163\n",
      "Student model 1-0 fidelity: 0.267300686837163\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2673734276163917\n",
      "Student model 1-1 fidelity: 0.2673734276163917\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26717434096538917\n",
      "Student model 1-2 fidelity: 0.26717434096538917\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26728692609167337\n",
      "Student model 1-3 fidelity: 0.26728692609167337\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26733062615337794\n",
      "Student model 1-4 fidelity: 0.26733062615337794\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26690524441097613\n",
      "Student model 2-0 fidelity: 0.26690524441097613\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26715454622586693\n",
      "Student model 2-1 fidelity: 0.26715454622586693\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26705276105166814\n",
      "Student model 2-2 fidelity: 0.26705276105166814\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2674504699546328\n",
      "Student model 2-3 fidelity: 0.2674504699546328\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2675274080799231\n",
      "Student model 2-4 fidelity: 0.2675274080799231\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26706597335817933\n",
      "Student model 3-0 fidelity: 0.26706597335817933\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26708589163713503\n",
      "Student model 3-1 fidelity: 0.26708589163713503\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26741077531952256\n",
      "Student model 3-2 fidelity: 0.26741077531952256\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2673618604795554\n",
      "Student model 3-3 fidelity: 0.2673618604795554\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2672938250544397\n",
      "Student model 3-4 fidelity: 0.2672938250544397\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26703359570589497\n",
      "Student model 4-0 fidelity: 0.26703359570589497\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2673485873972116\n",
      "Student model 4-1 fidelity: 0.2673485873972116\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.2672236012872508\n",
      "Student model 4-2 fidelity: 0.2672236012872508\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26722303790845964\n",
      "Student model 4-3 fidelity: 0.26722303790845964\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 4 and 5 leaves:\n",
      "Student model score: 0.26733363809790234\n",
      "Student model 4-4 fidelity: 0.26733363809790234\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999838035574795, 0.26741077531952256)\n",
      "Top-k Prunned explanation size: 9\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.758   1648161\n",
      "   macro avg      0.247     0.302     0.267   1648161\n",
      "weighted avg      0.649     0.758     0.694   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.487     0.946     0.643    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.000     0.000     0.000     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.758   1648161\n",
      "   macro avg      0.247     0.302     0.267   1648161\n",
      "weighted avg      0.649     0.758     0.694   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.822     0.988     0.897    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.739   1648161\n",
      "   macro avg      0.196     0.241     0.212   1648161\n",
      "weighted avg      0.619     0.739     0.668   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.486     0.944     0.642    138781\n",
      "           7      0.822     0.988     0.897    173016\n",
      "           8      0.000     0.000     0.000     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.739   1648161\n",
      "   macro avg      0.196     0.241     0.212   1648161\n",
      "weighted avg      0.619     0.739     0.668   1648161\n",
      "\n",
      "Progress || 100.0% Complete\n",
      "Iteration 6/5\n",
      "Fitting blackbox model...\n",
      "Done!\n",
      "Blackbox model score report with training data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.990     0.972    823804\n",
      "           1      0.713     0.198     0.310     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.739     0.527     0.615     41854\n",
      "           6      0.998     0.998     0.998    138781\n",
      "           7      0.997     0.994     0.995    173016\n",
      "           8      0.717     0.866     0.784     57846\n",
      "           9      1.000     0.998     0.999     56190\n",
      "          10      0.961     0.995     0.978     85913\n",
      "          11      0.972     0.993     0.983     12439\n",
      "          12      0.855     0.985     0.915      3267\n",
      "          13      0.699     0.996     0.821       517\n",
      "          14      0.999     1.000     0.999    206068\n",
      "\n",
      "    accuracy                          0.954   1648161\n",
      "   macro avg      0.707     0.703     0.691   1648161\n",
      "weighted avg      0.949     0.954     0.946   1648161\n",
      "\n",
      "Using Classification Trustee algorithm to extract DT...\n",
      "Initializing training dataset using MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.6923819891895024\n",
      "Initializing Trustee outer-loop with 5 iterations\n",
      "########## Outer-loop Iteration 0/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (2691994, 2691994) entries\n",
      "Student model 0-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35092053624243064\n",
      "Student model 0-0 fidelity: 0.35092053624243064\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (3499593, 3499593) entries\n",
      "Student model 0-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.350697854891477\n",
      "Student model 0-1 fidelity: 0.350697854891477\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (4307192, 4307192) entries\n",
      "Student model 0-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3505986999513521\n",
      "Student model 0-2 fidelity: 0.3505986999513521\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5114791, 5114791) entries\n",
      "Student model 0-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35071294534658026\n",
      "Student model 0-3 fidelity: 0.35071294534658026\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (5922390, 5922390) entries\n",
      "Student model 0-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35090265985359537\n",
      "Student model 0-4 fidelity: 0.35090265985359537\n",
      "########## Outer-loop Iteration 1/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (6729989, 6729989) entries\n",
      "Student model 1-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35063304283170504\n",
      "Student model 1-0 fidelity: 0.35063304283170504\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (7537588, 7537588) entries\n",
      "Student model 1-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35070368028344046\n",
      "Student model 1-1 fidelity: 0.35070368028344046\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (8345187, 8345187) entries\n",
      "Student model 1-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3507081938854861\n",
      "Student model 1-2 fidelity: 0.3507081938854861\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9152786, 9152786) entries\n",
      "Student model 1-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.350870482445314\n",
      "Student model 1-3 fidelity: 0.350870482445314\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (9960385, 9960385) entries\n",
      "Student model 1-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35051349184455977\n",
      "Student model 1-4 fidelity: 0.35051349184455977\n",
      "########## Outer-loop Iteration 2/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (10767984, 10767984) entries\n",
      "Student model 2-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35068389931873756\n",
      "Student model 2-0 fidelity: 0.35068389931873756\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (11575583, 11575583) entries\n",
      "Student model 2-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35088398313567265\n",
      "Student model 2-1 fidelity: 0.35088398313567265\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (12383182, 12383182) entries\n",
      "Student model 2-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3507309908160436\n",
      "Student model 2-2 fidelity: 0.3507309908160436\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13190781, 13190781) entries\n",
      "Student model 2-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35029381638110335\n",
      "Student model 2-3 fidelity: 0.35029381638110335\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (13998380, 13998380) entries\n",
      "Student model 2-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35065555584183317\n",
      "Student model 2-4 fidelity: 0.35065555584183317\n",
      "########## Outer-loop Iteration 3/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (14805979, 14805979) entries\n",
      "Student model 3-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3506020910844347\n",
      "Student model 3-0 fidelity: 0.3506020910844347\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (15613578, 15613578) entries\n",
      "Student model 3-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3506658423495021\n",
      "Student model 3-1 fidelity: 0.3506658423495021\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (16421177, 16421177) entries\n",
      "Student model 3-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3507721576421838\n",
      "Student model 3-2 fidelity: 0.3507721576421838\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (17228776, 17228776) entries\n",
      "Student model 3-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35048001629834563\n",
      "Student model 3-3 fidelity: 0.35048001629834563\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18036375, 18036375) entries\n",
      "Student model 3-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35052885838666786\n",
      "Student model 3-4 fidelity: 0.35052885838666786\n",
      "########## Outer-loop Iteration 4/5 ##########\n",
      "Initializing Trustee inner-loop with 5 iterations\n",
      "########## Inner-loop Iteration 0/5 ##########\n",
      "Sampling 2691994 points from training dataset with (18843974, 18843974) entries\n",
      "Student model 4-0 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35083412292116883\n",
      "Student model 4-0 fidelity: 0.35083412292116883\n",
      "########## Inner-loop Iteration 1/5 ##########\n",
      "Sampling 2691994 points from training dataset with (19651573, 19651573) entries\n",
      "Student model 4-1 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35047662044101147\n",
      "Student model 4-1 fidelity: 0.35047662044101147\n",
      "########## Inner-loop Iteration 2/5 ##########\n",
      "Sampling 2691994 points from training dataset with (20459172, 20459172) entries\n",
      "Student model 4-2 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.350520281407066\n",
      "Student model 4-2 fidelity: 0.350520281407066\n",
      "########## Inner-loop Iteration 3/5 ##########\n",
      "Sampling 2691994 points from training dataset with (21266771, 21266771) entries\n",
      "Student model 4-3 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.35055841535178894\n",
      "Student model 4-3 fidelity: 0.35055841535178894\n",
      "########## Inner-loop Iteration 4/5 ##########\n",
      "Sampling 2691994 points from training dataset with (22074370, 22074370) entries\n",
      "Student model 4-4 trained with depth 4 and 6 leaves:\n",
      "Student model score: 0.3507721562530763\n",
      "Student model 4-4 fidelity: 0.3507721562530763\n",
      "Done!\n",
      "Model explanation training (agreement, fidelity): (0.9999866954161059, 0.35092053624243064)\n",
      "Top-k Prunned explanation size: 11\n",
      "Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.515     1.000     0.680     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.800   1648161\n",
      "   macro avg      0.331     0.385     0.350   1648161\n",
      "weighted avg      0.713     0.800     0.750   1648161\n",
      "\n",
      "Top-k Model explanation global fidelity report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.924     0.857    855297\n",
      "           1      0.000     0.000     0.000     13385\n",
      "           5      0.000     0.000     0.000     29826\n",
      "           6      0.979     0.945     0.962    138687\n",
      "           7      0.822     0.991     0.898    172485\n",
      "           8      0.515     1.000     0.680     69893\n",
      "           9      0.000     0.000     0.000     56122\n",
      "          10      0.000     0.000     0.000     88944\n",
      "          11      0.000     0.000     0.000     12706\n",
      "          12      0.000     0.000     0.000      3763\n",
      "          13      0.000     0.000     0.000       737\n",
      "          14      0.860     0.760     0.807    206316\n",
      "\n",
      "    accuracy                          0.800   1648161\n",
      "   macro avg      0.331     0.385     0.350   1648161\n",
      "weighted avg      0.713     0.800     0.750   1648161\n",
      "\n",
      "Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.821     0.988     0.897    173016\n",
      "           8      0.427     1.000     0.598     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.774   1648161\n",
      "   macro avg      0.257     0.308     0.273   1648161\n",
      "weighted avg      0.675     0.774     0.716   1648161\n",
      "\n",
      "Top-k Model explanation score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.922     0.838    823804\n",
      "           1      0.000     0.000     0.000     48210\n",
      "           2      0.000     0.000     0.000       168\n",
      "           3      0.000     0.000     0.000        64\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000     41854\n",
      "           6      0.978     0.944     0.961    138781\n",
      "           7      0.821     0.988     0.897    173016\n",
      "           8      0.427     1.000     0.598     57846\n",
      "           9      0.000     0.000     0.000     56190\n",
      "          10      0.000     0.000     0.000     85913\n",
      "          11      0.000     0.000     0.000     12439\n",
      "          12      0.000     0.000     0.000      3267\n",
      "          13      0.000     0.000     0.000       517\n",
      "          14      0.860     0.761     0.807    206068\n",
      "\n",
      "    accuracy                          0.774   1648161\n",
      "   macro avg      0.257     0.308     0.273   1648161\n",
      "weighted avg      0.675     0.774     0.716   1648161\n",
      "\n",
      "Progress || 100.0% Complete\n",
      "Done!\n",
      "Progress || 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "from trustee.report.trust import TrustReport\n",
    "trust_report = TrustReport(\n",
    "        model,\n",
    "        X=X,\n",
    "        y=Y,\n",
    "        max_iter=5,\n",
    "        num_pruning_iter=5,\n",
    "        train_size=0.7,\n",
    "        trustee_num_iter=5,\n",
    "        trustee_num_stability_iter=5,\n",
    "        trustee_sample_size=1.0,\n",
    "        analyze_branches=True,\n",
    "        analyze_stability=True,\n",
    "        top_k=10,\n",
    "        verbose=True,\n",
    "        class_names=['Benign', 'Infilteration', 'Brute Force -Web',\n",
    "       'Brute Force -XSS', 'SQL Injection', 'DoS attacks-SlowHTTPTest',\n",
    "       'DoS attacks-Hulk', 'DDoS attacks-LOIC-HTTP', 'FTP-BruteForce',\n",
    "       'SSH-Bruteforce', 'Bot', 'DoS attacks-GoldenEye',\n",
    "       'DoS attacks-Slowloris', 'DDOS attack-LOIC-UDP',\n",
    "       'DDOS attack-HOIC'],\n",
    "        feature_names=X_train.columns,\n",
    "        is_classify=True,\n",
    "        predict_method_name='pred',\n",
    "        skip_retrain = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trustee_report.pkl', 'wb') as f:\n",
    "    pickle.dump(trust_report, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
